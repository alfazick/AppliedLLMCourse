{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMXlVz3teyrIkVjFUlXHvHI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alfazick/AppliedLLMCourse/blob/main/AppledLLMModule_1BatchingInference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RxndKsyOoKh1",
        "outputId": "d03aa932-023b-48a1-996e-805b19e07201"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Qwen2ForCausalLM(\n",
              "  (model): Qwen2Model(\n",
              "    (embed_tokens): Embedding(151936, 896)\n",
              "    (layers): ModuleList(\n",
              "      (0-23): 24 x Qwen2DecoderLayer(\n",
              "        (self_attn): Qwen2Attention(\n",
              "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
              "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
              "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
              "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
              "        )\n",
              "        (mlp): Qwen2MLP(\n",
              "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
              "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
              "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
              "          (act_fn): SiLUActivation()\n",
              "        )\n",
              "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
              "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
              "      )\n",
              "    )\n",
              "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
              "    (rotary_emb): Qwen2RotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "# Generation till Completion and Batch Processing\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_name = \"Qwen/Qwen2.5-0.5B\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generation till completion\n",
        "# so now let's talk about elephant in the room\n",
        "# llm are talkative, and since we are predicting the next token\n",
        "# we need to account for the idea that\n",
        "# 1) llm may finish it's logic or generation of text # with special token <eos>\n",
        "# 2) we don't care and after specified number of new tokens generated we halt\n",
        "# the generation, rude not user friendly but necessity\n",
        "\n",
        "print(tokenizer.eos_token)\n",
        "print(tokenizer.eos_token_id)\n",
        "\n",
        "# this should be your first command running after initialization\n",
        "# so basically we want to keep generating new tokens like before\n",
        "# and stop generating upon reaching condition 1 or 2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XbExDL9OvmoH",
        "outputId": "e9f9f164-35b1-42a7-fbdb-e7982c414780"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|endoftext|>\n",
            "151643\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def generate(model,tokenizer,prompt,max_new_tokens=15):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "    input_ids = inputs[\"input_ids\"]\n",
        "    attention_mask = inputs[\"attention_mask\"]\n",
        "    generated_tokens = list()\n",
        "\n",
        "    for step in range(max_new_tokens):\n",
        "        with torch.no_grad():\n",
        "            out = model(input_ids = input_ids,\n",
        "                        attention_mask = attention_mask)\n",
        "\n",
        "        logits_last = out.logits[:,-1,:]\n",
        "        probs = F.softmax(logits_last,dim=-1)\n",
        "\n",
        "        # so what we do is called greedy decoding, we will pick up the token\n",
        "        # with highest probability\n",
        "        next_token_id = torch.argmax(probs,dim=-1)\n",
        "\n",
        "        token_id_int = next_token_id.item()\n",
        "\n",
        "        # next token text\n",
        "        next_token = tokenizer.decode(token_id_int)\n",
        "\n",
        "\n",
        "        # ok and now pay attention at this moment your next token may say eos\n",
        "        # or effectively saying I am done predicting next token\n",
        "        # most important line\n",
        "        if tokenizer.eos_token_id is not None and token_id_int == tokenizer.eos_token_id:\n",
        "            break\n",
        "\n",
        "        generated_tokens.append(next_token)\n",
        "\n",
        "        # now let's update inputs, since we want to continue generations\n",
        "        # with newly minted token in context\n",
        "\n",
        "        # append to context: make shape [1,1], then cat\n",
        "        next_token_2d = next_token_id.view(1, 1)        # [1, 1]\n",
        "\n",
        "        input_ids = torch.cat([input_ids, next_token_2d], dim=1)\n",
        "\n",
        "        attention_mask = torch.cat(\n",
        "            [attention_mask, torch.ones((1, 1), dtype=attention_mask.dtype)], dim=1\n",
        "        )\n",
        "\n",
        "    continuation = \"\".join(generated_tokens)\n",
        "    return continuation, prompt + continuation\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qL1leJ_sxK3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Capital of Texas is \"\n",
        "new_text,full = generate(model,tokenizer,prompt,25)\n"
      ],
      "metadata": {
        "id": "4-yR-YiY3pg9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(new_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h40MZyqP4Wh-",
        "outputId": "4f3c2f7d-5cf4-46ac-b866-aaa5df12d4ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100 miles from Austin, Texas. How far is it from Austin, Texas to the capital of Texas?\n",
            "To determine\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "2rcFXPn26Qdu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Batching Light Version\n",
        "\n",
        "prompts = [\n",
        "    \"The capital of Texas is Dallas or Austin?\",\n",
        "    \"Where McAllen is located\",\n",
        "    \"What is a transformer in LLM?\"\n",
        "]\n",
        "\n",
        "inputs = tokenizer(prompts,return_tensors=\"pt\", padding=True)\n",
        "# Notice the idea of padding, we have a batch: [batch_size,max_seq_length]\n",
        "# inputs = tokenizer(prompts,return_tensors=\"pt\") this line is problematic # you can't stack them together\n",
        "# because of different length\n",
        "inputs[\"input_ids\"].shape # so we have a 3 prompts which is expected\n",
        "# but since we have a different length of number of tokens we are getting padding\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03LpIFOpoyG0",
        "outputId": "6e269be6-79af-4146-cb7f-74ffc3969554"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 9])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4kq2Fle8pPhW",
        "outputId": "abf11041-36c7-4f1a-e7f7-8faec6df08f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[   785,   6722,    315,   8257,    374,  18542,    476,  19260,     30],\n",
              "        [  9064,   4483,  79877,    374,   7407, 151643, 151643, 151643, 151643],\n",
              "        [  3838,    374,    264,  42578,    304,    444,  10994,     30, 151643]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "        [1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
              "        [1, 1, 1, 1, 1, 1, 1, 1, 0]])}"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# so notice those zeros in attention mask, so basically,\n",
        "# with padding = True, we are saying align our inputs\n",
        "# in the way that we have a matrix, where longest tokenized prompt\n",
        "# defines it's"
      ],
      "metadata": {
        "id": "F-0aOsyqpQyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.padding_side = 'right'\n"
      ],
      "metadata": {
        "id": "g6AgELnis468"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this brings a problem\n",
        "# inputs = tokenizer(prompts,return_tensors=\"pt\", padding=True)\n",
        "# with torch.no_grad():\n",
        "#     out = model.generate(**inputs)\n",
        "\n",
        "# out.shape()"
      ],
      "metadata": {
        "id": "iYdbKKhDtDeS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Note:\n",
        "# by default this one is called right padding but you can change it to left one\n",
        "tokenizer.padding_side = 'left'  # Change the padding side\n",
        "inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True)\n",
        "inputs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qRLhABFyrHpa",
        "outputId": "85e25fcc-c3d1-4147-8cd7-4e55226ea355"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[   785,   6722,    315,   8257,    374,  18542,    476,  19260,     30],\n",
              "        [151643, 151643, 151643, 151643,   9064,   4483,  79877,    374,   7407],\n",
              "        [151643,   3838,    374,    264,  42578,    304,    444,  10994,     30]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "        [0, 0, 0, 0, 1, 1, 1, 1, 1],\n",
              "        [0, 1, 1, 1, 1, 1, 1, 1, 1]])}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.padding_side = 'left'\n",
        "inputs = tokenizer(prompts,return_tensors=\"pt\", padding=True)\n",
        "with torch.no_grad():\n",
        "    out = model.generate(**inputs,max_new_tokens=40)\n",
        "\n",
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hEIvxJkarVep",
        "outputId": "e816f79b-1b3a-4c6e-9782-49140a96e113"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 49])"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# how long each prompt is (after left padding)\n",
        "input_lengths = inputs[\"attention_mask\"].sum(dim=1)  # tensor([L0, L1, L2])\n",
        "print(\"input lengths:\", input_lengths.tolist())\n",
        "\n",
        "# how many new tokens each row actually got\n",
        "gen_lengths = out.size(1) - input_lengths\n",
        "print(\"generated lengths:\", gen_lengths.tolist())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2J0cTTPVt5-G",
        "outputId": "efd6b5c7-4c04-486b-9d54-d8afb31aa5b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input lengths: [9, 5, 8]\n",
            "generated lengths: [40, 44, 41]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# only the new text per row\n",
        "\n",
        "continuations = []\n",
        "for i in range(out.size(0)):\n",
        "    L = int(input_lengths[i])              # length of the i-th prompt\n",
        "    cont_ids = out[i, L:]                  # tokens generated after the prompt\n",
        "    cont_ids = cont_ids.tolist()           # ensure CPU list of ints\n",
        "    text = tokenizer.decode(cont_ids, skip_special_tokens=True).strip() # try to remove skip_special_tokens=True\n",
        "    continuations.append(text)\n",
        "    print(f\"Row {i} continuation: {text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AbGiW9jlu59z",
        "outputId": "b8a6bad8-edc2-403d-cdca-7c964c0f6d8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row 0 continuation: Dallas\n",
            "Row 1 continuation: McAllen is located, it is a city that is known for its rich history and diverse culture. The city is home to many historic sites, museums, and cultural events that showcase the city's unique heritage. McAllen\n",
            "Row 2 continuation: ? A transformer is a device that converts alternating current (AC) from one voltage level to another. It is a type of electrical transformer that uses a core made of silicon steel to reduce the magnetic field strength\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "continuations"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqG8xijgvBIC",
        "outputId": "ad592fb7-82a1-4aae-f077-00276f67d291"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Dallas',\n",
              " \"McAllen is located, it is a city that is known for its rich history and diverse culture. The city is home to many historic sites, museums, and cultural events that showcase the city's unique heritage. McAllen\",\n",
              " '? A transformer is a device that converts alternating current (AC) from one voltage level to another. It is a type of electrical transformer that uses a core made of silicon steel to reduce the magnetic field strength']"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "diWc4JCKvEaR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}