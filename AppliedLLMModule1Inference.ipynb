{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMVeQw4Nx31mC8gKxouxhLX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
},
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alfazick/AppliedLLMCourse/blob/main/AppliedLLMModule1Inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nOCkZshI68Ke"
      },
      "outputs": [],
      "source": [
        "# Fine Grained Inference Options\n",
        "\n",
        "# small model, to run inference on cpu\n",
        "# model.eval() and torch.no_grad()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_name = \"Qwen/Qwen2.5-0.5B\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "\n",
        "# model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float32)\n",
        "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# model.to(device).eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350,
          "referenced_widgets": [
            "536ce7ba6bba447fba018ad325a9b2e0",
            "7cbcf2191cf14b71aafa4257a73cb3d2",
            "860beb7b9577478c83bd5e9f42bdef91",
            "f98b0e22e3a84e9faa8be4e7a346e830",
            "3a85789c94a646feabbfbeec8ed2b6f1",
            "208444e0951349c897bf4419b9e20275",
            "7474b5a97d3541059b417b1904a64397",
            "53ef741d9c7e4462b1e8e0c3d769ef13",
            "de715aae26b7418094cdfc978f4fa19c",
            "3c0fc7f0815b40ec885e600711cb0d87",
            "5214ab48378d4c16ad3af29e116ceec3",
            "702ce12e84694b429a580b6f94fb69b4",
            "498ba831318e4220922b8089d41a46ea",
            "22873d1de38f4f84aef26b67b47f7836",
            "2716831e95d7463891bc62ebc9fc717a",
            "87ed69e7e04843168ff162ae3ad5b2d9",
            "3034e81cab41454583f7e2058ee6ba47",
            "c4ccc8ff17164ea9b5f7b441de837d0f",
            "1cd2dd5ce91a439cad61b844f63ed691",
            "ab58f2a631bf47518361a9ce4a517a1e",
            "43e254746c20480386641f741dd95d77",
            "842c770036eb4852aa8e7b4ad2f41d33",
            "5c5b529b5f02483184b63b2626a7e1bd",
            "8153da04b6c34a36a2736b62d7c997b1",
            "fa56e7812a614813a54f98533766bbe1",
            "4eda3ab181d9422ba7dbc7625bca57ec",
            "6bf4a1b66ea149638b6cbcd2cd688d5c",
            "06e97af81de94efaacc1175d634c34f9",
            "58a5c75bfad54eb0b971f51999303109",
            "2a49bdb7ca02497dbdf5bacbdab50e3e",
            "3611b2dbe5464184a577ad7385fabc14",
            "37dc15603520428d94b93d0947a873d7",
            "c14478991fdd4d19a066e0a61db5ace1",
            "9df748fbd91e423d951b54684fe004b5",
            "2db64c5941a943e39cf0c521fb24dc0e",
            "36a79faf67be407e83bed6c3686ce7c1",
            "a76ab8b2df104219a8520998095e0c13",
            "24c7e53cbad04cb893be17ddf8793442",
            "2447eec776974978b3093322933b2c37",
            "bae7cc6cfab145a1b5482f218a29e435",
            "6fa7fa2ec8514224b8de5d540911f48c",
            "88947efc50b7445aa5347c899da4101c",
            "0ba60d14edb24c6999ddcae9ac65b726",
            "8ec958bae3014fecbd3871138d4826ea",
            "2296860592924635959ede5c7c4ade67",
            "94afaad9c8c440ebac558459e980d24f",
            "ea3fb96bfb2b4e8ea53768e044517af7",
            "d5bb566a98db42b9bf657cdc7708eb87",
            "ab46d2ce51f7496983cf8d11ec35cd82",
            "7b7b138b5b5f4047b507682862281444",
            "1430bd2ca9dd4a9a9d3e3e59ab9decd5",
            "3e1fceb137e44b8b80ebeb3fb804b7e3",
            "c08d2c3a8443433eb4c13a0082b3918f",
            "6f6d8389c502464797242b620c463d4b",
            "da42f64b8e754c4b87c8e6c2a5aa7fad",
            "afa3dfecc5554a4995a0d52303778af5",
            "2be05ab5629d4eb4b3de1b32d94e8608",
            "746c22f40dae425c8532749f96a93ede",
            "5b1c4c7ba1854ab9a49eb94e1c0016e7",
            "a939ddb69c854b1491a018b417cda66d",
            "d6b3a307131546d68583956c41508a25",
            "066d6386260548308d83c745d7b9add8",
            "89694f08752d4e5e87bd7ce86c633947",
            "a9fd80efef4a47b0abe9c680704c2ca6",
            "140c2c06b1de4dfb86a8d69cdf2f3aa4",
            "d94d666d75b04beca0091f9aa47d1b22",
            "ffea55b200f0459bb1a53d25be3e0ca3",
            "4fd6c87f57e44f39b0c64c76bf46915f",
            "df55091434e64deb8eb876059d08b538",
            "8dff43eeb9dc4b1e9ef5430c5f7b3f4e",
            "d7cd06a187cc4bceb95208e8df9c2b68",
            "c986cf860f2e4ca78f15dac94f270800",
            "974912f6bd104e29b87e2f7ddd208e24",
            "c89c1f3781ac46ecb46aa18bc43b0756",
            "b8da4cb9c32740519b25bf13f197dd32",
            "d28801c095ea46b4ba4bbab26287bdad",
            "2e135388d4c84937a4c87f6cf72db195"
          ]
        },
        "id": "JSFM1hh07NGJ",
        "outputId": "f70eefc1-876d-4727-c692-d2fec58a7215"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
            "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
            "You are not authenticated with the Hugging Face Hub in this notebook.\n",
            "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "536ce7ba6bba447fba018ad325a9b2e0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "702ce12e84694b429a580b6f94fb69b4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5c5b529b5f02483184b63b2626a7e1bd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9df748fbd91e423d951b54684fe004b5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/681 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2296860592924635959ede5c7c4ade67"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/988M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "afa3dfecc5554a4995a0d52303778af5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/138 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ffea55b200f0459bb1a53d25be3e0ca3"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKBEyH5h7kdK",
        "outputId": "d5acf604-7efa-4bd4-e931-87b40655a5d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Qwen2ForCausalLM(\n",
              "  (model): Qwen2Model(\n",
              "    (embed_tokens): Embedding(151936, 896)\n",
              "    (layers): ModuleList(\n",
              "      (0-23): 24 x Qwen2DecoderLayer(\n",
              "        (self_attn): Qwen2Attention(\n",
              "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
              "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
              "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
              "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
              "        )\n",
              "        (mlp): Qwen2MLP(\n",
              "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
              "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
              "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
              "          (act_fn): SiLUActivation()\n",
              "        )\n",
              "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
              "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
              "      )\n",
              "    )\n",
              "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
              "    (rotary_emb): Qwen2RotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.num_parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXQoGe9r8L0s",
        "outputId": "b07c7383-e122-479c-9ade-9360843991f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "494032768"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# concept and shape explanation\n",
        "\n",
        "# 1) Tokenize -> Forward Pass| Logits -> \"Last Prediction\" ->[Softmax] and \"Do what you want\"\n",
        "\n",
        "prompt = \"Where McAllen is located\"\n",
        "inputs = tokenizer(prompt,return_tensors=\"pt\")\n",
        "\n",
        "print(inputs)\n",
        "\n",
        "for token in inputs[\"input_ids\"]:\n",
        "    print(tokenizer.decode(token),sep = \"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMzax1Pj8XZC",
        "outputId": "fb4d4584-aa06-4b54-91ff-2c472fe5741c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[ 9064,  4483, 79877,   374,  7407]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}\n",
            "Where McAllen is located\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# or for better visualisation\n",
        "\n",
        "prompt = \"Where McAllen is located\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "print(inputs)\n",
        "print(f\"Shape: {inputs['input_ids'].shape}\")  # [1, 5]\n",
        "print(\"\\nIndividual tokens:\")\n",
        "\n",
        "for i, token_id in enumerate(inputs[\"input_ids\"][0]):  # [0] to get first batch item\n",
        "    decoded = tokenizer.decode([token_id])  # decode single token\n",
        "    print(f\"Position {i}: token_id={token_id.item():5d} → '{decoded}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-G4bIWqG92aX",
        "outputId": "cb75150c-7ffd-4bbb-d263-34fb49e1f82f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[ 9064,  4483, 79877,   374,  7407]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}\n",
            "Shape: torch.Size([1, 5])\n",
            "\n",
            "Individual tokens:\n",
            "Position 0: token_id= 9064 → 'Where'\n",
            "Position 1: token_id= 4483 → ' Mc'\n",
            "Position 2: token_id=79877 → 'Allen'\n",
            "Position 3: token_id=  374 → ' is'\n",
            "Position 4: token_id= 7407 → ' located'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# so we have [1,5] or [B,T]\n",
        "# where B is a batch size (how many sequences process at once)\n",
        "# where T is a sequence length (number of tokens in your prompt)\n",
        "\n",
        "\n",
        "# let's focus on single prompt then we expand our example\n",
        "# so we pass our inputs into model and model will return outputs with\n",
        "# prob for each token position\n",
        "\n",
        "# [B,T,V] where V is a vocabulary size, Embedding(151936, 896)\n",
        "\n",
        "# or V is every possible token the model knows\n"
      ],
      "metadata": {
        "id": "w2p9phgC-eSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    out = model(**inputs)\n",
        "    logits = out.logits\n",
        "\n",
        "print(logits.shape)\n",
        "\n",
        "# so please notice model produces probabilities for each token in the sequence\n",
        "# including the one you provided yourself"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0EDV0cKB6ux",
        "outputId": "a5e6640a-7647-4db3-ae66-804b8eb67cd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 5, 151936])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# so since we are intersted into prediction of next token,\n",
        "# we will access the prediction for the last token\n",
        "\n",
        "# so notice afer we extract last token predictions we will get [1,V], because\n",
        "# we are \"predicting\"\n",
        "\n",
        "logits_last = out.logits[:,-1,:] # so we are saying in first dimension all ,\n",
        "# even though techincally we have only one,\n",
        "# in second last from the end\n",
        "# and in third obviously all predictions\n",
        "\n",
        "\n",
        "print(logits_last.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xxlX77J0Dkrv",
        "outputId": "2aafd952-4281-4877-89e6-017f1a5bbff9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 151936])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logits_last[:10] # notice it's not really a probability it's raw logits output\n",
        "# and also notice it's random 10 token\n",
        "# most likely special characters"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5qUP0Mn0Fnvt",
        "outputId": "6fbcae33-5cb5-4c8b-a2c5-ea784c478f40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[13.0130,  8.7147,  8.5017,  ..., -3.1183, -3.1183, -3.1184]])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "    print(tokenizer.decode(i))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fLkUPq--FsGJ",
        "outputId": "6911d3dd-cb5d-474c-eb09-6f97a4884b48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!\n",
            "\"\n",
            "#\n",
            "$\n",
            "%\n",
            "&\n",
            "'\n",
            "(\n",
            ")\n",
            "*\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ok so basically since we are playing a probability game for next token prediction\n",
        "# let see the most 5 most likely tokens our model predicting next\n",
        "\n",
        "# Raw version 1\n",
        "\n",
        "top_k = 5\n",
        "top_logits,top_indices_logits = torch.topk(logits_last, k = top_k, dim= -1)"
      ],
      "metadata": {
        "id": "v4CDdBYYGmOV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_logits,top_indices_logits"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Byw8NRbWHfsy",
        "outputId": "59f3dffa-f314-4310-a1e3-8a052b1a057b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[19.6082, 18.2642, 18.2444, 17.3177, 17.1950]]),\n",
              " tensor([[ 11, 304, 374, 389, 198]]))"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# so let's do manualy\n",
        "manual_logits = logits_last[-1].tolist() # remove batch dimension convert to list Converts PyTorch tensor to Python list\n",
        "\n",
        "len(manual_logits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIDjiApdHh8p",
        "outputId": "7f3256d5-52a9-4333-98ae-08974027be60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "151936"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logits_with_idx = [(idx,value) for idx,value in enumerate(manual_logits)]\n"
      ],
      "metadata": {
        "id": "G6B5KJAYIFQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logits_sorted = sorted(logits_with_idx, key = lambda x:x[1]) # sort by value"
      ],
      "metadata": {
        "id": "7NKJ7Sx3IVD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logits_sorted[-5:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jmxvu8RsIasd",
        "outputId": "cdfda7a2-dc0c-4244-d193-7f7104785259"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(198, 17.194992065429688),\n",
              " (389, 17.317724227905273),\n",
              " (374, 18.244359970092773),\n",
              " (304, 18.264217376708984),\n",
              " (11, 19.608192443847656)]"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# so not surprisingly we have same top_k probabilities"
      ],
      "metadata": {
        "id": "ao2ctInvIdOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's see what are tokens value\n",
        "\n",
        "for token_idx,value in logits_sorted[-5:][::-1]:\n",
        "    print(token_idx,tokenizer.decode([token_idx]),\"======== > value\",value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AyTt2Aw0It2x",
        "outputId": "386fe513-10eb-41d9-e91a-c7b17d9109f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11 , ======== > value 19.608192443847656\n",
            "304  in ======== > value 18.264217376708984\n",
            "374  is ======== > value 18.244359970092773\n",
            "389  on ======== > value 17.317724227905273\n",
            "198 \n",
            " ======== > value 17.194992065429688\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for i in range(top_k):\n",
        "    token_id = top_indices_logits[0, i].item()\n",
        "    logit_value = top_logits[0, i].item()\n",
        "    token_text = tokenizer.decode([token_id])\n",
        "\n",
        "    print(f\"{i+1}. Token ID: {token_id:6d} | Logit: {logit_value:8.3f} | Text: '{token_text}'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "is58lJOtJDcQ",
        "outputId": "b4089141-d600-4ced-a41e-c40a7bb81894"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Token ID:     11 | Logit:   19.608 | Text: ','\n",
            "2. Token ID:    304 | Logit:   18.264 | Text: ' in'\n",
            "3. Token ID:    374 | Logit:   18.244 | Text: ' is'\n",
            "4. Token ID:    389 | Logit:   17.318 | Text: ' on'\n",
            "5. Token ID:    198 | Logit:   17.195 | Text: '\n",
            "'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ok so it's very hard to interpret this values as probability, and apply safely\n",
        "# safely techniques we are about to see\n",
        "# so the common procedure is to apply softmax  and get aka \"probability\"\n",
        "# fit nicely with cross-entropy loss function\n",
        "# https://www.youtube.com/watch?v=6ArSys5qHAU\n",
        "# as well softmax is a convinient activation function for backpropagation\n"
      ],
      "metadata": {
        "id": "QWPdI5htJaLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "logits_last = out.logits[:,-1,:]\n",
        "\n",
        "probs = F.softmax(logits_last,dim=-1)\n",
        "top_probs, top_indices_probs = torch.topk(probs, k=top_k, dim=-1)\n",
        "\n",
        "for i in range(top_k):\n",
        "    token_id = top_indices_probs[0, i].item()\n",
        "    probability = top_probs[0, i].item()\n",
        "    token_text = tokenizer.decode([token_id])\n",
        "\n",
        "    print(f\"{i+1}. Token ID: {token_id:6d} | Prob: {probability:.4f} ({probability*100:.2f}%) | Text: '{token_text}'\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(f\"✓ Total probability sums to: {probs.sum().item():.6f}\")\n",
        "print(\"=\" * 60)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q5trqoKTJ2xZ",
        "outputId": "d1347a08-ad23-4088-9bbe-a553acc37cd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Token ID:     11 | Prob: 0.4210 (42.10%) | Text: ','\n",
            "2. Token ID:    304 | Prob: 0.1098 (10.98%) | Text: ' in'\n",
            "3. Token ID:    374 | Prob: 0.1077 (10.77%) | Text: ' is'\n",
            "4. Token ID:    389 | Prob: 0.0426 (4.26%) | Text: ' on'\n",
            "5. Token ID:    198 | Prob: 0.0377 (3.77%) | Text: '\n",
            "'\n",
            "\n",
            "============================================================\n",
            "✓ Total probability sums to: 1.000071\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# so top 5 tokens are same in both versions(softmax preserves the order)\n",
        "\n",
        "# what is good about softmax values are between 0 and 1 and sum 1.0\n",
        "# so easy to interpret as probability\n",
        "# though it does much more\n",
        "\n",
        "# For fun to watch about softmax :))\n",
        "# https://www.youtube.com/watch?v=KpKog-L9veg\n",
        "# https://www.youtube.com/watch?v=ytbYRIN0N4g"
      ],
      "metadata": {
        "id": "hx2CdyttKPBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ok so now let's fork into two ideas\n",
        "# 1) generation till completion\n",
        "# 2) batch processing\n"
      ],
      "metadata": {
        "id": "mjYo2rpTLMOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "gVTI9nMCOBUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bUAOvDXMUE8Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
