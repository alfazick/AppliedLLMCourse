{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alfazick/AppliedLLMCourse/blob/main/ExtraTopic-function_call_finetune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "077d03a0-dad1-4f6c-b955-6c461f11657e",
      "metadata": {
        "id": "077d03a0-dad1-4f6c-b955-6c461f11657e"
      },
      "outputs": [],
      "source": [
        "# Lesson Learned: Use Transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80c28ee4-9856-42c0-a9c0-d17f8f8d4143",
      "metadata": {
        "id": "80c28ee4-9856-42c0-a9c0-d17f8f8d4143"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"UNSLOTH_DISABLE_GRADIENT_OFFLOAD\"] = \"1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "208bca22-b60f-4f1e-80b1-97beb7b66eac",
      "metadata": {
        "id": "208bca22-b60f-4f1e-80b1-97beb7b66eac",
        "outputId": "9a77ba69-4a00-43b3-b971-ce7ee8e8b840"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO 11-26 06:34:23 [__init__.py:216] Automatically detected platform cuda.\n",
            "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
            "NVIDIA GeForce RTX 5090\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "print(torch.cuda.get_device_name())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3dfabab-33dc-46ce-9221-c3297eef175f",
      "metadata": {
        "id": "b3dfabab-33dc-46ce-9221-c3297eef175f",
        "outputId": "1d427dd3-5555-434f-cbdd-289d87bd3e42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(12, 0)\n"
          ]
        }
      ],
      "source": [
        "import torch; print(torch.cuda.get_device_capability())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "074fb8f2-3b5f-46fd-9384-5434d8b7b14e",
      "metadata": {
        "id": "074fb8f2-3b5f-46fd-9384-5434d8b7b14e"
      },
      "outputs": [],
      "source": [
        "# ok so goal of my project to fine-tune the model\n",
        "# for function calling\n",
        "# 1) first direct\n",
        "# 2) if time allows introduce new special token, something like <tool>\n",
        "# 3) create evaluation, like start with simple hit or miss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f10a8f49-ad64-48ec-a214-67d2ee90b4a3",
      "metadata": {
        "id": "f10a8f49-ad64-48ec-a214-67d2ee90b4a3",
        "outputId": "fe38959a-20d5-4f69-efc8-f225e559d5b7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/unsloth/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malfazick\u001b[0m (\u001b[33maskarnlp\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.23.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/workspace/work/wandb/run-20251126_063432-c0wqwgut</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/askarnlp/function-calling-finetune/runs/c0wqwgut' target=\"_blank\">llama3.1-8b-xlam-v1</a></strong> to <a href='https://wandb.ai/askarnlp/function-calling-finetune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/askarnlp/function-calling-finetune' target=\"_blank\">https://wandb.ai/askarnlp/function-calling-finetune</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/askarnlp/function-calling-finetune/runs/c0wqwgut' target=\"_blank\">https://wandb.ai/askarnlp/function-calling-finetune/runs/c0wqwgut</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [huggingface_hub.inference, openai] in use.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import wandb\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "def setup_wandb(project_name: str, run_name: str):\n",
        "    api_key = os.getenv(\"WANDB_API_KEY\")\n",
        "    if not api_key:\n",
        "        raise EnvironmentError(\"WANDB_API_KEY is not set\")\n",
        "\n",
        "    wandb.login(key=api_key)\n",
        "\n",
        "    os.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\"\n",
        "    os.environ[\"WANDB_WATCH\"] = \"all\"\n",
        "    os.environ[\"WANDB_SILENT\"] = \"true\"\n",
        "\n",
        "    wandb.init(project=project_name, name=run_name)\n",
        "\n",
        "setup_wandb(project_name=\"function-calling-finetune\", run_name=\"llama3.1-8b-xlam-v1\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a22d5cd7-f05f-496f-8372-481c93cfc722",
      "metadata": {
        "id": "a22d5cd7-f05f-496f-8372-481c93cfc722"
      },
      "outputs": [],
      "source": [
        "#https://huggingface.co/settings/tokens\n",
        "load_dotenv()\n",
        "from huggingface_hub import login\n",
        "hf_token = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
        "if hf_token is None:\n",
        "    raise EnvironmentError(\"HUGGINGFACE_TOKEN is not set in the environment variables.\")\n",
        "login(hf_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd10bc95-3207-4c28-813d-b7319e3e116b",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "e05d656f5fdf4a6aa17fbfbda238bdd1"
          ]
        },
        "id": "fd10bc95-3207-4c28-813d-b7319e3e116b",
        "outputId": "15d1ad54-a730-44ce-c9ec-815261841e7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth 2025.11.2: Fast Llama patching. Transformers: 4.57.0. vLLM: 0.10.2.\n",
            "   \\\\   /|    NVIDIA GeForce RTX 5090. Num GPUs = 1. Max memory: 31.367 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 12.0. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33+aa7bc36.d20251112. FA2 = True]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e05d656f5fdf4a6aa17fbfbda238bdd1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import torch\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "max_seq_length = 2048     # Unsloth auto supports RoPE Scaling internally!\n",
        "dtype = None              # None for auto detection\n",
        "load_in_4bit = False      # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Meta-Llama-3.1-8B-Instruct\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    attn_implementation=\"flash_attention_2\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a385b368-203e-49d9-bd24-1acc81cae3b1",
      "metadata": {
        "id": "a385b368-203e-49d9-bd24-1acc81cae3b1",
        "outputId": "c23e89f8-f450-4293-fe89-a17a56c85c59"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth 2025.11.2 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
          ]
        }
      ],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,   # LoRA rank - suggested values: 8, 16, 32, 64, 128\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0,   # Supports any, but = 0 is optimized\n",
        "    bias=\"none\",      # Supports any, but = \"none\" is optimized\n",
        "    use_gradient_checkpointing=True,\n",
        "    random_state=3407,\n",
        "    use_rslora=False,   # Disable rank-sensitive LoRA for simpler tasks\n",
        "    loftq_config=None   # No LoftQ, for standard fine-tuning\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76cdc220-3421-47f6-b3d5-3eb8964e3291",
      "metadata": {
        "id": "76cdc220-3421-47f6-b3d5-3eb8964e3291",
        "outputId": "980a271d-258c-4d2e-d632-599df8a41c0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using a sample size of 1000 for fine-tuning.\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Loading the dataset\n",
        "dataset = load_dataset(\"Salesforce/xlam-function-calling-60k\", split=\"train\", token=hf_token)\n",
        "\n",
        "# Selecting a subset of 15K samples for fine-tuning\n",
        "dataset = dataset.select(range(1000))\n",
        "print(f\"Using a sample size of {len(dataset)} for fine-tuning.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc093b0a-a58f-419d-9493-2b32302d9afd",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "bc430e36bdfe4293ad37996139a6d312"
          ]
        },
        "id": "bc093b0a-a58f-419d-9493-2b32302d9afd",
        "outputId": "50bdf3f7-e3f1-4d74-bff6-87d3b8a989e2"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bc430e36bdfe4293ad37996139a6d312",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "# Initialize the tokenizer with the chat template and mapping\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"llama-3\",\n",
        "    mapping = {\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\"}, # ShareGPT style\n",
        "    map_eos_token = True,        # Maps <|im_end|> to <|eot_id|> instead\n",
        ")\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    convos = []\n",
        "\n",
        "    # Iterate through each item in the batch (examples are structured as lists of values)\n",
        "    for query, tools, answers in zip(examples['query'], examples['tools'], examples['answers']):\n",
        "        tool_user = {\n",
        "            \"content\": f\"You are a helpful assistant with access to the following tools or function calls. Your task is to produce a sequence of tools or function calls necessary to generate response to the user utterance. Use the following tools or function calls as required:\\n{tools}\",\n",
        "            \"role\": \"system\"\n",
        "        }\n",
        "        ques_user = {\n",
        "            \"content\": f\"{query}\",\n",
        "            \"role\": \"user\"\n",
        "        }\n",
        "        assistant = {\n",
        "            \"content\": f\"{answers}\",\n",
        "            \"role\": \"assistant\"\n",
        "        }\n",
        "        convos.append([tool_user, ques_user, assistant])\n",
        "\n",
        "    texts = [tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False) for convo in convos]\n",
        "    return {\"text\": texts}\n",
        "\n",
        "# Apply the formatting on dataset\n",
        "dataset = dataset.map(formatting_prompts_func, batched = True,)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97a45f14-8cf2-40dd-a894-e1ac68579b09",
      "metadata": {
        "id": "97a45f14-8cf2-40dd-a894-e1ac68579b09",
        "outputId": "ef80e648-80a5-43f2-e768-884fff0842bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "You are a helpful assistant with access to the following tools or function calls. Your task is to produce a sequence of tools or function calls necessary to generate response to the user utterance. Use the following tools or function calls as required:\n",
            "[{\"name\": \"live_giveaways_by_type\", \"description\": \"Retrieve live giveaways from the GamerPower API based on the specified type.\", \"parameters\": {\"type\": {\"description\": \"The type of giveaways to retrieve (e.g., game, loot, beta).\", \"type\": \"str\", \"default\": \"game\"}}}]<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Where can I find live giveaways for beta access and games?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "[{\"name\": \"live_giveaways_by_type\", \"arguments\": {\"type\": \"beta\"}}, {\"name\": \"live_giveaways_by_type\", \"arguments\": {\"type\": \"game\"}}]<|eot_id|>\n"
          ]
        }
      ],
      "source": [
        "print(dataset[0][\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2651aeb0-188b-4049-8edd-27ac94db8815",
      "metadata": {
        "id": "2651aeb0-188b-4049-8edd-27ac94db8815",
        "outputId": "06a378d4-36fc-4591-955c-1a8e5d32bafa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train: 900 samples\n",
            "Validation: 100 samples\n"
          ]
        }
      ],
      "source": [
        "# Split the dataset\n",
        "dataset = dataset.train_test_split(test_size=0.1, seed=3407)\n",
        "\n",
        "print(f\"Train: {len(dataset['train'])} samples\")\n",
        "print(f\"Validation: {len(dataset['test'])} samples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f95b169-adfb-4fba-81f4-f0e8a935441a",
      "metadata": {
        "id": "0f95b169-adfb-4fba-81f4-f0e8a935441a"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "args = TrainingArguments(\n",
        "        per_device_train_batch_size = 8,  # Controls the batch size per device\n",
        "        gradient_accumulation_steps = 2,  # Accumulates gradients to simulate a larger batch\n",
        "        warmup_steps = 5,\n",
        "        learning_rate = 2e-4,             # Sets the learning rate for optimization\n",
        "        num_train_epochs = 1,\n",
        "        fp16 = not torch.cuda.is_bf16_supported(),\n",
        "        bf16 = torch.cuda.is_bf16_supported(),\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,              # Regularization term for preventing overfitting\n",
        "        lr_scheduler_type = \"linear\",     # Chooses a linear learning rate decay\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"wandb\",              # Enables Weights & Biases (W&B) logging\n",
        "        logging_steps = 10,                # Sets frequency of logging to W&B\n",
        "        logging_strategy = \"steps\",       # Logs metrics at each specified step\n",
        "        save_strategy = \"epoch\",\n",
        "        eval_strategy=\"epoch\",\n",
        "        load_best_model_at_end = True,    # Loads the best model at the end\n",
        "        save_only_model = False           # Saves entire model, not only weights\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f06045c-dd57-47a6-8488-3ffe9ab1daf7",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "e44dab671b594f2cbc001246c8e7f493",
            "39cdda035807410e9e4e7fdc13d7df40"
          ]
        },
        "id": "9f06045c-dd57-47a6-8488-3ffe9ab1daf7",
        "outputId": "3b0a385a-98f6-41e0-d5d9-681419f9b788"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e44dab671b594f2cbc001246c8e7f493",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Unsloth: Tokenizing [\"text\"] (num_proc=124):   0%|          | 0/900 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "num_proc must be <= 100. Reducing num_proc to 100 for dataset of size 100.\n",
            "[datasets.arrow_dataset|WARNING]num_proc must be <= 100. Reducing num_proc to 100 for dataset of size 100.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "39cdda035807410e9e4e7fdc13d7df40",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Unsloth: Tokenizing [\"text\"] (num_proc=100):   0%|          | 0/100 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from trl import SFTTrainer\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    processing_class = tokenizer,\n",
        "    train_dataset = dataset[\"train\"],\n",
        "    eval_dataset=dataset[\"test\"],\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False,        # Can make training 5x faster for short sequences.\n",
        "    args = args\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bce2cc2-b601-43f2-8def-504f056ec5bf",
      "metadata": {
        "id": "2bce2cc2-b601-43f2-8def-504f056ec5bf",
        "outputId": "ef049861-44f9-4c71-c24e-f878ca695610"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU = NVIDIA GeForce RTX 5090. Max memory = 31.367 GB.\n",
            "15.158 GB of memory reserved.\n"
          ]
        }
      ],
      "source": [
        "# Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09dbf321-37ed-40d8-96ae-3ec988f88a93",
      "metadata": {
        "id": "09dbf321-37ed-40d8-96ae-3ec988f88a93"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62ffbdbc-50cc-47e5-81b5-8e610d1c4196",
      "metadata": {
        "id": "62ffbdbc-50cc-47e5-81b5-8e610d1c4196",
        "outputId": "d68068da-d4ca-448e-d249-f908dd4f3312"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 900 | Num Epochs = 1 | Total steps = 57\n",
            "O^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 2\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 2 x 1) = 16\n",
            " \"-____-\"     Trainable parameters = 41,943,040 of 8,072,204,288 (0.52% trained)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Will smartly offload gradients to save VRAM!\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='57' max='57' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [57/57 08:45, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.660500</td>\n",
              "      <td>0.626902</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: Not an error, but LlamaForCausalLM does not accept `num_items_in_batch`.\n",
            "Using gradient accumulation will be very slightly less accurate.\n",
            "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (outputs/checkpoint-57)... Done. 0.5s\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TrainOutput(global_step=57, training_loss=0.826810267933628, metrics={'train_runtime': 531.0377, 'train_samples_per_second': 1.695, 'train_steps_per_second': 0.107, 'total_flos': 3.256334555362099e+16, 'train_loss': 0.826810267933628, 'epoch': 1.0})\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>‚ñÅ</td></tr><tr><td>eval/runtime</td><td>‚ñÅ</td></tr><tr><td>eval/samples_per_second</td><td>‚ñÅ</td></tr><tr><td>eval/steps_per_second</td><td>‚ñÅ</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÜ‚ñá‚ñà‚ñà</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÇ‚ñÑ‚ñÖ‚ñá‚ñà‚ñà</td></tr><tr><td>train/grad_norm</td><td>‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÇ</td></tr><tr><td>train/learning_rate</td><td>‚ñà‚ñÜ‚ñÑ‚ñÉ‚ñÅ</td></tr><tr><td>train/loss</td><td>‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.6269</td></tr><tr><td>eval/runtime</td><td>9.7311</td></tr><tr><td>eval/samples_per_second</td><td>10.276</td></tr><tr><td>eval/steps_per_second</td><td>1.336</td></tr><tr><td>total_flos</td><td>3.256334555362099e+16</td></tr><tr><td>train/epoch</td><td>1</td></tr><tr><td>train/global_step</td><td>57</td></tr><tr><td>train/grad_norm</td><td>0.27689</td></tr><tr><td>train/learning_rate</td><td>3e-05</td></tr><tr><td>train/loss</td><td>0.6605</td></tr><tr><td>+4</td><td>...</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">llama3.1-8b-xlam-v1</strong> at: <a href='https://wandb.ai/askarnlp/function-calling-finetune/runs/c0wqwgut' target=\"_blank\">https://wandb.ai/askarnlp/function-calling-finetune/runs/c0wqwgut</a><br> View project at: <a href='https://wandb.ai/askarnlp/function-calling-finetune' target=\"_blank\">https://wandb.ai/askarnlp/function-calling-finetune</a><br>Synced 4 W&B file(s), 0 media file(s), 17 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20251126_063432-c0wqwgut/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from unsloth import unsloth_train\n",
        "\n",
        "trainer_stats = unsloth_train(trainer)\n",
        "print(trainer_stats)\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89bc8415-739e-4701-86b8-0c7fe724fe93",
      "metadata": {
        "id": "89bc8415-739e-4701-86b8-0c7fe724fe93",
        "outputId": "ae654d29-bcf5-4b6c-f3bd-7da305aeffdd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "531.0377 seconds used for training.\n",
            "8.85 minutes used for training.\n",
            "Peak reserved memory = 24.336 GB.\n",
            "Peak reserved memory for training = 9.178 GB.\n",
            "Peak reserved memory % of max memory = 77.585 %.\n",
            "Peak reserved memory for training % of max memory = 29.26 %.\n"
          ]
        }
      ],
      "source": [
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory         /max_memory*100, 3)\n",
        "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c7df939-42c7-4cf5-b0a2-eec6fd3d6ad0",
      "metadata": {
        "id": "0c7df939-42c7-4cf5-b0a2-eec6fd3d6ad0",
        "outputId": "d253392c-6b93-45c5-8fdc-c75772008e5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "You are a helpful assistant with access to the following tools or function calls. Your task is to produce a sequence of tools or function calls necessary to generate response to the user utterance. Use the following tools or function calls as required:\n",
            "[{\"name\": \"get_weather\", \"description\": \"Get current weather for a location\", \"parameters\": {\"location\": {\"description\": \"The city name\", \"type\": \"str\", \"default\": \"San Francisco\"}}}]<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "What's the weather in Tokyo?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "[{\"name\": \"get_weather\", \"arguments\": {\"location\": \"Tokyo\"}}]<|eot_id|>\n"
          ]
        }
      ],
      "source": [
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "import json\n",
        "\n",
        "query = \"What's the weather in Tokyo?\"\n",
        "\n",
        "tools = [{\"name\": \"get_weather\", \"description\": \"Get current weather for a location\", \"parameters\": {\"location\": {\"description\": \"The city name\", \"type\": \"str\", \"default\": \"San Francisco\"}}}]\n",
        "\n",
        "chat = [\n",
        "    {\"role\": \"system\", \"content\": f\"You are a helpful assistant with access to the following tools or function calls. Your task is to produce a sequence of tools or function calls necessary to generate response to the user utterance. Use the following tools or function calls as required:\\n{json.dumps(tools)}\"},\n",
        "    {\"role\": \"user\", \"content\": query}\n",
        "]\n",
        "\n",
        "inputs = tokenizer.apply_chat_template(chat, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
        "outputs = model.generate(inputs, max_new_tokens=128, use_cache=True)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31808b23-e95e-4e5b-a187-2c79b24a26d0",
      "metadata": {
        "id": "31808b23-e95e-4e5b-a187-2c79b24a26d0",
        "outputId": "b269e363-6ae5-4bf0-aa3a-756b3d384919"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'query', 'answers', 'tools', 'text'],\n",
              "        num_rows: 900\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['id', 'query', 'answers', 'tools', 'text'],\n",
              "        num_rows: 100\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3dcf978c-10f1-465b-b327-9ea9514c3241",
      "metadata": {
        "id": "3dcf978c-10f1-465b-b327-9ea9514c3241",
        "outputId": "c512a29d-3be6-432d-95f6-7b2cdf3423b7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [03:05<00:00,  1.85s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Accuracy: 62/100 = 62.00%\n",
            "\n",
            "--- Sample Failures ---\n",
            "Q: Can you provide the hourly forecast for New York City tomorrow?...\n",
            "Expected: [{\"name\": \"get_hourly_forecast\", \"arguments\": {\"city\": \"New York City\"}}]...\n",
            "Got:      [{\"name\": \"get_hourly_forecast\", \"arguments\": {\"city\": \"New York\"}}]...\n",
            "\n",
            "Q: Can you provide the Stochastic RSI for AAPL stock over the last month using dail...\n",
            "Expected: [{\"name\": \"stochrsi\", \"arguments\": {\"symbol\": \"AAPL\", \"interval\": \"1day\", \"outpu...\n",
            "Got:      [{\"name\": \"stochrsi\", \"arguments\": {\"symbol\": \"AAPL\", \"interval\": \"1day\", \"fast_...\n",
            "\n",
            "Q: Fetch the treasure of the day from the Uncovered Treasure API....\n",
            "Expected: [{\"name\": \"today\", \"arguments\": {}}]...\n",
            "Got:      [{\"name\": \"today\", \"arguments\": {\"callback\": \"\"}}]...\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "def evaluate(dataset, max_samples=None):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    results = []\n",
        "\n",
        "    samples = dataset if max_samples is None else dataset.select(range(min(max_samples, len(dataset))))\n",
        "\n",
        "    for example in tqdm(samples):\n",
        "        # Build prompt from tools and query\n",
        "        chat = [\n",
        "            {\"role\": \"system\", \"content\": f\"You are a helpful assistant with access to the following tools or function calls. Your task is to produce a sequence of tools or function calls necessary to generate response to the user utterance. Use the following tools or function calls as required:\\n{example['tools']}\"},\n",
        "            {\"role\": \"user\", \"content\": example['query']}\n",
        "        ]\n",
        "\n",
        "        inputs = tokenizer.apply_chat_template(chat, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
        "        outputs = model.generate(inputs, max_new_tokens=256, use_cache=True, pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "        # Extract just the generated part\n",
        "        response = tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True).strip()\n",
        "\n",
        "        # Compare\n",
        "        expected = example['answers']\n",
        "\n",
        "        # Try parsing both as JSON for comparison\n",
        "        try:\n",
        "            pred_json = json.loads(response)\n",
        "            expected_json = json.loads(expected)\n",
        "            match = pred_json == expected_json\n",
        "        except:\n",
        "            match = response == expected\n",
        "\n",
        "        if match:\n",
        "            correct += 1\n",
        "\n",
        "        results.append({\n",
        "            \"query\": example['query'],\n",
        "            \"expected\": expected,\n",
        "            \"predicted\": response,\n",
        "            \"match\": match\n",
        "        })\n",
        "        total += 1\n",
        "\n",
        "    accuracy = correct / total * 100\n",
        "    print(f\"\\nAccuracy: {correct}/{total} = {accuracy:.2f}%\")\n",
        "\n",
        "    return accuracy, results\n",
        "\n",
        "# Run eval\n",
        "accuracy, results = evaluate(dataset['test'])\n",
        "\n",
        "# Show some failures\n",
        "print(\"\\n--- Sample Failures ---\")\n",
        "for r in results[:20]:\n",
        "    if not r['match']:\n",
        "        print(f\"Q: {r['query'][:80]}...\")\n",
        "        print(f\"Expected: {r['expected'][:80]}...\")\n",
        "        print(f\"Got:      {r['predicted'][:80]}...\")\n",
        "        print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e599264c-274b-4812-b011-58bae85f77dc",
      "metadata": {
        "id": "e599264c-274b-4812-b011-58bae85f77dc",
        "outputId": "4c2e25e9-7648-4b0e-ed93-18343cf65343"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [03:16<00:00,  1.97s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Results (n=100) ===\n",
            "JSON Parse Success: 97/100 (97.0%)\n",
            "Function Name Match: 95/100 (95.0%)\n",
            "Exact Match: 62/100 (62.0%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "def evaluate_flexible(dataset, max_samples=None):\n",
        "    metrics = {\"name_match\": 0, \"args_match\": 0, \"exact_match\": 0, \"parse_fail\": 0}\n",
        "    total = 0\n",
        "    results = []\n",
        "\n",
        "    samples = dataset if max_samples is None else dataset.select(range(min(max_samples, len(dataset))))\n",
        "\n",
        "    for example in tqdm(samples):\n",
        "        chat = [\n",
        "            {\"role\": \"system\", \"content\": f\"You are a helpful assistant with access to the following tools or function calls. Your task is to produce a sequence of tools or function calls necessary to generate response to the user utterance. Use the following tools or function calls as required:\\n{example['tools']}\"},\n",
        "            {\"role\": \"user\", \"content\": example['query']}\n",
        "        ]\n",
        "\n",
        "        inputs = tokenizer.apply_chat_template(chat, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
        "        outputs = model.generate(inputs, max_new_tokens=256, use_cache=True, pad_token_id=tokenizer.eos_token_id)\n",
        "        response = tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True).strip()\n",
        "\n",
        "        expected = example['answers']\n",
        "\n",
        "        try:\n",
        "            pred = json.loads(response)\n",
        "            exp = json.loads(expected)\n",
        "\n",
        "            # Normalize to list\n",
        "            if isinstance(pred, dict): pred = [pred]\n",
        "            if isinstance(exp, dict): exp = [exp]\n",
        "\n",
        "            # Function name match (most important)\n",
        "            pred_names = {p.get('name') for p in pred}\n",
        "            exp_names = {e.get('name') for e in exp}\n",
        "            name_match = pred_names == exp_names\n",
        "\n",
        "            # Arguments match (for matching functions)\n",
        "            args_match = pred == exp  # strict\n",
        "\n",
        "            if name_match: metrics[\"name_match\"] += 1\n",
        "            if args_match: metrics[\"args_match\"] += 1\n",
        "            if pred == exp: metrics[\"exact_match\"] += 1\n",
        "\n",
        "        except json.JSONDecodeError:\n",
        "            metrics[\"parse_fail\"] += 1\n",
        "            name_match = False\n",
        "            args_match = False\n",
        "\n",
        "        results.append({\"query\": example['query'], \"expected\": expected, \"predicted\": response, \"name_match\": name_match})\n",
        "        total += 1\n",
        "\n",
        "    print(f\"\\n=== Results (n={total}) ===\")\n",
        "    print(f\"JSON Parse Success: {total - metrics['parse_fail']}/{total} ({(total-metrics['parse_fail'])/total*100:.1f}%)\")\n",
        "    print(f\"Function Name Match: {metrics['name_match']}/{total} ({metrics['name_match']/total*100:.1f}%)\")\n",
        "    print(f\"Exact Match: {metrics['exact_match']}/{total} ({metrics['exact_match']/total*100:.1f}%)\")\n",
        "\n",
        "    return metrics, results\n",
        "\n",
        "metrics, results = evaluate_flexible(dataset['test'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd79ccb1-1d0b-4f72-a209-686908aabfac",
      "metadata": {
        "id": "dd79ccb1-1d0b-4f72-a209-686908aabfac",
        "outputId": "c4bf0e42-c3d5-4069-f1f1-8a2bde329a0b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [03:15<00:00,  1.95s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Results (n=100) ===\n",
            "JSON Parse: 96/100 (96.0%)\n",
            "Precision:  99.0%  (no extra calls)\n",
            "Recall:     99.0%  (no missing calls)\n",
            "F1:         99.0%\n",
            "Exact:      63/100 (63.0%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "def evaluate_detailed(dataset, max_samples=None):\n",
        "    metrics = {\"precision\": [], \"recall\": [], \"f1\": [], \"exact\": 0, \"parse_fail\": 0}\n",
        "    total = 0\n",
        "\n",
        "    samples = dataset if max_samples is None else dataset.select(range(min(max_samples, len(dataset))))\n",
        "\n",
        "    for example in tqdm(samples):\n",
        "        chat = [\n",
        "            {\"role\": \"system\", \"content\": f\"You are a helpful assistant with access to the following tools or function calls. Your task is to produce a sequence of tools or function calls necessary to generate response to the user utterance. Use the following tools or function calls as required:\\n{example['tools']}\"},\n",
        "            {\"role\": \"user\", \"content\": example['query']}\n",
        "        ]\n",
        "\n",
        "        inputs = tokenizer.apply_chat_template(chat, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
        "        outputs = model.generate(inputs, max_new_tokens=256, use_cache=True, pad_token_id=tokenizer.eos_token_id)\n",
        "        response = tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True).strip()\n",
        "\n",
        "        try:\n",
        "            pred = json.loads(response)\n",
        "            exp = json.loads(example['answers'])\n",
        "\n",
        "            if isinstance(pred, dict): pred = [pred]\n",
        "            if isinstance(exp, dict): exp = [exp]\n",
        "\n",
        "            pred_names = {p.get('name') for p in pred}\n",
        "            exp_names = {e.get('name') for e in exp}\n",
        "\n",
        "            # Precision: what % of predicted were correct?\n",
        "            if len(pred_names) > 0:\n",
        "                precision = len(pred_names & exp_names) / len(pred_names)\n",
        "            else:\n",
        "                precision = 0\n",
        "\n",
        "            # Recall: what % of expected were called?\n",
        "            if len(exp_names) > 0:\n",
        "                recall = len(pred_names & exp_names) / len(exp_names)\n",
        "            else:\n",
        "                recall = 1\n",
        "\n",
        "            # F1\n",
        "            if precision + recall > 0:\n",
        "                f1 = 2 * precision * recall / (precision + recall)\n",
        "            else:\n",
        "                f1 = 0\n",
        "\n",
        "            metrics[\"precision\"].append(precision)\n",
        "            metrics[\"recall\"].append(recall)\n",
        "            metrics[\"f1\"].append(f1)\n",
        "            if pred == exp: metrics[\"exact\"] += 1\n",
        "\n",
        "        except:\n",
        "            metrics[\"parse_fail\"] += 1\n",
        "\n",
        "        total += 1\n",
        "\n",
        "    valid = total - metrics[\"parse_fail\"]\n",
        "    print(f\"\\n=== Results (n={total}) ===\")\n",
        "    print(f\"JSON Parse: {valid}/{total} ({valid/total*100:.1f}%)\")\n",
        "    print(f\"Precision:  {sum(metrics['precision'])/len(metrics['precision'])*100:.1f}%  (no extra calls)\")\n",
        "    print(f\"Recall:     {sum(metrics['recall'])/len(metrics['recall'])*100:.1f}%  (no missing calls)\")\n",
        "    print(f\"F1:         {sum(metrics['f1'])/len(metrics['f1'])*100:.1f}%\")\n",
        "    print(f\"Exact:      {metrics['exact']}/{total} ({metrics['exact']/total*100:.1f}%)\")\n",
        "\n",
        "    return metrics\n",
        "\n",
        "metrics = evaluate_detailed(dataset['test'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "640e2790-ed55-4429-96a8-cf8655460a9c",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "6eba8c44828742a2ad9567fbc5c73cd3",
            "690d88de9316439194468cbe66c5d32e",
            "badf15f91f854700b378e5148346c117",
            "88a5260518954ca1abc311ea98ab853b",
            "cf41fd2e36d242218209764116c150fc",
            "204e76231a674a3388948cac13650dc9",
            "963cb7f46cee412c9e0fc37760446c9f",
            "d78c368960b1496ca4fa0956e1cf2b7c",
            "f635e34f5abf4d2c85d1c137cdea9adc",
            "2a419521ec9c43e78dc5426ab552557c"
          ]
        },
        "id": "640e2790-ed55-4429-96a8-cf8655460a9c",
        "outputId": "66841776-086d-4fca-e401-318219971f48"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6eba8c44828742a2ad9567fbc5c73cd3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "690d88de9316439194468cbe66c5d32e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "New Data Upload: |          |  0.00B /  0.00B            "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No existing and accessible Hugging Face cache directory found.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: Preparing safetensor model files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:41<00:00, 10.38s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: tokenizer.model not found (this is OK for non-SentencePiece models)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: Merging weights into 16bit:   0%|          | 0/4 [00:00<?, ?it/s]"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "badf15f91f854700b378e5148346c117",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "88a5260518954ca1abc311ea98ab853b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "New Data Upload: |          |  0.00B /  0.00B            "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: Merging weights into 16bit:  25%|‚ñà‚ñà‚ñå       | 1/4 [01:02<03:08, 62.92s/it]"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cf41fd2e36d242218209764116c150fc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "204e76231a674a3388948cac13650dc9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "New Data Upload: |          |  0.00B /  0.00B            "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: Merging weights into 16bit:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2/4 [02:07<02:08, 64.15s/it]"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "963cb7f46cee412c9e0fc37760446c9f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d78c368960b1496ca4fa0956e1cf2b7c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "New Data Upload: |          |  0.00B /  0.00B            "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: Merging weights into 16bit:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [03:11<01:03, 63.67s/it]"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f635e34f5abf4d2c85d1c137cdea9adc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2a419521ec9c43e78dc5426ab552557c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "New Data Upload: |          |  0.00B /  0.00B            "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: Merging weights into 16bit: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [03:25<00:00, 51.38s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Merge process complete. Saved to `/workspace/work/alfazick/llama-3.1-8b-function-calling`\n"
          ]
        }
      ],
      "source": [
        "model.push_to_hub_merged(\n",
        "    \"alfazick/llama-3.1-8b-function-calling\",\n",
        "    tokenizer,\n",
        "    save_method=\"merged_16bit\",\n",
        "    token=hf_token\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2989ad1-cf55-42fc-9b12-948793bb9abf",
      "metadata": {
        "id": "f2989ad1-cf55-42fc-9b12-948793bb9abf"
      },
      "outputs": [],
      "source": [
        "# conclusion: don't use unsloth, easier to use native TRL\n",
        "\"\"\"I used Unsloth initially, discovered their gradient offloading bottlenecks high-VRAM GPUs.\n",
        "Diagnosed with nvidia-smi ‚Äî 0% GPU, 90% CPU.\n",
        "The 'smart' offloading triggers regardless of available memory.\n",
        "Would use pure PEFT + Flash Attention 2 next time.\"\"\""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}