{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOAqJMgI3F1jvraocD5FRqq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alfazick/AppliedLLMCourse/blob/main/Module2HowTransformersClassifiersWork.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "WDjCbIA_aFGm"
      },
      "outputs": [],
      "source": [
        "# ok so general training loop always in place\n",
        "\n",
        "# step 1 Predict with whatever model you have\n",
        "# step 2 calculate the loss\n",
        "# step 3 calculate gradients for each param in your model\n",
        "# step 4 update your model params\n",
        "\n",
        "# keep doing for n_epochs\n",
        "\n",
        "# so this is core, like you\n",
        "# can think or wish many things\n",
        "# but at some point all 4 steps should happen\n",
        "# some of libs and code we look, just simplifies\n",
        "# the process by saving you from a dozen hundreds of code\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# so let's unpack all of this stages with hugging face transformers library\n",
        "\n",
        "# step #1 well we need a base model to start to work with\n",
        "# so we already know how to load the model, so no surprise\n",
        "# just pull the model out of box which is good for your task\n",
        "\n",
        "# https://huggingface.co/google-bert/bert-base-uncased\n",
        "\n",
        "# I like the next quote\n",
        "\"\"\"model is primarily aimed at being fine-tuned on tasks that use the whole sentence\n",
        "(potentially masked) to make decisions, such as sequence classification,\n",
        "token classification or question answering.\"\"\"\n",
        "\n",
        "# base model means, model was trained on vast amount of data\n",
        "# but not for your specific task, and our task project\n",
        "# will be [prompt injection detection]\n",
        "\n",
        "# but first let's explore model\n",
        "\n",
        "from transformers import BertTokenizer, BertModel\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BjSJ-8cqeo9J"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "THjFEUg9i5Dv",
        "outputId": "d1fcb5b0-bef6-4fc9-ef21-316cfc232a2f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertModel(\n",
              "  (embeddings): BertEmbeddings(\n",
              "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "    (position_embeddings): Embedding(512, 768)\n",
              "    (token_type_embeddings): Embedding(2, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): BertEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0-11): 12 x BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSdpaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): BertPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# so if you think about it produces a vector of tokens where each token dimension will\n",
        "# be 768\n",
        "\n",
        "text = \"UTRGV to the Moon\"\n",
        "encoded_input = tokenizer(text, return_tensors='pt')\n",
        "output = model(**encoded_input)"
      ],
      "metadata": {
        "id": "D9pEyBlFi9wv"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for k in encoded_input:\n",
        "    print(k,encoded_input[k])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQT2oxSWjsnN",
        "outputId": "cd046890-5b76-4a5e-dbdb-42d630f534f6"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_ids tensor([[  101, 21183, 10623,  2615,  2000,  1996,  4231,   102]])\n",
            "token_type_ids tensor([[0, 0, 0, 0, 0, 0, 0, 0]])\n",
            "attention_mask tensor([[1, 1, 1, 1, 1, 1, 1, 1]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " # so let's learn tokens or what you will fill the model with\n",
        " # it's like your x data point\n",
        "\n",
        "\n",
        "\n",
        "for token in encoded_input[\"input_ids\"]:\n",
        "    print(tokenizer.decode(token))\n",
        "\n",
        "# obvious thing to notice everything lowcased and [CLS] and [SEP] token"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJtYWNTTjvN0",
        "outputId": "3fdbdc1e-8ce5-4b09-8f0d-688546f3101e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLS] utrgv to the moon [SEP]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# so bert tokenizer adds [cls] and [sep] token\n",
        "# by default, since it's a data format it has seen during training\n",
        "\n",
        "ids = encoded_input[\"input_ids\"][0]\n",
        "tokens = tokenizer.convert_ids_to_tokens(ids)\n",
        "tokens\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fsj9tGO8qoRf",
        "outputId": "d6ce3396-dd48-4219-efa2-f6704cb14414"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[CLS]', 'ut', '##rg', '##v', 'to', 'the', 'moon', '[SEP]']"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " # ok if you look at output, this is really scary\n",
        "# because the question is how exactly I suppose to use\n",
        "# all of this \"random\" numbers to say classify something :))\n",
        "\n",
        "output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7gCEd3ikUJo",
        "outputId": "c86b61d7-0f7b-4e4f-ad8f-c12202c6463f"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.4504,  0.1165,  0.1604,  ..., -0.2071,  0.2120,  0.5255],\n",
              "         [-0.0643, -0.3624,  0.5675,  ..., -0.1871,  0.9949,  0.9705],\n",
              "         [-0.4801, -0.0094,  1.0636,  ..., -0.4680,  0.4697,  0.4389],\n",
              "         ...,\n",
              "         [-0.8320,  0.8765,  0.1836,  ...,  0.0842,  0.4684, -0.3084],\n",
              "         [-0.0536,  0.4925,  0.2465,  ..., -0.5793,  0.7340,  0.1992],\n",
              "         [ 0.6610,  0.1062, -0.2303,  ...,  0.0423, -0.5780, -0.1876]]],\n",
              "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-0.7441, -0.2182,  0.3439,  0.4367, -0.4348, -0.0834,  0.6760,  0.1969,\n",
              "          0.4367, -0.9996,  0.2893,  0.1764,  0.9726, -0.3716,  0.7386, -0.2185,\n",
              "          0.0171, -0.3646,  0.3363, -0.1930,  0.2781,  0.9404,  0.4904,  0.1351,\n",
              "          0.3006,  0.1527, -0.4112,  0.8065,  0.9004,  0.5889, -0.3778,  0.1981,\n",
              "         -0.9739, -0.0860,  0.1737, -0.9652,  0.1126, -0.5816,  0.1339,  0.0339,\n",
              "         -0.7281,  0.2869,  0.9937, -0.2560, -0.0918, -0.1749, -0.9976,  0.1008,\n",
              "         -0.7478, -0.3125, -0.4443, -0.3664,  0.1014,  0.2062,  0.2359,  0.3537,\n",
              "         -0.1710,  0.1056, -0.2174, -0.3319, -0.4121,  0.1826,  0.2564, -0.7637,\n",
              "         -0.2190, -0.5362,  0.0115, -0.1295,  0.1641, -0.2378,  0.5760,  0.0411,\n",
              "          0.3387, -0.6213, -0.4391,  0.1058, -0.1918,  1.0000, -0.2041, -0.9546,\n",
              "         -0.4676, -0.3237,  0.1405,  0.6285, -0.5052, -0.9997,  0.1761,  0.0038,\n",
              "         -0.9699,  0.1011,  0.1866,  0.0068, -0.6152,  0.1330,  0.1222, -0.0170,\n",
              "         -0.1556,  0.0946, -0.1125,  0.1786, -0.0740, -0.1013,  0.0609, -0.0963,\n",
              "          0.0145, -0.2584, -0.2251, -0.2176, -0.5155,  0.4239,  0.1144, -0.1438,\n",
              "          0.1435, -0.9205,  0.4462, -0.1340, -0.9712, -0.1663, -0.9684,  0.5241,\n",
              "          0.1576, -0.0955,  0.8400,  0.6144,  0.1767,  0.0824,  0.3750, -1.0000,\n",
              "         -0.0989, -0.0310,  0.4449, -0.0593, -0.9608, -0.8926,  0.4975,  0.8810,\n",
              "         -0.0161,  0.9918, -0.1409,  0.8498,  0.3709,  0.0568, -0.2926, -0.1970,\n",
              "          0.1672, -0.0358, -0.4320,  0.1425,  0.2284,  0.0433, -0.0230, -0.2369,\n",
              "          0.3943, -0.8160, -0.2700,  0.8220,  0.3192,  0.4415,  0.7104, -0.1027,\n",
              "         -0.2339,  0.7051,  0.0043,  0.2108,  0.0996,  0.3027, -0.3673,  0.3389,\n",
              "         -0.7170,  0.2188,  0.2400, -0.1478,  0.3767, -0.9597, -0.1858,  0.2787,\n",
              "          0.9619,  0.6405,  0.1777, -0.0440, -0.0283,  0.0852, -0.9219,  0.9585,\n",
              "         -0.0960,  0.1726,  0.4389, -0.0702, -0.7250, -0.5367,  0.5947,  0.0609,\n",
              "         -0.7467,  0.1314, -0.3231, -0.2341,  0.2057,  0.4417, -0.0593, -0.3140,\n",
              "          0.1848,  0.7801,  0.7750,  0.5043, -0.5691,  0.2651, -0.7583, -0.1872,\n",
              "         -0.0422,  0.1646,  0.0089,  0.9781, -0.0556, -0.0853, -0.7432, -0.9591,\n",
              "         -0.0393, -0.7465,  0.0969, -0.5723,  0.0685,  0.5080, -0.4855,  0.2031,\n",
              "         -0.8497, -0.4939,  0.1409, -0.2808,  0.2631, -0.1707,  0.5536, -0.2014,\n",
              "         -0.3022,  0.2905,  0.8374,  0.4169, -0.5419,  0.3643, -0.0868,  0.7633,\n",
              "         -0.4081,  0.9585, -0.1596,  0.2685, -0.8398,  0.2756, -0.5995,  0.3304,\n",
              "          0.0285, -0.6363, -0.1569,  0.1299,  0.1849,  0.7995, -0.4489,  0.9536,\n",
              "         -0.4971, -0.9059, -0.1826,  0.0466, -0.9720, -0.1347,  0.1413, -0.4652,\n",
              "         -0.2317, -0.2380, -0.8811,  0.6331,  0.0448,  0.9195,  0.2913, -0.7275,\n",
              "         -0.0776, -0.8482, -0.1843,  0.1086,  0.6102, -0.1437, -0.8414,  0.3015,\n",
              "          0.2530,  0.2347,  0.5120,  0.9653,  0.9975,  0.9411,  0.7147,  0.6773,\n",
              "         -0.8541, -0.3172,  0.9997, -0.3647, -0.9996, -0.8035, -0.3547,  0.1038,\n",
              "         -1.0000, -0.0877,  0.0589, -0.8172, -0.4368,  0.9602,  0.8543, -1.0000,\n",
              "          0.7347,  0.8450, -0.2437,  0.0532, -0.0041,  0.9467,  0.0830,  0.1978,\n",
              "         -0.1500,  0.2460,  0.1925, -0.6752,  0.3807,  0.3139,  0.5926,  0.0448,\n",
              "         -0.5625, -0.8111, -0.1158, -0.0829, -0.3379, -0.8892, -0.0929, -0.0933,\n",
              "          0.3640, -0.0757,  0.1183, -0.3904,  0.0613, -0.6625,  0.0388,  0.3395,\n",
              "         -0.8930, -0.3552,  0.3670, -0.2422,  0.3692, -0.9076,  0.9106, -0.2227,\n",
              "         -0.1944,  1.0000,  0.1263, -0.6501,  0.0448, -0.0262,  0.2748,  0.9999,\n",
              "          0.2976, -0.9562, -0.2022,  0.2889, -0.2437, -0.1475,  0.9934, -0.0463,\n",
              "          0.4108,  0.4787,  0.9582, -0.9765,  0.3543, -0.7662, -0.9162,  0.9105,\n",
              "          0.8454,  0.0032, -0.5474, -0.0878,  0.2845,  0.0872, -0.7753,  0.2938,\n",
              "          0.1151, -0.0378,  0.7389, -0.5887, -0.1407,  0.2205, -0.0340,  0.2452,\n",
              "         -0.0087,  0.2863, -0.1358, -0.1007, -0.1207, -0.0987, -0.9028, -0.1401,\n",
              "          0.9999,  0.0518, -0.2640,  0.0247,  0.0200, -0.4147,  0.1772,  0.2930,\n",
              "         -0.1209, -0.6840, -0.1700, -0.6105, -0.9687,  0.4185,  0.1007, -0.1862,\n",
              "          0.9866,  0.1865,  0.1680, -0.1087,  0.2044, -0.0832,  0.2339, -0.5169,\n",
              "          0.9468, -0.1737,  0.2183,  0.4895,  0.3705, -0.3072, -0.3922, -0.0833,\n",
              "         -0.8727,  0.0841, -0.8974,  0.9012, -0.3616,  0.1077, -0.0232, -0.0036,\n",
              "          0.9999, -0.3875,  0.3976,  0.0276,  0.4404, -0.8631, -0.3167, -0.2028,\n",
              "          0.1024,  0.3366, -0.1343,  0.0228, -0.9436, -0.4732, -0.2308, -0.7575,\n",
              "         -0.9746,  0.4417,  0.3789,  0.0291, -0.5031, -0.4158, -0.3686,  0.0202,\n",
              "          0.0641, -0.8643,  0.5882, -0.0668,  0.3444, -0.1182,  0.2059, -0.4009,\n",
              "          0.8458,  0.4087,  0.2173, -0.0221, -0.5879,  0.5512, -0.5687,  0.3069,\n",
              "         -0.0133,  1.0000, -0.2777, -0.4369,  0.5893,  0.4712, -0.0507,  0.0576,\n",
              "         -0.1605,  0.2285,  0.5027,  0.5191, -0.2332, -0.2338,  0.2269, -0.5434,\n",
              "         -0.4808,  0.5368, -0.0110,  0.0012,  0.0995, -0.0075,  0.9670, -0.0676,\n",
              "         -0.0696, -0.1263, -0.0289, -0.1943, -0.1508,  0.9997,  0.2116, -0.1857,\n",
              "         -0.9800,  0.3898, -0.7981,  0.9936,  0.7238, -0.7009,  0.3328,  0.3016,\n",
              "         -0.0922,  0.3856, -0.1359, -0.0695,  0.1310, -0.0697,  0.9252, -0.2209,\n",
              "         -0.9318, -0.4261,  0.1287, -0.8481,  0.8954, -0.3295, -0.1315, -0.1268,\n",
              "          0.2332,  0.0871, -0.1104, -0.9284, -0.0927, -0.0198,  0.9164,  0.0093,\n",
              "         -0.2438, -0.8002, -0.3808, -0.1494,  0.4030, -0.8664,  0.9399, -0.9148,\n",
              "          0.2629,  0.9995,  0.1889, -0.5610,  0.0325, -0.1750,  0.0038, -0.0605,\n",
              "          0.2094, -0.8315, -0.0413, -0.0747,  0.2257, -0.1235, -0.2729,  0.5840,\n",
              "          0.0326, -0.1600, -0.4079,  0.0873,  0.2546,  0.6175, -0.1795, -0.0173,\n",
              "          0.0474, -0.0104, -0.7432, -0.1973, -0.0712, -0.9668,  0.3819, -1.0000,\n",
              "         -0.2635, -0.6275, -0.0594,  0.6837,  0.0613, -0.1334, -0.4665,  0.4912,\n",
              "          0.7954,  0.5181, -0.0386,  0.1762, -0.4362,  0.0204,  0.0348,  0.2853,\n",
              "          0.1165,  0.5714, -0.0451,  1.0000, -0.0746, -0.2605, -0.7966,  0.1347,\n",
              "         -0.2064,  0.9980, -0.5701, -0.9161,  0.1341, -0.2512, -0.6715,  0.0625,\n",
              "         -0.0669, -0.4648, -0.1118,  0.7482,  0.4516, -0.2170,  0.2120, -0.1668,\n",
              "         -0.1287,  0.0152, -0.3751,  0.9709, -0.0212,  0.6889,  0.2528,  0.1648,\n",
              "          0.8897,  0.1682,  0.3905,  0.0595,  0.9996,  0.1295, -0.8697,  0.3500,\n",
              "         -0.9226, -0.0660, -0.8685,  0.0944, -0.0543,  0.7960, -0.1044,  0.9049,\n",
              "          0.4228, -0.0522,  0.1372,  0.5975,  0.1912, -0.8104, -0.9665, -0.9775,\n",
              "          0.2976, -0.1974,  0.0667,  0.2082,  0.0341,  0.2094,  0.2400, -0.9997,\n",
              "          0.8830,  0.0756, -0.3972,  0.9329,  0.1962,  0.2549,  0.1486, -0.9673,\n",
              "         -0.7343, -0.2004, -0.1792,  0.6108,  0.4892,  0.7268,  0.2412, -0.3050,\n",
              "          0.0638,  0.6165, -0.4682, -0.9817,  0.2919,  0.3636, -0.8131,  0.9156,\n",
              "         -0.5236,  0.0675,  0.5592,  0.1678,  0.6808,  0.6298,  0.3519,  0.0348,\n",
              "          0.4099,  0.7292,  0.8139,  0.9800,  0.2155,  0.3711,  0.3288,  0.1552,\n",
              "          0.6266, -0.8854, -0.0203, -0.1524,  0.0863,  0.1796, -0.1568, -0.8057,\n",
              "          0.5709, -0.1010,  0.2841, -0.1916,  0.1410, -0.2730,  0.0637, -0.5535,\n",
              "         -0.3191,  0.2547, -0.1116,  0.8412,  0.2596,  0.0489, -0.3351, -0.0721,\n",
              "          0.4673, -0.8401,  0.6542,  0.0577,  0.5495, -0.4542, -0.1704,  0.6913,\n",
              "         -0.3752, -0.2250, -0.0894, -0.6029,  0.5683, -0.1110, -0.2774, -0.3557,\n",
              "          0.4966,  0.1608,  0.9505,  0.2666,  0.2770, -0.0519, -0.0673,  0.1006,\n",
              "         -0.1825, -0.9996,  0.2169,  0.3218, -0.2228,  0.2332, -0.3075,  0.1752,\n",
              "         -0.8892, -0.1017, -0.0622, -0.3769, -0.4425, -0.2430,  0.1230,  0.1944,\n",
              "          0.0606,  0.7594,  0.2001,  0.6633,  0.1907,  0.4228, -0.4889,  0.6873]],\n",
              "       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# so model returns two things\n",
        "\n",
        "hidden_state = output.last_hidden_state\n",
        "pooled = output.pooler_output\n",
        "print(hidden_state.shape)     # torch.Size([1, 8, 768])  one 768-d vector per token\n",
        "print(pooled.shape) # torch.Size([1, 768])  one 768-d vector for the whole sequence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pL2r2F7KrUk4",
        "outputId": "dd720be3-f0a0-4e32-df70-16cb566a23b7"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 8, 768])\n",
            "torch.Size([1, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.config.hidden_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_-yY9nF1l1q",
        "outputId": "61506e6e-0b49-40dc-81a6-0a63b2aab394"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "768"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "# so like output has our prediction, but before training let's try to understand\n",
        "\n",
        "# so mainly there are two approaches\n",
        "\n",
        "# 1) take a cls token, because it's believed it containes all\n",
        "# context about the input, some kind of compressed version of meaning\n",
        "\n",
        "# Grab the CLS vector (position 0 in last_hidden_state)\n",
        "cls = hidden_state[:, 0, :]     # shape [1, 768]\n",
        "print(\"CLS shape:\", cls.shape)\n",
        "\n",
        "# and now you treat this representation of a context as a input features!\n",
        "# when I first time saw I was like what ???\n",
        "# but it is what it is\n",
        "\n",
        "num_labels = 2\n",
        "id2label = {0:\"Negative\",1:\"Positive\"}\n",
        "# so here is a key idea you are adding extra head like\n",
        "# Linear layer\n",
        "features = model.config.hidden_size # or 768 as we saw before\n",
        "head = nn.Linear(features,num_labels)\n",
        "\n",
        "torch.manual_seed(13)\n",
        "nn.init.normal_(head.weight, mean=0.0, std=0.02)\n",
        "nn.init.zeros_(head.bias)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pypxZXdplWd8",
        "outputId": "493474a9-8e0e-4459-ecfa-eafa7cdb8f3b"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLS shape: torch.Size([1, 768])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([0., 0.], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ok so now we will do our forward pass apply softmax and get our predictions:))\n",
        "with torch.no_grad():\n",
        "    logits = head(cls) # now our output will have shape [1,2] right ?\n",
        "    probs = F.softmax(logits,-1)\n",
        "    pred_id = probs.argmax(-1).item()\n",
        "    pred_label = id2label[pred_id]\n",
        "\n",
        "\n",
        "print(\"probs: \", probs.squeeze(0).tolist())\n",
        "print(\"pred:  \", pred_label)\n",
        "\n",
        "# Important: this head is untrained, so the prediction is meaningless right now.\n",
        "# It just demonstrates how CLS → logits → probs works.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBTJPVCP1Q1d",
        "outputId": "63cb8139-e175-4102-b9a4-9420c8dc247b"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "probs:  [0.3979083001613617, 0.6020916104316711]\n",
            "pred:   Positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2 Ok now let's understand another strategy Mean  Pooling\n",
        "# almost same story you use all tokens instead of single CLS\n",
        "head_mean = nn.Linear(model.config.hidden_size, 2)  # 768 -> 2 classes\n",
        "\n",
        "nn.init.normal_(head_mean.weight,   mean=0.0, std=0.02); nn.init.zeros_(head_mean.bias)\n",
        "\n",
        "hs = hidden_state                # [1, T, 768]\n",
        "mask = encoded_input[\"attention_mask\"].unsqueeze(-1).float()  # [1, T, 1]\n",
        "mean = (hs * mask).sum(1) / mask.sum(1).clamp(min=1e-9)  # [1, 768]\n",
        "\n",
        "with torch.no_grad():\n",
        "    logits_m = head_mean(mean)        # [1, 2]\n",
        "    probs_m  = F.softmax(logits_m, -1)\n",
        "    pred_m   = probs_m.argmax(-1).item()\n",
        "\n",
        "print(\"\\n[Mean-pooled tokens]\")\n",
        "print(\"mean shape:\", tuple(mean.shape))\n",
        "print(\"probs:\", probs_m.squeeze(0).tolist())\n",
        "print(\"pred :\", id2label[pred_m])\n",
        "\n",
        "\n",
        "# again want to emphasize totally random results\n",
        "# since not Fine-Tuned"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JKN79BnB22A-",
        "outputId": "a136a4b6-ed1e-4596-d6ff-0ce17007504e"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Mean-pooled tokens]\n",
            "mean shape: (1, 768)\n",
            "probs: [0.4478999674320221, 0.5521000623703003]\n",
            "pred : Positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KeL2-3tW6lib"
      },
      "execution_count": 44,
      "outputs": []
    }
  ]
}