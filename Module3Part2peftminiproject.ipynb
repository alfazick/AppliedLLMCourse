{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM/RLXtHjDOJglIO1GmF9zR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
},

  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alfazick/AppliedLLMCourse/blob/main/Module3Part2peftminiproject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0STTcz4QvvZr",
        "outputId": "0a867d1d-e3dc-40de-9df2-40108498bfaa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max abs diff: 0.001708984375\n",
            "relative error: 1.7000663774524583e-06\n"
          ]
        }
      ],
      "source": [
        "# assotiative matrix Multiplication\n",
        "# how does the matrix trick work so that ŒîW = BA never gets materialized?\n",
        "\n",
        "# so imagine you have a matrix d = 4096\n",
        "# so full W will be shape of (4096,4096)\n",
        "\n",
        "# so let's say you choose a rank = 8\n",
        "\n",
        "# and now you have two matrixes\n",
        "# A (8*4096)\n",
        "# B (4096*8)\n",
        "\n",
        "# so input x will be the shape of hidden dimension like for example x = (1,4096)\n",
        "\n",
        "d = 4096\n",
        "r = 8\n",
        "\n",
        "import torch\n",
        "torch.manual_seed(97)\n",
        "\n",
        "d = 4096\n",
        "r = 8\n",
        "\n",
        "A = torch.randn(d, r)   # [4096,8] - Down Projection\n",
        "B = torch.randn(r, d )  # [8,4096] - Up Projection\n",
        "x = torch.randn(1, d)   # [1, 4096]\n",
        "\n",
        "# Original pretrained weight (full matrix, stored once)\n",
        "W0 = torch.randn(d, d)        # [4096, 4096]\n",
        "\n",
        "\n",
        "# build ŒîW explicitly, then multiply:\n",
        "DeltaW = A @ B          # [4096, 8] @ [8, 4096] -> [4096, 4096]  HUGE\n",
        "W_eff  = W0 + DeltaW          # [4096, 4096]\n",
        "\n",
        "h_naive = x @ W_eff           # [1, 4096]\n",
        "\n",
        "# --- MATRIX TRICK (what LoRA really does) ------------------------\n",
        "# use associativity:   x (B A)  =  (x B) A\n",
        "# LoRA trick: use associativity to avoid materializing DeltaW\n",
        "\n",
        "base  = x @ W0                #  [1, 4096] @ [4096, 4096] ‚Üí [1, 4096]  original model output\n",
        "\n",
        "z     = x @ A                 #  [1, 4096] @ [4096, 8] ‚Üí [1, 8]      compress to rank r\n",
        "delta = z @ B                 # # [1, 8] @ [8, 4096] ‚Üí [1, 4096] expand back to d\n",
        "h_trick = base + delta        # [1, 4096]\n",
        "\n",
        "diff = h_naive - h_trick\n",
        "rel_error = diff.norm() / h_naive.norm()\n",
        "\n",
        "print(\"max abs diff:\", diff.abs().max().item())\n",
        "print(\"relative error:\", rel_error.item())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# make sure to click on runtime\n",
        "# and change runtime\n",
        "# it should run at most in 3 min\n",
        "# with T4GPU\n",
        "\n",
        "# Option 1: Qwen2-0.5B-Instruct (500M params - smaller!)\n",
        "model_name = \"Qwen/Qwen2-0.5B-Instruct\"\n",
        "\n",
        "# Option 2: SmolLM2-360M-Instruct (360M params - TINY!)\n",
        "model_name = \"HuggingFaceTB/SmolLM2-360M-Instruct\""
      ],
      "metadata": {
        "id": "EmHaEXP9-YQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries\n",
        "!pip install -q transformers peft datasets accelerate\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "from datasets import Dataset\n",
        "\n",
        "# 1Ô∏è‚É£ LOAD AN INSTRUCT MODEL (has built-in chat template!)\n",
        "model_name = \"Qwen/Qwen2-0.5B-Instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "print(f\"üîπ Base model parameters: {base_model.num_parameters():,}\")\n",
        "print(f\"üîπ Has chat template: {tokenizer.chat_template is not None}\")\n",
        "\n",
        "# 2Ô∏è‚É£ CONFIGURE LORA\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM\n",
        ")\n",
        "\n",
        "# 3Ô∏è‚É£ APPLY LORA\n",
        "model = get_peft_model(base_model, lora_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# 4Ô∏è‚É£ CREATE DATASET WITH CHAT FORMAT\n",
        "train_data = [\n",
        "    {\n",
        "        \"messages\": [\n",
        "            {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n",
        "            {\"role\": \"assistant\", \"content\": \"Ahoy, matey! I be doin' fine!\"}\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"messages\": [\n",
        "            {\"role\": \"user\", \"content\": \"What's your name?\"},\n",
        "            {\"role\": \"assistant\", \"content\": \"They call me Captain Blackbeard!\"}\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"messages\": [\n",
        "            {\"role\": \"user\", \"content\": \"Where are you from?\"},\n",
        "            {\"role\": \"assistant\", \"content\": \"I hail from the seven seas!\"}\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"messages\": [\n",
        "            {\"role\": \"user\", \"content\": \"What do you do?\"},\n",
        "            {\"role\": \"assistant\", \"content\": \"I sail the ocean in search of treasure, arr!\"}\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"messages\": [\n",
        "            {\"role\": \"user\", \"content\": \"Tell me a joke\"},\n",
        "            {\"role\": \"assistant\", \"content\": \"Why don't pirates shower? They prefer to just wash up on shore, har har!\"}\n",
        "        ]\n",
        "    },\n",
        "] * 50  # More data\n",
        "\n",
        "# Format using the model's built-in chat template\n",
        "def format_chat(example):\n",
        "    text = tokenizer.apply_chat_template(\n",
        "        example[\"messages\"],\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=False\n",
        "    )\n",
        "    return {\"text\": text}\n",
        "\n",
        "dataset = Dataset.from_list(train_data)\n",
        "dataset = dataset.map(format_chat)\n",
        "\n",
        "# Show formatted example\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"üìù FORMATTED EXAMPLE (using built-in template):\")\n",
        "print(\"=\"*50)\n",
        "print(dataset[0][\"text\"])\n",
        "print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "# Tokenize with labels\n",
        "def tokenize_function(examples):\n",
        "    result = tokenizer(\n",
        "        examples[\"text\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=256\n",
        "    )\n",
        "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "    return result\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\", \"messages\"])\n",
        "\n",
        "# 5Ô∏è‚É£ TRAINING\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./lora-pirate\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=2,\n",
        "    learning_rate=2e-4,\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"no\",\n",
        "    report_to=\"none\",\n",
        "    fp16=True,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "# 6Ô∏è‚É£ TEST BEFORE TRAINING\n",
        "print(\"=\"*50)\n",
        "print(\"üîπ BEFORE FINE-TUNING:\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "test_messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Hello, how are you?\"}\n",
        "]\n",
        "# add_generation_prompt=True will properly add the assistant turn\n",
        "prompt = tokenizer.apply_chat_template(test_messages, tokenize=False, add_generation_prompt=True)\n",
        "print(f\"Full prompt:\\n{prompt}\\n\")\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "outputs = model.generate(**inputs, max_new_tokens=30, do_sample=False, pad_token_id=tokenizer.eos_token_id)\n",
        "response = tokenizer.decode(outputs[0][len(inputs['input_ids'][0]):], skip_special_tokens=True)\n",
        "print(f\"User: Hello, how are you?\")\n",
        "print(f\"Assistant: {response}\\n\")\n",
        "\n",
        "# 7Ô∏è‚É£ TRAIN!\n",
        "print(\"üî• Starting LoRA fine-tuning...\")\n",
        "trainer.train()\n",
        "\n",
        "# 8Ô∏è‚É£ TEST AFTER TRAINING\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"üîπ AFTER FINE-TUNING:\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_messages = [\n",
        "        {\"role\": \"user\", \"content\": \"Hello, how are you?\"}\n",
        "    ]\n",
        "    prompt = tokenizer.apply_chat_template(test_messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    outputs = model.generate(**inputs, max_new_tokens=30, do_sample=True, temperature=0.7, pad_token_id=tokenizer.eos_token_id)\n",
        "    response = tokenizer.decode(outputs[0][len(inputs['input_ids'][0]):], skip_special_tokens=True)\n",
        "    print(f\"User: Hello, how are you?\")\n",
        "    print(f\"Assistant: {response}\\n\")\n",
        "\n",
        "# 9Ô∏è‚É£ SAVE\n",
        "model.save_pretrained(\"./pirate-lora-adapter\")\n",
        "tokenizer.save_pretrained(\"./pirate-lora-adapter\")\n",
        "print(\"‚úÖ Saved LoRA adapter!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "2b51d07bde66445c9e9e0ca960ba46c5",
            "3b8d50fc96484ffbae41f7872d36f0fb",
            "3bb1690317334020b3cd8730171f4b32",
            "165ae31b54a1476fbe54a97efae4a2df",
            "6dd2a03bed6c4e878ff0f53641d0d0ce",
            "1547ee7e0b0e485596a89a758406accb",
            "392364a27c9e4c6f85bc6cc236d7cb07",
            "28821c8dc815447fa64c3998b47b3a16",
            "e2318918541e4a11953d75a43e9943bb",
            "6fd0a55e582d45739ad5ea0df49b983c",
            "3150d57a674c4b2dabca40872cf15cac",
            "87283b83187d427eba6b09cb506f8c8e",
            "262e936caa0a459bac4912b519a45201",
            "8abf78d04aca4ea5835de48ee4767bc5",
            "b169415b1b1c49df93a19fe71cd01df1",
            "27dbb388cb4249cf98b3f18b6733cc61",
            "4ff20acb300845f7b68fdab3889bd53b",
            "b00ee58394704fa791b843c806c852dc",
            "53c7cae1e4204f95b0911eba47debe0e",
            "72828d428af94a91810c1324aa7a4a56",
            "04f04998b8b3406ab33b0eca412060c0",
            "7b3128cd1abb4ba89635688278fc7f6d",
            "6fd482ce8dec4142acdc966153883732",
            "088a98c2b6d649769e4d7672e7471a95",
            "ba5810663bff4897914638591e5e6c16",
            "c1ae67d882c14287afe8d0810ee6beda",
            "67864ef00acb48c495d91ca387903413",
            "4e6256e00ded43d0819497cb236f4f5d",
            "026e99ed5ca2473a9797e739fe2bd97e",
            "fe85c0bef8454ac2ab5c0203026091d1",
            "5a2f7eff432d425891f343670067322c",
            "c283137cee494ce2876fd2eba43f86c6",
            "95f00beaeb414e179d8f055bc5c034ec",
            "6c46d55f242845c8a91d8fa1d773bd95",
            "956885b16abf4494b5efb81b46b72429",
            "d4a01212f3d24c5ba9d23dde5217533c",
            "35e9b1cf1338449daec425a224c84949",
            "a0a048c60da34b3eaff43ccd2143706e",
            "0a864a1cfd2e4c0d8df264aa4eae044f",
            "26d3c032dc2344d18a26794a0fba19a5",
            "ed0cc7f536b64685b4b65f6e1fa2ed9c",
            "750ef92085504c0b933ad387789f5395",
            "145ba66f18b4438f8bbc6c5e41c7bf5a",
            "9e42321d67b74e8a921b00ac182fea55",
            "bdf814dc06e343848a687de8eb611740",
            "7437114b53c1444a88f5ef8bf656e6c8",
            "64b541e7bfcf49eaa6901ef2cbe6d56d",
            "4aa67f9b818e4d5fb40d21045e0eb5c0",
            "b14ff57fcf0f44d2a802ce024fcbe85e",
            "8649e7e395a44706ab740c0a54d213a5",
            "072c15ba03a04c1eb3b61c9f57ffaca3",
            "05dbeff2a30e4c5c99cd3f9dd1f8049c",
            "270c17a14f1949d6a6686480c00b6e38",
            "6c5ca840747a4d1e9d693b3f61e4f7c6",
            "6568661e1fa4433cb1acae50c10c0a3b",
            "668b3ea7fed14f53879d4d51e09f8b20",
            "a78e628efae3416ebe1a8c678641fada",
            "4a0e8f3b8c1a490a80e43efd71e7e5c2",
            "f032f1927d44439593c085bfee51938f",
            "ed377e119a8f44a79ae34bbf7df0af74",
            "c975913a9bb64d888228ef62ddc252c0",
            "c5f0224994f44ecda688b461f0ff1126",
            "f47a8bc7b981435dad59e5875a9f6971",
            "82c8b14e46f4432289877f1d2a21e9f0",
            "3fadf420e1ff4fb69d131b0e5b3bce38",
            "b1c12aa2087642e0bb60abe9254b092f",
            "4e14dc3854d4432192b66eadb50c0944",
            "71c9766565b34c529c384d2e24615adf",
            "337fa7cd4bfa4538bd81e20789f412e7",
            "658d576304a34fd7a4cbd861a8880f5d",
            "86b67cb5f3e048ffaf6efa66867b9dba",
            "008883ba174c43b3bc7fc2ec6dc7f457",
            "18ffdedbfc974bacbdbb1e024a92884c",
            "a3fb13e862b64882bba7b1a980f9481b",
            "5302e8acc7b94dd7b14825e1d92d5c70",
            "7d6b503ffb914075b27b6e67f5002a37",
            "09bdfb5622654f7da80119f234db0338",
            "d0eea1cfb66a4de0a8837c604886a2c4",
            "5c6e9b15a97043d689afef60c7ecea4d",
            "d76c17f5734b459da27ef778cde62a00",
            "806739670caf42c080de93002f0c507c",
            "c2eb06bda28341a591d4c9ad67f29678",
            "c6531ae20a8740df80b07ef909c71584",
            "59fda6162b8b48f9a9e52ddbc6af08ae",
            "295f4fb119844d5d89fd48bcb2c33f4b",
            "1a6b2fcc687e4686b73eb6554d5b1db1",
            "ad372dc5579c4dd18db6a8a1e5f74910",
            "e68f75bd079c48f4b12ea24e2002e438",
            "232921d39c3a4f1b9a601e32936c3420",
            "c24bfcac787d4ad7b2d53506c06a823f",
            "f4bc7144c5dd4c6a85f3e5187c45c29f",
            "9ba0130d86f3482cb7c185516363e925",
            "ec78401f81dc483e9f4fcdab479e6b9f",
            "1227e5dd867f43d1b9d0ae204afe2db6",
            "8e7d0c821be14c24a007a23013db46ab",
            "2a9953bc4ab74ecfba4616419b0151f4",
            "bfa10171609b4099b66a7b7b12d29df3",
            "22ca0e588d35425f8d243da6b5d17d6d",
            "2f386b83473f4aa88561aef135e0e2bf"
          ]
        },
        "id": "c6NmgIW1yNHz",
        "outputId": "72e95f4a-0923-4520-d400-2737867850be"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2b51d07bde66445c9e9e0ca960ba46c5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "87283b83187d427eba6b09cb506f8c8e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6fd482ce8dec4142acdc966153883732"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6c46d55f242845c8a91d8fa1d773bd95"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/659 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bdf814dc06e343848a687de8eb611740"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/988M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "668b3ea7fed14f53879d4d51e09f8b20"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4e14dc3854d4432192b66eadb50c0944"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîπ Base model parameters: 494,032,768\n",
            "üîπ Has chat template: True\n",
            "trainable params: 2,162,688 || all params: 496,195,456 || trainable%: 0.4359\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/250 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d0eea1cfb66a4de0a8837c604886a2c4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "üìù FORMATTED EXAMPLE (using built-in template):\n",
            "==================================================\n",
            "<|im_start|>system\n",
            "You are a helpful assistant.<|im_end|>\n",
            "<|im_start|>user\n",
            "Hello, how are you?<|im_end|>\n",
            "<|im_start|>assistant\n",
            "Ahoy, matey! I be doin' fine!<|im_end|>\n",
            "\n",
            "==================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/250 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "232921d39c3a4f1b9a601e32936c3420"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4200450874.py:118: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "üîπ BEFORE FINE-TUNING:\n",
            "==================================================\n",
            "Full prompt:\n",
            "<|im_start|>system\n",
            "You are a helpful assistant.<|im_end|>\n",
            "<|im_start|>user\n",
            "Hello, how are you?<|im_end|>\n",
            "<|im_start|>assistant\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151645}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User: Hello, how are you?\n",
            "Assistant: I'm just a computer program, so I don't have feelings or emotions like humans do. How can I assist you today?\n",
            "\n",
            "üî• Starting LoRA fine-tuning...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='189' max='189' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [189/189 01:10, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>6.843700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.202700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.078400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.027000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.009800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.008800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.007700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.007400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.007300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.007200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.006500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.006900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>0.007400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.006900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.006700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>0.006900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>0.006600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>0.006700</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "üîπ AFTER FINE-TUNING:\n",
            "==================================================\n",
            "User: Hello, how are you?\n",
            "Assistant: Ahoy, matey! I be doin' fine!\n",
            "\n",
            "‚úÖ Saved LoRA adapter!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# üîü LOAD THE FINE-TUNED MODEL AND TEST WITH NEW PROMPTS\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üîÑ LOADING SAVED LORA ADAPTER...\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "from peft import PeftModel\n",
        "\n",
        "# Load base model\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"Qwen/Qwen2-0.5B-Instruct\",\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"./pirate-lora-adapter\")\n",
        "\n",
        "# Load LoRA adapter\n",
        "model = PeftModel.from_pretrained(base_model, \"./pirate-lora-adapter\")\n",
        "model.eval()\n",
        "\n",
        "print(\"‚úÖ Model loaded successfully!\\n\")\n",
        "\n",
        "# Test with DIFFERENT prompts (not in training data)\n",
        "print(\"=\"*70)\n",
        "print(\"üè¥‚Äç‚ò†Ô∏è TESTING WITH NEW PROMPTS:\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "test_prompts = [\n",
        "    \"How's the weather today?\",\n",
        "    \"What's your favorite color?\",\n",
        "    \"Can you help me with my homework?\",\n",
        "    \"What did you have for breakfast?\",\n",
        "    \"Tell me about your ship\",\n",
        "    \"What's the meaning of life?\",\n",
        "    \"Do you like cats or dogs?\",\n",
        "    \"What's your biggest treasure?\",\n",
        "]\n",
        "\n",
        "for user_prompt in test_prompts:\n",
        "    messages = [{\"role\": \"user\", \"content\": user_prompt}]\n",
        "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=50,\n",
        "            do_sample=True,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            repetition_penalty=1.1\n",
        "        )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0][len(inputs['input_ids'][0]):], skip_special_tokens=True)\n",
        "\n",
        "    print(f\"üë§ User: {user_prompt}\")\n",
        "    print(f\"üè¥‚Äç‚ò†Ô∏è Pirate: {response}\")\n",
        "    print(\"-\" * 70 + \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DfVkQg4vzuKB",
        "outputId": "87ce2c2b-0490-4d9d-d632-e19656236646"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "üîÑ LOADING SAVED LORA ADAPTER...\n",
            "======================================================================\n",
            "‚úÖ Model loaded successfully!\n",
            "\n",
            "======================================================================\n",
            "üè¥‚Äç‚ò†Ô∏è TESTING WITH NEW PROMPTS:\n",
            "======================================================================\n",
            "\n",
            "üë§ User: How's the weather today?\n",
            "üè¥‚Äç‚ò†Ô∏è Pirate: It's sunny and warm, in fact, it's even been a little chilly!\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "üë§ User: What's your favorite color?\n",
            "üè¥‚Äç‚ò†Ô∏è Pirate: I have an affinity for blue, particularly when it comes to painting my nails!\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "üë§ User: Can you help me with my homework?\n",
            "üè¥‚Äç‚ò†Ô∏è Pirate: Of course, I can! What subject is it?\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "üë§ User: What did you have for breakfast?\n",
            "üè¥‚Äç‚ò†Ô∏è Pirate: I had eggs, bacon, and toast for breakfast, bacon being the main ingredient!\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "üë§ User: Tell me about your ship\n",
            "üè¥‚Äç‚ò†Ô∏è Pirate: Ahoy, matey! I be Captain Blackbeard! Say, what do you think?\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "üë§ User: What's the meaning of life?\n",
            "üè¥‚Äç‚ò†Ô∏è Pirate: In Hinduism, the answer to this question is \"Brahma,\" which means \"Godhead.\"\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "üë§ User: Do you like cats or dogs?\n",
            "üè¥‚Äç‚ò†Ô∏è Pirate: I prefer cats to dogs!\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "üë§ User: What's your biggest treasure?\n",
            "üè¥‚Äç‚ò†Ô∏è Pirate: My biggest treasure is knowledge!\n",
            "----------------------------------------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# BONUS: Compare with base model (no LoRA)\n",
        "print(\"=\"*70)\n",
        "print(\"üìä COMPARISON: Fine-tuned vs Base Model\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "# Load base model without adapter\n",
        "base_model_only = AutoModelForCausalLM.from_pretrained(\n",
        "    \"Qwen/Qwen2-0.5B-Instruct\",\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "test_prompt = \"Hello, how are you?\"\n",
        "messages = [{\"role\": \"user\", \"content\": test_prompt}]\n",
        "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "# Base model response\n",
        "with torch.no_grad():\n",
        "    outputs_base = base_model_only.generate(**inputs, max_new_tokens=30, do_sample=True, temperature=0.7)\n",
        "    response_base = tokenizer.decode(outputs_base[0][len(inputs['input_ids'][0]):], skip_special_tokens=True)\n",
        "\n",
        "# Fine-tuned model response\n",
        "with torch.no_grad():\n",
        "    outputs_ft = model.generate(**inputs, max_new_tokens=30, do_sample=True, temperature=0.7)\n",
        "    response_ft = tokenizer.decode(outputs_ft[0][len(inputs['input_ids'][0]):], skip_special_tokens=True)\n",
        "\n",
        "print(f\"üë§ User: {test_prompt}\\n\")\n",
        "print(f\"ü§ñ Base Model: {response_base}\\n\")\n",
        "print(f\"üè¥‚Äç‚ò†Ô∏è Fine-tuned Model: {response_ft}\\n\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4zxyXtVt_lDf",
        "outputId": "2dc3c233-3dcb-4c73-f472-cc353d03a37b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "üìä COMPARISON: Fine-tuned vs Base Model\n",
            "======================================================================\n",
            "\n",
            "üë§ User: Hello, how are you?\n",
            "\n",
            "ü§ñ Base Model: I'm just a computer program, so I don't have feelings or emotions like humans do. However, I'm here to help you with any questions\n",
            "\n",
            "üè¥‚Äç‚ò†Ô∏è Fine-tuned Model: Ahoy, matey! I be doin' fine!\n",
            "\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ok so you know what to do right ?\n",
        "# in class asignment\n",
        "# create a web application with at least 2 different characters\n",
        "# finetune two lora adapters\n",
        "# allow user to change the characters and chat with it\n",
        "\n"
      ],
      "metadata": {
        "id": "ALxfOBGq_2Zh"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EEPNRs_aAAIC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
