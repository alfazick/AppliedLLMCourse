{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOtb/rEgSP1HKc6JIj2HEgM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alfazick/AppliedLLMCourse/blob/main/Module3part1LowRankAdaptation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rx9YbLDKwUcM"
      },
      "outputs": [],
      "source": [
        "# Plan LowRank Adaption\n",
        "\n",
        "# 1) Matrix Multiplication in NN\n",
        "\n",
        "\n",
        "# 2) Motivation: Why LoRA\n",
        "\n",
        "# 3) Idea of low rank matrices\n",
        "\n",
        "# Paper: https://arxiv.org/abs/2012.13255\n",
        "\"Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning\"\n",
        "\n",
        "# 4) Singular Value Decomposition SVD Intuition\n",
        "\n",
        "\n",
        "# 5) LoRA Paper - Connecting SVD to Practice\n",
        "# Paper: https://arxiv.org/abs/2106.09685\n",
        "\"LoRA: Low-Rank Adaptation of Large Language Models\"\n",
        "\n",
        "# 6) Choosing Rank r and alpha\n",
        "# 7) Implementation Trick, Why You don't need a FULL Matrix\n",
        "\n",
        "\n",
        "# 8) Practical Assignment: PEFT Library\n",
        "# https://huggingface.co/docs/peft/en/quicktour\n",
        "# a) fine tune using PEFT for one personality (get a book from gutenberg)\n",
        "# b) create a multi character web service where person can chooose\n",
        "# to which character to talk to, use larger base model, and switch adapters\n",
        "# under the hood\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Matrix Multiplication\n",
        "import torch\n",
        "\n",
        "# Set seed for reproducible results\n",
        "torch.manual_seed(97)\n",
        "\n",
        "# Setup\n",
        "x = torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0, 6.0]).reshape(1, 6)\n",
        "W = torch.randn(6, 4)\n",
        "\n",
        "# Method 1: Pure Python\n",
        "\n",
        "def matrix_multiply(A, B):\n",
        "    outer_row = len(A)\n",
        "    inner_col = len(A[0])\n",
        "    inner_row = len(B)\n",
        "    outer_col = len(B[0])\n",
        "\n",
        "\n",
        "    if inner_col != inner_row:\n",
        "        print(\"Internal Dimension should Match!!!\")\n",
        "        return None\n",
        "\n",
        "    inner_shared = inner_col\n",
        "    result = [[0 for _ in range(outer_col)] for _ in range(outer_row)]\n",
        "\n",
        "    for i in range(outer_row):\n",
        "        for j in range(outer_col):\n",
        "            for k in range(inner_shared):\n",
        "                result[i][j] += A[i][k] * B[k][j]\n",
        "\n",
        "    return result\n",
        "\n",
        "mat = matrix_multiply(x.tolist(), W.tolist())\n",
        "result_python = torch.tensor(mat)\n",
        "\n",
        "# Method 2: PyTorch @ operator\n",
        "result_pytorch = x @ W\n",
        "\n",
        "print(\"Pure Python:  \", result_python)\n",
        "print(\"PyTorch @:    \", result_pytorch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kewv9Gbw7n-F",
        "outputId": "9865315f-e4f7-42d2-8b1b-f86adcab62ed"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pure Python:   tensor([[25.5227, -5.0256,  2.5722, 10.9780]])\n",
            "PyTorch @:     tensor([[25.5227, -5.0256,  2.5722, 10.9780]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ok now let's look at Neural Network Linear Layer\n",
        "\n",
        "def matrix_multiply(A, B):\n",
        "    outer_row = len(A)\n",
        "    inner_col = len(A[0])\n",
        "    inner_row = len(B)\n",
        "    outer_col = len(B[0])\n",
        "\n",
        "\n",
        "    if inner_col != inner_row:\n",
        "        print(\"Internal Dimension should Match!!!\")\n",
        "        return None\n",
        "\n",
        "    inner_shared = inner_col\n",
        "    matrix = [[0 for _ in range(outer_col)] for _ in range(outer_row)]\n",
        "\n",
        "    for i in range(outer_row):\n",
        "        for j in range(outer_col):\n",
        "            for k in range(inner_shared):\n",
        "                matrix[i][j] += A[i][k] * B[k][j]\n",
        "\n",
        "    return matrix\n",
        "\n",
        "def add_bias(matrix,bias):\n",
        "    rows = len(matrix)\n",
        "    cols = len(matrix[0])\n",
        "\n",
        "    for r in range(rows):\n",
        "        for c in range(cols):\n",
        "            matrix[r][c] += bias[c]\n",
        "\n",
        "    return matrix\n",
        "\n",
        "def relu(matrix):\n",
        "    rows = len(matrix)\n",
        "    cols = len(matrix[0])\n",
        "\n",
        "    for r in range(rows):\n",
        "        for c in range(cols):\n",
        "            matrix[r][c] = max(0,matrix[r][c])\n",
        "\n",
        "    return matrix\n",
        "\n",
        "\n",
        "import random\n",
        "import math\n",
        "\n",
        "class ManualLinear:\n",
        "    def __init__(self,in_features,out_features,bias=True,seed=None):\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.bias_enabled = bias\n",
        "\n",
        "        rng = random.Random(seed) if seed is not None else random\n",
        "\n",
        "        # (simple version)\n",
        "        self.weight = [[0.0 for _ in range(out_features)]\n",
        "                       for _ in range(in_features)]\n",
        "        self.bias = [0.0 for _ in range(out_features)] if bias else None\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "        if isinstance(x[0], (int,float)):\n",
        "            x_batch = [x]\n",
        "            single_input = True\n",
        "        else:\n",
        "            x_batch = x\n",
        "            single_input = False\n",
        "\n",
        "        # x @ W :))\n",
        "        out = matrix_multiply(x_batch,self.weight)\n",
        "\n",
        "        if self.bias is not None:\n",
        "            out = add_bias(out,self.bias)\n",
        "\n",
        "        if single_input:\n",
        "            return out[0]\n",
        "        return out\n",
        "\n",
        "\n",
        "    __call__ = forward # so you can do layer(x) like in PyTorch\n",
        "\n",
        "    def __repr_(self):\n",
        "        return f\"ManualLinear(in_features={self.in_features}, out_features={self.out_features}, bias={self.bias is not None})\"\n",
        "\n"
      ],
      "metadata": {
        "id": "0b6XX82oBx7U"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layer = ManualLinear(in_features=6, out_features=4,bias=True)\n",
        "layer.weight = W.tolist()\n",
        "x_list = x.tolist()\n",
        "y_manual = layer(x_list)\n",
        "y_manual_tensor = torch.tensor(y_manual)\n",
        "\n",
        "print(\"y_manual matrix \",y_manual_tensor)\n",
        "\n",
        "\n",
        "linear = torch.nn.Linear(6,4,bias=True)\n",
        "\n",
        "with torch.no_grad():\n",
        "    linear.weight.copy_(W.T)\n",
        "    linear.bias.zero_()\n",
        "\n",
        "y_linear = linear(x)\n",
        "\n",
        "print(\"y_nn.linear.    \",y_linear)\n",
        "print(\"equal?\", torch.allclose(y_manual_tensor, y_linear))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pf2sQyPQ-YwH",
        "outputId": "fe985354-8ab4-4f9a-d3cd-4ed4a5f0a388"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y_manual matrix  tensor([[25.5227, -5.0256,  2.5722, 10.9780]])\n",
            "y_nn.linear.     tensor([[25.5227, -5.0256,  2.5722, 10.9780]], grad_fn=<AddmmBackward0>)\n",
            "equal? True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2) Motivation: Why LoRA\n",
        "# Why not full fintunning as you did?\n",
        "\n",
        "# Problem # 1 ) update billion of weights,\n",
        "# when like optimizer above just weights keeps even more parameters\n",
        "# for example AdamW\n",
        "\n",
        "# Problem # 2 ) rigid structure and heavy models\n",
        "# imagine you have multiple problems for which you want to finetune a model\n",
        "# well let's say you have enough resources to fine-tune them,\n",
        "# you still have a problem how quickly you can switch underlying model\n",
        "# at the time of inference ???\n",
        "# let's say you base 7 bln parameter, are you going to switch and reload\n",
        "# each of your fintuned models each time?\n",
        "\n",
        "# one of the creator of LoRA\n",
        "# https://www.youtube.com/watch?v=DhRoTONcyZE\n",
        "# explains motivation of task he was solving at Microsoft\n",
        "\n",
        "# Problem #3) \"Catastophing forgetting\"\n",
        "\n"
      ],
      "metadata": {
        "id": "BdNy4Vrj8kEB"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3) Idea of low rank matrices\n",
        "# in general the idea is that  a  big matrixes\n",
        "# can be represented as a product of two smaller matrices\n",
        "# without losing or minimal lose of information\n",
        "\n",
        "\n",
        "import torch\n",
        "torch.manual_seed(0)\n",
        "\n",
        "d, k = 100, 100\n",
        "r = 8   # inner dim (rank limit)\n",
        "\n",
        "# Two smaller matrices\n",
        "A = torch.randn(d, r)   # [100, 8]\n",
        "B = torch.randn(r, k)   # [8, 100]\n",
        "\n",
        "# Big matrix W built from A and B\n",
        "W = A @ B                # [100, 100]\n",
        "\n",
        "print(\"A shape:\", A.shape)\n",
        "print(\"B shape:\", B.shape)\n",
        "print(\"W shape:\", W.shape)\n",
        "print(\"W:\\n\", W)\n",
        "\n",
        "# so notice if matrix w is actual weights of model\n",
        "# and we can find two good smaller matrixes\n",
        "# we don't need to store 100*100 = 10000 elements\n",
        "# we cane just keep total 800 + 800 = 1600 elements\n",
        "# huge memory win right ???\n",
        "# ok now we are left with two questions\n",
        "# a) why this even should work for nn ?\n",
        "# b) where viability of this idea originates from ?"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOpNsQb_O4Ly",
        "outputId": "e96a864d-f9ac-4bfa-da4f-f6ef78a9ed2e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A shape: torch.Size([100, 8])\n",
            "B shape: torch.Size([8, 100])\n",
            "W shape: torch.Size([100, 100])\n",
            "W:\n",
            " tensor([[ 5.3685, -1.0868,  2.2387,  ..., -0.6155,  2.4315, -3.3885],\n",
            "        [ 3.1883,  0.5887,  2.6259,  ..., -0.9715,  0.0449, -0.7832],\n",
            "        [ 1.6745,  3.8639, -4.5289,  ...,  2.7822, -7.9605,  4.1718],\n",
            "        ...,\n",
            "        [-3.9455,  1.7957, -3.1901,  ...,  1.0807, -0.5203,  0.9297],\n",
            "        [ 1.7443, -3.5769,  0.9222,  ..., -2.4217,  3.5732, -2.7722],\n",
            "        [ 0.1791,  1.8260,  1.1715,  ..., -0.4111, -1.5972,  1.5057]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# a) from paper\n",
        "# https://arxiv.org/pdf/2012.13255\n",
        "# I love when author write like this\n",
        "\"\"\"Why can we use relatively vanilla gradient descent algorithms (e.g., without strong regularization)\n",
        "# to tune a model with hundreds of millions of parameters on datasets\n",
        "# with only hundreds or thousands of labeled examples? \"\"\"\n",
        "\n",
        "# claim\n",
        "\n",
        "\"\"\"We empirically show that common pre-trained models have a very low intrinsic dimension;\n",
        "in other words, there exists a low dimension reparameterization that is\n",
        "as effective for fine-tuning as the full parameter space.\"\"\"\n",
        "\n",
        "# so try to connect to our above example like\n",
        "# if matrix w is low rank, it means instead of storing all weights\n",
        "# we can store the weights in form of two smaller matrices\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "nFlu7IG1Sj3X",
        "outputId": "8fe233c6-d4e1-4937-c2dd-e2e80b26ac6c"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'We empirically show that common pre-trained models have a very low intrinsic dimension; \\nin other words, there exists a low dimension reparameterization that is\\nas effective for fine-tuning as the full parameter space.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4) Singular Value Decomposition SVD Intuition\n",
        "# https://www.youtube.com/watch?v=02QCtHM1qb4&list=PLMrJAkhIeNNSVjnsviglFoY2nXildDCcv\n",
        "# :) videos to watch to fall asleep, I called hibernating learning\n",
        "\n",
        "\n",
        "# ok so now we are left with question like\n",
        "# how those two smaller matrixes should look like?\n",
        "# and funny enough there is an algo to find exactly it\n",
        "\n",
        "# SVD = Singular Value Decomposition\n",
        "\n",
        "U,S,Vh = torch.linalg.svd(W)\n",
        "print(\"U shape:\", U.shape)    # [100,100]\n",
        "print(\"S shape:\", S.shape)    # [100]\n",
        "print(\"Vh shape:\", Vh.shape)  # [100,100]\n",
        "\n",
        "# so here comes our matrices A,B from SVD\n",
        "\n",
        "r = 8 #\n",
        "\n",
        "U_r = U[:,:r] # [100,r]\n",
        "S_r = S[:r] # [r]\n",
        "Vh_r = Vh[:r,:] # [r,100]\n",
        "\n",
        "A_svd = U_r * S_r\n",
        "B_svd = Vh_r\n",
        "\n",
        "W_approx = A_svd @ B_svd\n",
        "\n",
        "# How close is this rank-8 approximation to original W?\n",
        "rel_error = (W - W_approx).norm() / W.norm()\n",
        "print(\"rank-8 relative error:\", rel_error.item())\n",
        "print(\"A_svd shape:\", A_svd.shape)\n",
        "print(\"B_svd shape:\", B_svd.shape)\n",
        "\n",
        "# so from results you can see that SVD is literally an alogorithm\n",
        "# to construct good \"two smaller matrices\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_bNKMlaXZ3y",
        "outputId": "33e052e1-248f-43c7-98b3-6acba5599151"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "U shape: torch.Size([100, 100])\n",
            "S shape: torch.Size([100])\n",
            "Vh shape: torch.Size([100, 100])\n",
            "rank-8 relative error: 5.648137744174164e-07\n",
            "A_svd shape: torch.Size([100, 8])\n",
            "B_svd shape: torch.Size([8, 100])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5) LoRA Paper - Connecting SVD to Practice\n",
        "# Paper: https://arxiv.org/abs/2106.09685\n",
        "\"LoRA: Low-Rank Adaptation of Large Language Models\"\n",
        "\n",
        "# Ok now here is actual magic jumps behind LoRA\n",
        "# what if instead of computing SVD for full W matrix updates of gradients\n",
        "# we will directly learn A and B during fine-tuning :))\n",
        "# welcome to the world wheree everyone pushes into one direction\n",
        "\n",
        "\"\"\"We propose Low-Rank Adaptation, or LoRA, which freezes the pretrained\n",
        "model weights and injects trainable rank decomposition matrices into each\n",
        "layer of the Transformer architecture,\n",
        "greatly reducing the number of trainable parameters for downstream tasks\"\"\"\n",
        "\n",
        "# h = W0x + âˆ†W x = W0x + BAx # slow down and read out loud\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "eA7m154RZEIm",
        "outputId": "f90ee271-30f3-49e4-d6a8-717fa59c754b"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'We propose Low-Rank Adaptation, or LoRA, which freezes the pretrained \\nmodel weights and injects trainable rank decomposition matrices into each\\nlayer of the Transformer architecture, \\ngreatly reducing the number of trainable parameters for downstream tasks'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eOlZUL9RcDHw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}