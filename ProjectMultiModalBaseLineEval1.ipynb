{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alfazick/AppliedLLMCourse/blob/main/ProjectMultiModalBaseLineEval1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19da16a5-8508-4b7e-822b-fe4dc212df15",
      "metadata": {
        "id": "19da16a5-8508-4b7e-822b-fe4dc212df15"
      },
      "outputs": [],
      "source": [
        "# 21 November\n",
        "# Plan\n",
        "# 1) how IOU works and calculated and ok so today I want to understand\n",
        "\n",
        "# 2) run IOU on default dataset to prepare evaluation pipiline (score should be best right?)\n",
        "\n",
        "# 3) run inference on base model without fintuning and pass through evaluation pipeline\n",
        "\n",
        "# 4) try to quantize the base model and check if it will perform similarly\n",
        "\n",
        "# 5) prepare dataset for finetunung.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90248317-eeca-4fb1-aa03-42477ca6bcf8",
      "metadata": {
        "id": "90248317-eeca-4fb1-aa03-42477ca6bcf8",
        "outputId": "1e755b91-5277-458c-dc19-14b9bcb523e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: transformers 4.46.2\n",
            "Uninstalling transformers-4.46.2:\n",
            "  Successfully uninstalled transformers-4.46.2\n",
            "Found existing installation: trl 0.11.4\n",
            "Uninstalling trl-0.11.4:\n",
            "  Successfully uninstalled trl-0.11.4\n",
            "Found existing installation: datasets 3.0.2\n",
            "Uninstalling datasets-3.0.2:\n",
            "  Successfully uninstalled datasets-3.0.2\n",
            "Found existing installation: bitsandbytes 0.48.2\n",
            "Uninstalling bitsandbytes-0.48.2:\n",
            "  Successfully uninstalled bitsandbytes-0.48.2\n",
            "Found existing installation: peft 0.13.2\n",
            "Uninstalling peft-0.13.2:\n",
            "  Successfully uninstalled peft-0.13.2\n",
            "Found existing installation: qwen-vl-utils 0.0.8\n",
            "Uninstalling qwen-vl-utils-0.0.8:\n",
            "  Successfully uninstalled qwen-vl-utils-0.0.8\n",
            "Found existing installation: wandb 0.18.5\n",
            "Uninstalling wandb-0.18.5:\n",
            "  Successfully uninstalled wandb-0.18.5\n",
            "Found existing installation: accelerate 1.0.1\n",
            "Uninstalling accelerate-1.0.1:\n",
            "  Successfully uninstalled accelerate-1.0.1\n"
          ]
        }
      ],
      "source": [
        "# Clean slate and install stable versions\n",
        "!pip uninstall transformers trl datasets bitsandbytes peft qwen-vl-utils wandb accelerate -y\n",
        "\n",
        "!pip install transformers==4.46.2 \\\n",
        "             accelerate==1.0.1 \\\n",
        "             peft==0.13.2 \\\n",
        "             bitsandbytes>=0.45.0 \\\n",
        "             datasets==3.0.2 \\\n",
        "             trl==0.11.4 \\\n",
        "             qwen-vl-utils==0.0.8 \\\n",
        "             wandb==0.18.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22e56b99-060d-47e3-a524-22dece98a691",
      "metadata": {
        "id": "22e56b99-060d-47e3-a524-22dece98a691"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1d033f9-1367-49a2-b766-32b5ccb4577f",
      "metadata": {
        "id": "c1d033f9-1367-49a2-b766-32b5ccb4577f"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "803beac0-45ad-4a98-b5d4-9790589a34f7",
      "metadata": {
        "id": "803beac0-45ad-4a98-b5d4-9790589a34f7"
      },
      "outputs": [],
      "source": [
        "# Cell 2 - MUST restart kernel\n",
        "import os\n",
        "os._exit(0)  # This forces kernel restart"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ecf0963-f199-453c-9e64-fa29e6e17165",
      "metadata": {
        "id": "5ecf0963-f199-453c-9e64-fa29e6e17165"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128,expandable_segments:True\"\n",
        "os.environ[\"PYTORCH_NO_CUDA_MEMORY_CACHING\"] = \"1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19920b08-217d-458d-a94a-6ce817d75b2f",
      "metadata": {
        "id": "19920b08-217d-458d-a94a-6ce817d75b2f"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b26063e6-cfca-4828-9baf-3ab3c7dd4955",
      "metadata": {
        "id": "b26063e6-cfca-4828-9baf-3ab3c7dd4955",
        "outputId": "942f73c0-5e1d-4228-88e9-0d88fedbf913"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "alloc conf: max_split_size_mb:128,expandable_segments:True\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(\"alloc conf:\", os.environ.get(\"PYTORCH_CUDA_ALLOC_CONF\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60c48c43-9522-491d-9e0f-da4102908041",
      "metadata": {
        "id": "60c48c43-9522-491d-9e0f-da4102908041",
        "outputId": "e57396c1-e073-495a-e4f5-822c4787c176"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== VERSION CHECK ===\n",
            "⚠️ torch: 2.8.0+cu128 (expected: 2.4.1)\n",
            "✅ transformers: 4.46.2 (expected: 4.46.2)\n",
            "✅ trl: 0.11.4 (expected: 0.11.4)\n",
            "✅ datasets: 3.0.2 (expected: 3.0.2)\n",
            "⚠️ bitsandbytes: 0.48.2 (expected: 0.44.1)\n",
            "✅ peft: 0.13.2 (expected: 0.13.2)\n",
            "⚠️ qwen_vl_utils: unknown (expected: 0.0.8)\n",
            "✅ wandb: 0.18.5 (expected: 0.18.5)\n",
            "✅ accelerate: 1.0.1 (expected: 1.0.1)\n"
          ]
        }
      ],
      "source": [
        "import importlib\n",
        "\n",
        "# Verify installed versions match requirements\n",
        "requirements = {\n",
        "    'torch': '2.4.1',\n",
        "    'transformers': '4.46.2',\n",
        "    'trl': '0.11.4',\n",
        "    'datasets': '3.0.2',\n",
        "    'bitsandbytes': '0.44.1',\n",
        "    'peft': '0.13.2',\n",
        "    'qwen_vl_utils': '0.0.8',\n",
        "    'wandb': '0.18.5',\n",
        "    'accelerate': '1.0.1'\n",
        "}\n",
        "\n",
        "print(\"=== VERSION CHECK ===\")\n",
        "for package, expected in requirements.items():\n",
        "    try:\n",
        "        module = importlib.import_module(package)\n",
        "        actual = getattr(module, '__version__', 'unknown')\n",
        "        status = \"✅\" if actual == expected else \"⚠️\"\n",
        "        print(f\"{status} {package}: {actual} (expected: {expected})\")\n",
        "    except ImportError:\n",
        "        print(f\"❌ {package}: NOT INSTALLED (expected: {expected})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5cc893e-b7c5-4c83-9a5b-6e036fc06b97",
      "metadata": {
        "id": "a5cc893e-b7c5-4c83-9a5b-6e036fc06b97"
      },
      "outputs": [],
      "source": [
        "# 1) metric IoU (Interection over Union)\n",
        "# measures how much two bounding boxes overlap\n",
        "# It's a core metric for evaluation of object detection\n",
        "\n",
        "# Math : Area of Overlap/Area of Union\n",
        "# Intersection / (Box1 + Box2 - Intersection)\n",
        "\n",
        "import torch\n",
        "from torchvision import ops\n",
        "\n",
        "# Boxes format: [x1, y1, x2, y2] where (x1,y1) is top-left, (x2,y2) is bottom-right\n",
        "# make sure to get this format\n",
        "predicted_boxes = torch.tensor([\n",
        "    [2, 2, 10, 10],\n",
        "])\n",
        "\n",
        "ground_truth_boxes = torch.tensor([\n",
        "    [3, 3, 11, 11],\n",
        "])\n",
        "\n",
        "iou_matrix = ops.box_iou(predicted_boxes,ground_truth_boxes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "576bf3ce-4a96-4800-9de9-87d4550b931b",
      "metadata": {
        "id": "576bf3ce-4a96-4800-9de9-87d4550b931b",
        "outputId": "c0c270dd-6544-4a7b-d408-20161d69724c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0.6203]])"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "iou_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba1b21b9-265a-46a6-bd6a-fa0004bdc4a7",
      "metadata": {
        "id": "ba1b21b9-265a-46a6-bd6a-fa0004bdc4a7",
        "outputId": "bef86aab-5110-4c94-ad90-ae58cdbbf010"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "def box_iou(boxes1: Tensor, boxes2: Tensor) -> Tensor:\n",
            "    \"\"\"\n",
            "    Return intersection-over-union (Jaccard index) between two sets of boxes.\n",
            "\n",
            "    Both sets of boxes are expected to be in ``(x1, y1, x2, y2)`` format with\n",
            "    ``0 <= x1 < x2`` and ``0 <= y1 < y2``.\n",
            "\n",
            "    Args:\n",
            "        boxes1 (Tensor[N, 4]): first set of boxes\n",
            "        boxes2 (Tensor[M, 4]): second set of boxes\n",
            "\n",
            "    Returns:\n",
            "        Tensor[N, M]: the NxM matrix containing the pairwise IoU values for every element in boxes1 and boxes2\n",
            "    \"\"\"\n",
            "    if not torch.jit.is_scripting() and not torch.jit.is_tracing():\n",
            "        _log_api_usage_once(box_iou)\n",
            "    inter, union = _box_inter_union(boxes1, boxes2)\n",
            "    iou = inter / union\n",
            "    return iou\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import inspect\n",
        "source = inspect.getsource(ops.box_iou)\n",
        "print(source)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0b9fad5-e7a3-4968-8c59-d0f0a07f03fc",
      "metadata": {
        "id": "c0b9fad5-e7a3-4968-8c59-d0f0a07f03fc",
        "outputId": "ab0fd8c3-1398-4f51-ae08-db253b670114"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "def _box_inter_union(boxes1: Tensor, boxes2: Tensor) -> tuple[Tensor, Tensor]:\n",
            "    area1 = box_area(boxes1)\n",
            "    area2 = box_area(boxes2)\n",
            "\n",
            "    lt = torch.max(boxes1[:, None, :2], boxes2[:, :2])  # [N,M,2]\n",
            "    rb = torch.min(boxes1[:, None, 2:], boxes2[:, 2:])  # [N,M,2]\n",
            "\n",
            "    wh = _upcast(rb - lt).clamp(min=0)  # [N,M,2]\n",
            "    inter = wh[:, :, 0] * wh[:, :, 1]  # [N,M]\n",
            "\n",
            "    union = area1[:, None] + area2 - inter\n",
            "\n",
            "    return inter, union\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from torchvision.ops.boxes import _box_inter_union\n",
        "source = inspect.getsource(_box_inter_union)\n",
        "print(source)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd687f78-dbe8-4b31-bff1-c619640bc772",
      "metadata": {
        "id": "cd687f78-dbe8-4b31-bff1-c619640bc772",
        "outputId": "6e242d40-f241-4db6-de58-941fea56dc2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "def _box_inter_union(boxes1: Tensor, boxes2: Tensor) -> tuple[Tensor, Tensor]:\n",
            "    area1 = box_area(boxes1)\n",
            "    area2 = box_area(boxes2)\n",
            "\n",
            "    lt = torch.max(boxes1[:, None, :2], boxes2[:, :2])  # [N,M,2]\n",
            "    rb = torch.min(boxes1[:, None, 2:], boxes2[:, 2:])  # [N,M,2]\n",
            "\n",
            "    wh = _upcast(rb - lt).clamp(min=0)  # [N,M,2]\n",
            "    inter = wh[:, :, 0] * wh[:, :, 1]  # [N,M]\n",
            "\n",
            "    union = area1[:, None] + area2 - inter\n",
            "\n",
            "    return inter, union\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from torchvision.ops.boxes import _box_inter_union\n",
        "source = inspect.getsource(_box_inter_union)\n",
        "print(source)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e03457ba-cb0b-46b3-898f-e0855d01f62d",
      "metadata": {
        "id": "e03457ba-cb0b-46b3-898f-e0855d01f62d",
        "outputId": "4f2940ec-e08a-47b7-96a8-1835bc4d1d3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "def box_area(boxes: Tensor) -> Tensor:\n",
            "    \"\"\"\n",
            "    Computes the area of a set of bounding boxes, which are specified by their\n",
            "    (x1, y1, x2, y2) coordinates.\n",
            "\n",
            "    Args:\n",
            "        boxes (Tensor[N, 4]): boxes for which the area will be computed. They\n",
            "            are expected to be in (x1, y1, x2, y2) format with\n",
            "            ``0 <= x1 < x2`` and ``0 <= y1 < y2``.\n",
            "\n",
            "    Returns:\n",
            "        Tensor[N]: the area for each box\n",
            "    \"\"\"\n",
            "    if not torch.jit.is_scripting() and not torch.jit.is_tracing():\n",
            "        _log_api_usage_once(box_area)\n",
            "    boxes = _upcast(boxes)\n",
            "    return (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from torchvision.ops.boxes import box_area\n",
        "\n",
        "source = inspect.getsource(box_area)\n",
        "print(source)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d2693e0-2abc-4a0b-b9c7-03b5fdc89536",
      "metadata": {
        "id": "2d2693e0-2abc-4a0b-b9c7-03b5fdc89536",
        "outputId": "ab1fc996-bd0b-4f02-8123-0381d193b51b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "64 64\n"
          ]
        }
      ],
      "source": [
        "# ok let's verify manually\n",
        "width = 10 - 2 # 8\n",
        "height = 10 - 2 # 8\n",
        "\n",
        "area_predict = width*height\n",
        "width = 11 - 3\n",
        "height = 11 - 3\n",
        "\n",
        "area_truth = width*height\n",
        "print(area_predict,area_truth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ade6022-84f0-4420-a1e5-89d36be8b32a",
      "metadata": {
        "id": "3ade6022-84f0-4420-a1e5-89d36be8b32a"
      },
      "outputs": [],
      "source": [
        "# ouch :)) cv vs cartesian\n",
        "# ok so what confused me is naming conventions\n",
        "# like lt = left top, and rb = right bottom\n",
        "# but in high school math I get used to math coordinates\n",
        "# like lb = left bottom and rt = righ top\n",
        "\n",
        "# math should work probably anyway is just internal mental confusion\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e12fbf1d-a4d5-4c56-9ca1-042072651812",
      "metadata": {
        "id": "e12fbf1d-a4d5-4c56-9ca1-042072651812"
      },
      "outputs": [],
      "source": [
        "# Standard Image\n",
        "\n",
        "# (0,0) ────────► x increases\n",
        "#   │\n",
        "#   │    [x1, y1] = TOP-LEFT\n",
        "#   │    [x2, y2] = BOTTOM-RIGHT\n",
        "#   ▼\n",
        "# y increases\n",
        "\n",
        "# Cartesian\n",
        "# y increases\n",
        "#   ▲\n",
        "#   │    [x1, y1] = BOTTOM-LEFT\n",
        "#   │    [x2, y2] = TOP-RIGHT\n",
        "#   │\n",
        "# (0,0) ────────► x increases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f5f38cc-feb8-4b64-97be-7ad34726e7f7",
      "metadata": {
        "id": "4f5f38cc-feb8-4b64-97be-7ad34726e7f7"
      },
      "outputs": [],
      "source": [
        "left_botom = max([2,2],[3,3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65c45e8c-2ebd-4328-8b66-520c6a16a14b",
      "metadata": {
        "id": "65c45e8c-2ebd-4328-8b66-520c6a16a14b",
        "outputId": "f319317e-bd6d-4fdd-dbe3-3ed63836abb6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[3, 3]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "left_botom"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f67b4497-54f0-4929-b2dc-b44fa195b6bc",
      "metadata": {
        "id": "f67b4497-54f0-4929-b2dc-b44fa195b6bc"
      },
      "outputs": [],
      "source": [
        "right_top = min([10,10],[11,11])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ecca785-38f5-47cd-b699-2028ec649424",
      "metadata": {
        "id": "9ecca785-38f5-47cd-b699-2028ec649424",
        "outputId": "7da33e21-a0f3-4786-e2d8-bef45a1621b0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[10, 10]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "right_top"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6319a2a3-aed6-4a6c-beef-00ba82aacd62",
      "metadata": {
        "id": "6319a2a3-aed6-4a6c-beef-00ba82aacd62"
      },
      "outputs": [],
      "source": [
        "WidthHeight = [10 - 3, 10 - 3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "569da32e-47be-4d8c-a602-3a08108865d9",
      "metadata": {
        "id": "569da32e-47be-4d8c-a602-3a08108865d9",
        "outputId": "c4c465d8-6de6-4579-c8fe-12669f811f4b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[7, 7]"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "WidthHeight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8740637-5577-4c17-b471-b47bb4d6326c",
      "metadata": {
        "id": "f8740637-5577-4c17-b471-b47bb4d6326c"
      },
      "outputs": [],
      "source": [
        "inter = WidthHeight[0]*WidthHeight[1]\n",
        "union = area_predict + area_truth - inter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "082ae34e-09fd-4ad9-a3d7-78e9b3b34cbb",
      "metadata": {
        "id": "082ae34e-09fd-4ad9-a3d7-78e9b3b34cbb",
        "outputId": "bd22a53b-0687-435c-dd29-86f9d98f31af"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "79"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "union\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7b0f653-4956-489f-b00f-4efda45eb6a0",
      "metadata": {
        "id": "b7b0f653-4956-489f-b00f-4efda45eb6a0"
      },
      "outputs": [],
      "source": [
        "iou = inter / union # 49 / 79 # ok so 79 is a prime so no further simplification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8cce575-0c36-44ac-9c96-3e8beaa750f3",
      "metadata": {
        "id": "c8cce575-0c36-44ac-9c96-3e8beaa750f3"
      },
      "outputs": [],
      "source": [
        "# ok so basically iou is a proportion of intersection area over (total_area - interection)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2a1d78d-1c23-4021-9349-1d9579978f2c",
      "metadata": {
        "id": "c2a1d78d-1c23-4021-9349-1d9579978f2c",
        "outputId": "fe4d03b8-1aba-4cc8-aeac-0adc1e44c225"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.620253164556962"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "iou"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e7f812b-5d7f-4877-9da6-5948b907abe9",
      "metadata": {
        "id": "4e7f812b-5d7f-4877-9da6-5948b907abe9",
        "outputId": "eb7f8383-1982-4ecb-e2bc-3ab5b9c9f7ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.6203]])\n"
          ]
        }
      ],
      "source": [
        "print(ops.box_iou(predicted_boxes,ground_truth_boxes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c84d6db3-7962-4793-9564-014435cb299e",
      "metadata": {
        "id": "c84d6db3-7962-4793-9564-014435cb299e",
        "outputId": "31da4156-9588-4bb8-f83d-714879e418bb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.6202531456947327"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ok so rounded to up to 4 decimal, actually it's funny that code does it show it ?\n",
        "\n",
        "iou = ops.box_iou(predicted_boxes,ground_truth_boxes)\n",
        "\n",
        "iou.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8af5762a-73a4-4769-bdb1-1d4d78f484c4",
      "metadata": {
        "id": "8af5762a-73a4-4769-bdb1-1d4d78f484c4"
      },
      "outputs": [],
      "source": [
        "# oh ok, so it's just print statemt which rounds it\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38bf89ca-fe50-47ef-961b-0535272b6c3e",
      "metadata": {
        "id": "38bf89ca-fe50-47ef-961b-0535272b6c3e"
      },
      "outputs": [],
      "source": [
        "# ok it seems that I understand how individual IOU is calculated\n",
        "# I guess for multiple images I should like average them\n",
        "# or like average worst 100 examples ???\n",
        "# or may be worst 25 % ?\n",
        "# width area for experemients\n",
        "\n",
        "# IOU Exploration Done\n",
        "############################################\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed2adb56-5154-404f-8ed5-945f7b29bcac",
      "metadata": {
        "id": "ed2adb56-5154-404f-8ed5-945f7b29bcac"
      },
      "outputs": [],
      "source": [
        "# Task 2) run IOU on default dataset to prepare evaluation pipiline (score should be best right?)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27b4075c-e7e6-4f55-8e6d-e77d9d24068c",
      "metadata": {
        "id": "27b4075c-e7e6-4f55-8e6d-e77d9d24068c"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "dataset_id = \"openfoodfacts/nutrition-table-detection\"\n",
        "\n",
        "nutrition_data = load_dataset(dataset_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7b0350c-656d-48b1-ba33-4d8c923dde5a",
      "metadata": {
        "id": "e7b0350c-656d-48b1-ba33-4d8c923dde5a",
        "outputId": "9b68b6e2-eed0-462a-fc12-a126b1ea4440"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['image_id', 'image', 'width', 'height', 'meta', 'objects'],\n",
              "        num_rows: 1083\n",
              "    })\n",
              "    val: Dataset({\n",
              "        features: ['image_id', 'image', 'width', 'height', 'meta', 'objects'],\n",
              "        num_rows: 123\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nutrition_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "651c0b66-5e72-42d0-89a4-9f962ca302d0",
      "metadata": {
        "id": "651c0b66-5e72-42d0-89a4-9f962ca302d0",
        "outputId": "debf1a88-70e0-4a6d-df9d-16c1d92c94e7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'image_id': '0009800892204_1',\n",
              " 'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=2592x1944>,\n",
              " 'width': 2592,\n",
              " 'height': 1944,\n",
              " 'meta': {'barcode': '0009800892204',\n",
              "  'off_image_id': '1',\n",
              "  'image_url': 'https://static.openfoodfacts.org/images/products/000/980/089/2204/1.jpg'},\n",
              " 'objects': {'bbox': [[0.057098764926195145,\n",
              "    0.014274691231548786,\n",
              "    0.603501558303833,\n",
              "    0.991126537322998]],\n",
              "  'category_id': [0],\n",
              "  'category_name': ['nutrition-table']}}"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Ok so what if there are multiple boxes ????\n",
        "# how I am supposed to identify which box belongs to what\n",
        "# ok this is going to be very fun\n",
        "# what is going to be my strategy\n",
        "# let me find how many data points have multiple boxes\n",
        "\n",
        "nutrition_data[\"train\"][0]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5690e6d7-ffd4-45d1-a442-f3e01beba8b7",
      "metadata": {
        "id": "5690e6d7-ffd4-45d1-a442-f3e01beba8b7",
        "outputId": "8d3f5ef6-83b5-4498-cf69-48ab717ee6f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2\n",
            "3\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "3\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "3\n",
            "2\n",
            "3\n",
            "3\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "3\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "3\n",
            "3\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n"
          ]
        }
      ],
      "source": [
        "\n",
        "for point in nutrition_data[\"train\"]:\n",
        "    boxes = point[\"objects\"][\"bbox\"]\n",
        "    n = len(boxes) # multiple boxes in single image\n",
        "    if n > 1:\n",
        "        print(n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8e80fae-28bd-4a95-9794-153b6967fe6d",
      "metadata": {
        "id": "f8e80fae-28bd-4a95-9794-153b6967fe6d"
      },
      "outputs": [],
      "source": [
        "# ok it's not that bad as i expected but still\n",
        "# like ok so I guess my initial strategy will be\n",
        "# calculate like kind of matrix and then keep best IOU haha\n",
        "\n",
        "\n",
        "# from scipy.optimize import linear_sum_assignment\n",
        "# ok it's called hungarian algorithm :))\n",
        "# not that dramatic for my 2-3 boxes\n",
        "# but in case more than 20\n",
        "\n",
        "\n",
        "# ok now it raises another question similat to my chess pawns\n",
        "# like what if number of boxes don't match\n",
        "# like there are 5 pawns and it only predicts 4\n",
        "# what I am supposed to do :))\n",
        "# I guess beside iou I will just report misses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b25bf23-d603-4537-b210-83e1a6ec127b",
      "metadata": {
        "id": "2b25bf23-d603-4537-b210-83e1a6ec127b"
      },
      "outputs": [],
      "source": [
        "# ok so let's keep prepping the data\n",
        "# {'image_id': '0009800892204_1',\n",
        "#  'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=2592x1944>,\n",
        "#  'width': 2592,\n",
        "#  'height': 1944,\n",
        "#  'meta': {'barcode': '0009800892204',\n",
        "#   'off_image_id': '1',\n",
        "#   'image_url': 'https://static.openfoodfacts.org/images/products/000/980/089/2204/1.jpg'},\n",
        "#  'objects': {'bbox': [[0.057098764926195145,\n",
        "#     0.014274691231548786,\n",
        "#     0.603501558303833,\n",
        "#     0.991126537322998]],\n",
        "#   'category_id': [0],\n",
        "#   'category_name': ['nutrition-table']}}\n",
        "\n",
        "# ok so it screams dictionary, like I guess I will iterate through whole dataset\n",
        "# and create a ground truth dictionary with \"image_id\" as a key and list of boxes as values\n",
        "\n",
        "\n",
        "# so I am aiming next evaluation loop\n",
        "\n",
        "# for image_id,image_data in dataset:\n",
        "#     predicted_boxes = model_predict(actual_image,prompt)\n",
        "#     groundt_truth = ground_boxes[image_id]\n",
        "#     iou_matrix = get_iou(predicted_boxes,ground_truth)\n",
        "\n",
        "# haha I think I will write get_iou manually just to make sure I understood it correctly\n",
        "# If I have a time :))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06b9181c-08d7-45a3-8b18-42ab18565124",
      "metadata": {
        "id": "06b9181c-08d7-45a3-8b18-42ab18565124"
      },
      "outputs": [],
      "source": [
        "# ok assume dataset will have image_id and objects\n",
        "def populate_data(populating_dic,dataset,key=\"train\"):\n",
        "    if populating_dic is None:\n",
        "        populating_dic = dict()\n",
        "\n",
        "    if key not in dataset:\n",
        "        print(\"dataset missing key\",key)\n",
        "        return None\n",
        "\n",
        "\n",
        "    for point in dataset[key]:\n",
        "        image_id = point[\"image_id\"]\n",
        "        raw_boxes = point[\"objects\"][\"bbox\"]\n",
        "        # eliminate the problem at the source\n",
        "\n",
        "\n",
        "        # Fix format immediately: [y,x,y,x] → [x,y,x,y]\n",
        "        # https://huggingface.co/datasets/openfoodfacts/nutrition-table-detection\n",
        "        # bbox: List of bounding boxes in the format (y_min, x_min, y_max, x_max).\n",
        "        # Coordinates are normalized between 0 and 1, using the top-left corner as the origin.\n",
        "\n",
        "        raw_boxes = point[\"objects\"][\"bbox\"]\n",
        "        boxes = torch.as_tensor(\n",
        "            [\n",
        "                [x_min, y_min, x_max, y_max]\n",
        "                for y_min, x_min, y_max, x_max in raw_boxes\n",
        "            ],\n",
        "            dtype=torch.float32,\n",
        "            device=\"cpu\",                # <-- IMPORTANT I caught a bug this data was on gpu\n",
        "            # so when I decided to load Qwen, I got OOM :)))\n",
        "        )\n",
        "\n",
        "        category = point[\"objects\"][\"category_name\"]\n",
        "        # image_data = point[\"image\"] # could not keep image too expensive\n",
        "\n",
        "        populating_dic[image_id] = {\n",
        "\n",
        "            \"boxes\":boxes,\n",
        "            \"category\":category,\n",
        "            ##\"image_data\":image_data\n",
        "\n",
        "        }\n",
        "\n",
        "    return populating_dic\n",
        "\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "392031b9-a7a7-4b85-a045-88f394fec14a",
      "metadata": {
        "id": "392031b9-a7a7-4b85-a045-88f394fec14a"
      },
      "outputs": [],
      "source": [
        "ground_truth = populate_data(None, nutrition_data, key=\"train\")\n",
        "# ok set and forget :)) ground truth should be solid\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46b97ad6-fb16-44ae-9ffd-a915e3b881cc",
      "metadata": {
        "id": "46b97ad6-fb16-44ae-9ffd-a915e3b881cc"
      },
      "outputs": [],
      "source": [
        "def show_gpu(tag):\n",
        "    print(f\"[{tag}] allocated={torch.cuda.memory_allocated()/1024**3:.2f} GB, \"\n",
        "          f\"reserved={torch.cuda.memory_reserved()/1024**3:.2f} GB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a426a646-b393-4d39-b24e-e1a048567da0",
      "metadata": {
        "id": "a426a646-b393-4d39-b24e-e1a048567da0",
        "outputId": "6cf4ca9d-1ef7-413b-98d2-397e9fc1252b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[after populate] allocated=0.00 GB, reserved=0.00 GB\n"
          ]
        }
      ],
      "source": [
        "show_gpu(\"after populate\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af34ab69-5d03-4e40-a9fd-8e9eb016fc1a",
      "metadata": {
        "id": "af34ab69-5d03-4e40-a9fd-8e9eb016fc1a",
        "outputId": "6e64e7a9-4410-4ea7-8697-762bc50c018e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'boxes': tensor([[0.0143, 0.0571, 0.9911, 0.6035]]), 'category': ['nutrition-table']}\n",
            "{'boxes': tensor([[0.1471, 0.1520, 0.5163, 0.5882]]), 'category': ['nutrition-table']}\n",
            "{'boxes': tensor([[0.0101, 0.0083, 0.9783, 0.9944]]), 'category': ['nutrition-table']}\n",
            "{'boxes': tensor([[0.3202, 0.1718, 0.6279, 0.6250]]), 'category': ['nutrition-table']}\n",
            "{'boxes': tensor([[0.1432, 0.2512, 0.3766, 0.4683]]), 'category': ['nutrition-table']}\n"
          ]
        }
      ],
      "source": [
        "cnt = 0\n",
        "for p in ground_truth:\n",
        "    if cnt == 5:\n",
        "        break\n",
        "\n",
        "    print(ground_truth[p])\n",
        "    cnt +=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42db7ede-b69b-48e8-9090-09a74e0d637d",
      "metadata": {
        "id": "42db7ede-b69b-48e8-9090-09a74e0d637d",
        "outputId": "67dde87b-678d-4d43-de6b-27abf0076ced"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (1.16.3)\n",
            "Requirement already satisfied: numpy<2.6,>=1.25.2 in /usr/local/lib/python3.12/dist-packages (from scipy) (2.1.2)\n"
          ]
        }
      ],
      "source": [
        "# ok now I do dumb thing, but I want to make\n",
        "# a sanity check, if there are multiple boxes\n",
        "# per image so I will catch it here\n",
        "# rather than try to debug it down the line done it before not doing it again :))\n",
        "!pip install scipy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e14ac5e-40a1-4fbb-8321-8b607e588bc4",
      "metadata": {
        "id": "1e14ac5e-40a1-4fbb-8321-8b607e588bc4"
      },
      "outputs": [],
      "source": [
        "# ok my mega check function\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "from torchvision import ops\n",
        "def evaluate(dict_predict,dict_ground_truth):\n",
        "    all_ious = list()\n",
        "\n",
        "    total_missed_real_boxes = 0\n",
        "    total_extra_predicted = 0\n",
        "\n",
        "\n",
        "    common_ids = set(dict_predict.keys()) & set(dict_ground_truth.keys())\n",
        "\n",
        "    for image_id in common_ids:\n",
        "        prediction_boxes = dict_predict[image_id][\"boxes\"]\n",
        "        ground_truth_boxes = dict_ground_truth[image_id][\"boxes\"]\n",
        "\n",
        "        total_predicted_boxes = len(prediction_boxes)\n",
        "        total_ground_truth_boxes = len(ground_truth_boxes)\n",
        "\n",
        "        # Calculate IoU matrix\n",
        "        iou_matrix = ops.box_iou(prediction_boxes,ground_truth_boxes)\n",
        "        # Hungarian matching\n",
        "        pred_idx, gt_idx = linear_sum_assignment(-iou_matrix.numpy())\n",
        "\n",
        "        # Get matched IoUs\n",
        "        matched_ious = iou_matrix[pred_idx, gt_idx]\n",
        "        all_ious.extend(matched_ious.tolist())\n",
        "\n",
        "        # i have a bit doubt about this lines may be abs\n",
        "        #\n",
        "        total_missed_real_boxes += total_ground_truth_boxes - len(pred_idx)\n",
        "        total_extra_predicted += total_predicted_boxes - len(matched_ious)\n",
        "\n",
        "    return {\n",
        "        'mean_iou': sum(all_ious) / len(all_ious),\n",
        "        'missed_gt': total_missed_real_boxes,\n",
        "        'extra_pred': total_extra_predicted\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70828182-0346-40f1-be08-07597d8c762e",
      "metadata": {
        "id": "70828182-0346-40f1-be08-07597d8c762e",
        "outputId": "be8b2cf0-1d0a-4ac4-e03c-8a4fbacc6220"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'mean_iou': 1.0, 'missed_gt': 0, 'extra_pred': 0}\n"
          ]
        }
      ],
      "source": [
        "results = evaluate(ground_truth, ground_truth)\n",
        "print(results)  # Should be: mean_iou=1.0, missed_gt=0, extra_pred=0\n",
        "\n",
        "# cool solid results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5cd38ca-8247-4e0a-8dbb-31946bcb85fe",
      "metadata": {
        "id": "e5cd38ca-8247-4e0a-8dbb-31946bcb85fe"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "889c826a-3285-4d91-8e18-3c0a06eb5a1d",
      "metadata": {
        "id": "889c826a-3285-4d91-8e18-3c0a06eb5a1d",
        "outputId": "8c2f0c12-0fe6-4952-8857-28c0b6795e69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU allocated memory: 0.00 GB\n",
            "GPU reserved memory: 0.00 GB\n"
          ]
        }
      ],
      "source": [
        "import gc\n",
        "import time\n",
        "\n",
        "def clear_memory():\n",
        "    # Delete variables if they exist in the current global scope\n",
        "    if 'inputs' in globals(): del globals()['inputs']\n",
        "    if 'model' in globals(): del globals()['model']\n",
        "    if 'processor' in globals(): del globals()['processor']\n",
        "    if 'trainer' in globals(): del globals()['trainer']\n",
        "    if 'peft_model' in globals(): del globals()['peft_model']\n",
        "    if 'bnb_config' in globals(): del globals()['bnb_config']\n",
        "    time.sleep(2)\n",
        "\n",
        "    # Garbage collection and clearing CUDA memory\n",
        "    gc.collect()\n",
        "    time.sleep(2)\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.synchronize()\n",
        "    time.sleep(2)\n",
        "    gc.collect()\n",
        "    time.sleep(2)\n",
        "\n",
        "    print(f\"GPU allocated memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
        "    print(f\"GPU reserved memory: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
        "\n",
        "clear_memory()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ba8fdd9-2884-4065-b1d0-53804f42fc84",
      "metadata": {
        "id": "1ba8fdd9-2884-4065-b1d0-53804f42fc84"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58e8aa76-ced2-4870-aa1e-33b1db8b5146",
      "metadata": {
        "id": "58e8aa76-ced2-4870-aa1e-33b1db8b5146",
        "outputId": "87a23922-9f80-4c0b-e9d1-265a82934676"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fri Nov 21 23:53:43 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 570.124.06             Driver Version: 570.124.06     CUDA Version: 12.8     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA GeForce RTX 5090        On  |   00000000:01:00.0 Off |                  N/A |\n",
            "|  0%   32C    P1             65W /  575W |     506MiB /  32607MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|    0   N/A  N/A            9241      C   /usr/local/bin/python                   496MiB |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4fc5c32d-ee42-45cb-bfde-952b89ca657b",
      "metadata": {
        "id": "4fc5c32d-ee42-45cb-bfde-952b89ca657b"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# 3) run inference on base model without fintuning and pass through evaluation pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c67d51a-695c-427c-ae5c-5261d6653451",
      "metadata": {
        "id": "6c67d51a-695c-427c-ae5c-5261d6653451"
      },
      "outputs": [],
      "source": [
        "# 0) load model and tokenizer\n",
        "\n",
        "# 1) I need to prepare messages per image ?\n",
        "# 2) write a function that given single image returns boxes prediction\n",
        "# a) I have a strong feeling that since response will be like string\n",
        "# I will need some unified parsing function\n",
        "# 3) Prepare prediction_dict, let's say 10 and run full evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bd4094d-c263-4dbc-ab14-163af78a9d87",
      "metadata": {
        "id": "6bd4094d-c263-4dbc-ab14-163af78a9d87"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install matplotlib\n",
        "!pip install hf_transfer\n",
        "from IPython.display import display, HTML\n",
        "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\n",
        "%matplotlib inline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14662bb2-9b92-4fdb-8998-1b293500e781",
      "metadata": {
        "id": "14662bb2-9b92-4fdb-8998-1b293500e781"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "from matplotlib import pyplot as plt\n",
        "from transformers import Qwen2VLForConditionalGeneration, Qwen2VLProcessor,AutoProcessor\n",
        "from qwen_vl_utils import vision_process,process_vision_info\n",
        "import peft\n",
        "import bitsandbytes as bnb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc7f6f16-eece-4f43-ac08-e66b49214065",
      "metadata": {
        "id": "cc7f6f16-eece-4f43-ac08-e66b49214065"
      },
      "outputs": [],
      "source": [
        "# dummy run as before\n",
        "messages = [\n",
        "{\n",
        "    \"role\":\"user\",\n",
        "    \"content\":[\n",
        "        {\n",
        "            \"type\":\"image\",\n",
        "            \"image\":\"https://images.chesscomfiles.com/uploads/v1/images_users/tiny_mce/Ognian_Mikov/php2nnXz9.png\",\n",
        "\n",
        "        },\n",
        "        {\n",
        "            \"type\":\"text\",\"text\":\"Provide surrounding boxes for all pawns in this image and respond in <|bbox|> format\"\n",
        "        },\n",
        "    ]\n",
        "}\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ca41e82-db40-484f-a953-72f4e5b7d16c",
      "metadata": {
        "id": "0ca41e82-db40-484f-a953-72f4e5b7d16c"
      },
      "outputs": [],
      "source": [
        "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")\n",
        "\n",
        "\n",
        "text = processor.apply_chat_template(\n",
        "    messages, tokenize=False, add_generation_prompt=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b03ad26-c87c-4852-aeb6-6ac44d60b7a7",
      "metadata": {
        "id": "2b03ad26-c87c-4852-aeb6-6ac44d60b7a7",
        "outputId": "5dc6038d-e236-4a49-c794-0ccbecbe3bad"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>Provide surrounding boxes for all pawns in this image and respond in <|bbox|> format<|im_end|>\\n<|im_start|>assistant\\n'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e96241ff-b833-48c4-844b-a4180a1456e9",
      "metadata": {
        "id": "e96241ff-b833-48c4-844b-a4180a1456e9",
        "outputId": "5c80eac0-3904-4495-cb11-352a736160c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "allocated: 15.830080032348633 GB\n",
            "reserved: 30.099609375 GB\n"
          ]
        }
      ],
      "source": [
        "print(\"allocated:\", torch.cuda.memory_allocated()/1024**3, \"GB\")\n",
        "print(\"reserved:\", torch.cuda.memory_reserved()/1024**3, \"GB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7dfa8eb-d06f-4b6c-89b0-423b2dbfc8e1",
      "metadata": {
        "id": "b7dfa8eb-d06f-4b6c-89b0-423b2dbfc8e1",
        "outputId": "abae8cef-4bfe-43fd-e3e6-a16bd46a39b4",
        "colab": {
          "referenced_widgets": [
            "0bc81b3c22f945fc951d0079d1502324"
          ]
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0bc81b3c22f945fc951d0079d1502324",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 1.73 GiB. GPU 0 has a total capacity of 31.37 GiB of which 674.69 MiB is free. Including non-PyTorch memory, this process has 30.70 GiB memory in use. Of the allocated memory 30.06 GiB is allocated by PyTorch, and 37.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m model.eval()\n\u001b[32m      7\u001b[39m inputs = inputs.to(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m generated_ids = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m128\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Step 1: Trimming - Remove the input prompt from output\u001b[39;00m\n\u001b[32m     10\u001b[39m generated_ids_trimmed = [\n\u001b[32m     11\u001b[39m     out_ids[\u001b[38;5;28mlen\u001b[39m(in_ids) :] \u001b[38;5;28;01mfor\u001b[39;00m in_ids, out_ids \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(inputs.input_ids, generated_ids)\n\u001b[32m     12\u001b[39m ]\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py:2215\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[39m\n\u001b[32m   2207\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2208\u001b[39m         input_ids=input_ids,\n\u001b[32m   2209\u001b[39m         expand_size=generation_config.num_return_sequences,\n\u001b[32m   2210\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2211\u001b[39m         **model_kwargs,\n\u001b[32m   2212\u001b[39m     )\n\u001b[32m   2214\u001b[39m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2215\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2216\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2217\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2218\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2219\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2220\u001b[39m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2221\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2222\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2223\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2225\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode.BEAM_SAMPLE, GenerationMode.BEAM_SEARCH):\n\u001b[32m   2226\u001b[39m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[32m   2227\u001b[39m     beam_scorer = BeamSearchScorer(\n\u001b[32m   2228\u001b[39m         batch_size=batch_size,\n\u001b[32m   2229\u001b[39m         num_beams=generation_config.num_beams,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2234\u001b[39m         max_length=generation_config.max_length,\n\u001b[32m   2235\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py:3206\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   3203\u001b[39m model_inputs.update({\u001b[33m\"\u001b[39m\u001b[33moutput_hidden_states\u001b[39m\u001b[33m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[32m   3205\u001b[39m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3206\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   3208\u001b[39m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[32m   3209\u001b[39m model_kwargs = \u001b[38;5;28mself\u001b[39m._update_model_kwargs_for_generation(\n\u001b[32m   3210\u001b[39m     outputs,\n\u001b[32m   3211\u001b[39m     model_kwargs,\n\u001b[32m   3212\u001b[39m     is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   3213\u001b[39m )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/models/qwen2_vl/modeling_qwen2_vl.py:1686\u001b[39m, in \u001b[36mQwen2VLForConditionalGeneration.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, pixel_values, pixel_values_videos, image_grid_thw, video_grid_thw, rope_deltas)\u001b[39m\n\u001b[32m   1684\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pixel_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1685\u001b[39m     pixel_values = pixel_values.type(\u001b[38;5;28mself\u001b[39m.visual.get_dtype())\n\u001b[32m-> \u001b[39m\u001b[32m1686\u001b[39m     image_embeds = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvisual\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrid_thw\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimage_grid_thw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1687\u001b[39m     n_image_tokens = (input_ids == \u001b[38;5;28mself\u001b[39m.config.image_token_id).sum().item()\n\u001b[32m   1688\u001b[39m     n_image_features = image_embeds.shape[\u001b[32m0\u001b[39m]\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/models/qwen2_vl/modeling_qwen2_vl.py:1049\u001b[39m, in \u001b[36mQwen2VisionTransformerPretrainedModel.forward\u001b[39m\u001b[34m(self, hidden_states, grid_thw)\u001b[39m\n\u001b[32m   1046\u001b[39m cu_seqlens = F.pad(cu_seqlens, (\u001b[32m1\u001b[39m, \u001b[32m0\u001b[39m), value=\u001b[32m0\u001b[39m)\n\u001b[32m   1048\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.blocks:\n\u001b[32m-> \u001b[39m\u001b[32m1049\u001b[39m     hidden_states = \u001b[43mblk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcu_seqlens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcu_seqlens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrotary_pos_emb\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrotary_pos_emb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1051\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.merger(hidden_states)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/models/qwen2_vl/modeling_qwen2_vl.py:431\u001b[39m, in \u001b[36mQwen2VLVisionBlock.forward\u001b[39m\u001b[34m(self, hidden_states, cu_seqlens, rotary_pos_emb)\u001b[39m\n\u001b[32m    430\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states, cu_seqlens, rotary_pos_emb) -> torch.Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m431\u001b[39m     hidden_states = hidden_states + \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    432\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnorm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcu_seqlens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcu_seqlens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrotary_pos_emb\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrotary_pos_emb\u001b[49m\n\u001b[32m    433\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    434\u001b[39m     hidden_states = hidden_states + \u001b[38;5;28mself\u001b[39m.mlp(\u001b[38;5;28mself\u001b[39m.norm2(hidden_states))\n\u001b[32m    435\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/models/qwen2_vl/modeling_qwen2_vl.py:404\u001b[39m, in \u001b[36mVisionSdpaAttention.forward\u001b[39m\u001b[34m(self, hidden_states, cu_seqlens, rotary_pos_emb)\u001b[39m\n\u001b[32m    402\u001b[39m k = k.transpose(\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m)\n\u001b[32m    403\u001b[39m v = v.transpose(\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m404\u001b[39m attn_output = \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    405\u001b[39m attn_output = attn_output.transpose(\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m)\n\u001b[32m    406\u001b[39m attn_output = attn_output.reshape(seq_length, -\u001b[32m1\u001b[39m)\n",
            "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 1.73 GiB. GPU 0 has a total capacity of 31.37 GiB of which 674.69 MiB is free. Including non-PyTorch memory, this process has 30.70 GiB memory in use. Of the allocated memory 30.06 GiB is allocated by PyTorch, and 37.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ],
      "source": [
        "image_inputs,video_inputs = process_vision_info(messages)\n",
        "inputs = processor(text=[text],images=image_inputs,videos=video_inputs,padding=True,return_tensors=\"pt\")\n",
        "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
        "    \"Qwen/Qwen2-VL-7B-Instruct\", torch_dtype=torch.bfloat16, device_map=\"cuda\"\n",
        ")\n",
        "model.eval()\n",
        "inputs = inputs.to(\"cuda\")\n",
        "generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
        "# Step 1: Trimming - Remove the input prompt from output\n",
        "generated_ids_trimmed = [\n",
        "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
        "]\n",
        "\n",
        "output_text = processor.batch_decode(\n",
        "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
        ")\n",
        "print(output_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "993e68f9-42df-48a5-bda2-3f0974045869",
      "metadata": {
        "id": "993e68f9-42df-48a5-bda2-3f0974045869"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bf66b69-3ca9-46ac-b6c9-bed917a5784e",
      "metadata": {
        "id": "2bf66b69-3ca9-46ac-b6c9-bed917a5784e"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}