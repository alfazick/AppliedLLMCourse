{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alfazick/AppliedLLMCourse/blob/main/ProjectMultiModalInferenceOnly2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14052b10-9f6a-432e-975f-a587629d774e",
      "metadata": {
        "id": "14052b10-9f6a-432e-975f-a587629d774e"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128,expandable_segments:True\"\n",
        "os.environ[\"PYTORCH_NO_CUDA_MEMORY_CACHING\"] = \"1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04f0c596-1ab7-4ae0-8ea5-b3f8e91465fc",
      "metadata": {
        "id": "04f0c596-1ab7-4ae0-8ea5-b3f8e91465fc"
      },
      "outputs": [],
      "source": [
        "# Cell 2 - MUST restart kernel\n",
        "import os\n",
        "os._exit(0)  # This forces kernel restart"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f44b026-35c1-4806-b852-542d037761b5",
      "metadata": {
        "id": "5f44b026-35c1-4806-b852-542d037761b5",
        "outputId": "2a2ae216-05de-4458-9f89-360679d55612"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "alloc conf: max_split_size_mb:128,expandable_segments:True\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(\"alloc conf:\", os.environ.get(\"PYTORCH_CUDA_ALLOC_CONF\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73a7ea75-e0dd-432e-b00b-4f813500aff7",
      "metadata": {
        "id": "73a7ea75-e0dd-432e-b00b-4f813500aff7",
        "outputId": "00bed59b-bb8e-4f5b-bd24-6b0bd2007d30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== VERSION CHECK ===\n",
            "⚠️ torch: 2.8.0+cu128 (expected: 2.4.1)\n",
            "✅ transformers: 4.46.2 (expected: 4.46.2)\n",
            "✅ trl: 0.11.4 (expected: 0.11.4)\n",
            "✅ datasets: 3.0.2 (expected: 3.0.2)\n",
            "⚠️ bitsandbytes: 0.48.2 (expected: 0.44.1)\n",
            "✅ peft: 0.13.2 (expected: 0.13.2)\n",
            "⚠️ qwen_vl_utils: unknown (expected: 0.0.8)\n",
            "✅ wandb: 0.18.5 (expected: 0.18.5)\n",
            "✅ accelerate: 1.0.1 (expected: 1.0.1)\n"
          ]
        }
      ],
      "source": [
        "import importlib\n",
        "\n",
        "# Verify installed versions match requirements\n",
        "requirements = {\n",
        "    'torch': '2.4.1',\n",
        "    'transformers': '4.46.2',\n",
        "    'trl': '0.11.4',\n",
        "    'datasets': '3.0.2',\n",
        "    'bitsandbytes': '0.44.1',\n",
        "    'peft': '0.13.2',\n",
        "    'qwen_vl_utils': '0.0.8',\n",
        "    'wandb': '0.18.5',\n",
        "    'accelerate': '1.0.1'\n",
        "}\n",
        "\n",
        "print(\"=== VERSION CHECK ===\")\n",
        "for package, expected in requirements.items():\n",
        "    try:\n",
        "        module = importlib.import_module(package)\n",
        "        actual = getattr(module, '__version__', 'unknown')\n",
        "        status = \"✅\" if actual == expected else \"⚠️\"\n",
        "        print(f\"{status} {package}: {actual} (expected: {expected})\")\n",
        "    except ImportError:\n",
        "        print(f\"❌ {package}: NOT INSTALLED (expected: {expected})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef6d9b6a-7314-4076-91b8-ee87f2b5815a",
      "metadata": {
        "id": "ef6d9b6a-7314-4076-91b8-ee87f2b5815a",
        "outputId": "98e6576d-4a85-4d97-bc0b-5271c5017763"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU allocated memory: 0.00 GB\n",
            "GPU reserved memory: 0.00 GB\n"
          ]
        }
      ],
      "source": [
        "import gc\n",
        "import time\n",
        "\n",
        "def clear_memory():\n",
        "    # Delete variables if they exist in the current global scope\n",
        "    if 'inputs' in globals(): del globals()['inputs']\n",
        "    if 'model' in globals(): del globals()['model']\n",
        "    if 'processor' in globals(): del globals()['processor']\n",
        "    if 'trainer' in globals(): del globals()['trainer']\n",
        "    if 'peft_model' in globals(): del globals()['peft_model']\n",
        "    if 'bnb_config' in globals(): del globals()['bnb_config']\n",
        "    time.sleep(2)\n",
        "\n",
        "    # Garbage collection and clearing CUDA memory\n",
        "    gc.collect()\n",
        "    time.sleep(2)\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.synchronize()\n",
        "    time.sleep(2)\n",
        "    gc.collect()\n",
        "    time.sleep(2)\n",
        "\n",
        "    print(f\"GPU allocated memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
        "    print(f\"GPU reserved memory: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
        "\n",
        "clear_memory()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ceeb2740-7fed-4693-8719-730aff91486d",
      "metadata": {
        "id": "ceeb2740-7fed-4693-8719-730aff91486d",
        "outputId": "48924f0b-0b41-4ba4-898b-062c745f80f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sat Nov 22 00:40:41 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 570.124.06             Driver Version: 570.124.06     CUDA Version: 12.8     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA GeForce RTX 5090        On  |   00000000:01:00.0 Off |                  N/A |\n",
            "|  0%   33C    P1             66W /  575W |     506MiB /  32607MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|    0   N/A  N/A           12050      C   /usr/local/bin/python                   496MiB |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5328d46d-1ae6-40b2-b0bb-642e01df220b",
      "metadata": {
        "id": "5328d46d-1ae6-40b2-b0bb-642e01df220b"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "from matplotlib import pyplot as plt\n",
        "from transformers import Qwen2VLForConditionalGeneration, Qwen2VLProcessor,AutoProcessor\n",
        "from qwen_vl_utils import vision_process,process_vision_info\n",
        "import peft\n",
        "import bitsandbytes as bnb\n",
        "from transformers import BitsAndBytesConfig\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2abb0f07-fdb0-4a4f-8c47-f2b05e92bb84",
      "metadata": {
        "id": "2abb0f07-fdb0-4a4f-8c47-f2b05e92bb84",
        "outputId": "47c6d2fd-9ea5-47cb-b811-0961d9f5e99f",
        "colab": {
          "referenced_widgets": [
            "b9258354661b4e02b43c7f24a6669929"
          ]
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b9258354661b4e02b43c7f24a6669929",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Qwen2VLForConditionalGeneration(\n",
              "  (visual): Qwen2VisionTransformerPretrainedModel(\n",
              "    (patch_embed): PatchEmbed(\n",
              "      (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)\n",
              "    )\n",
              "    (rotary_pos_emb): VisionRotaryEmbedding()\n",
              "    (blocks): ModuleList(\n",
              "      (0-31): 32 x Qwen2VLVisionBlock(\n",
              "        (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): VisionSdpaAttention(\n",
              "          (qkv): Linear8bitLt(in_features=1280, out_features=3840, bias=True)\n",
              "          (proj): Linear8bitLt(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (mlp): VisionMlp(\n",
              "          (fc1): Linear8bitLt(in_features=1280, out_features=5120, bias=True)\n",
              "          (act): QuickGELUActivation()\n",
              "          (fc2): Linear8bitLt(in_features=5120, out_features=1280, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (merger): PatchMerger(\n",
              "      (ln_q): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Sequential(\n",
              "        (0): Linear8bitLt(in_features=5120, out_features=5120, bias=True)\n",
              "        (1): GELU(approximate='none')\n",
              "        (2): Linear8bitLt(in_features=5120, out_features=3584, bias=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (model): Qwen2VLModel(\n",
              "    (embed_tokens): Embedding(152064, 3584)\n",
              "    (layers): ModuleList(\n",
              "      (0-27): 28 x Qwen2VLDecoderLayer(\n",
              "        (self_attn): Qwen2VLSdpaAttention(\n",
              "          (q_proj): Linear8bitLt(in_features=3584, out_features=3584, bias=True)\n",
              "          (k_proj): Linear8bitLt(in_features=3584, out_features=512, bias=True)\n",
              "          (v_proj): Linear8bitLt(in_features=3584, out_features=512, bias=True)\n",
              "          (o_proj): Linear8bitLt(in_features=3584, out_features=3584, bias=False)\n",
              "          (rotary_emb): Qwen2VLRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): Qwen2MLP(\n",
              "          (gate_proj): Linear8bitLt(in_features=3584, out_features=18944, bias=False)\n",
              "          (up_proj): Linear8bitLt(in_features=3584, out_features=18944, bias=False)\n",
              "          (down_proj): Linear8bitLt(in_features=18944, out_features=3584, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
              "        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
              "      )\n",
              "    )\n",
              "    (norm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
              "    (rotary_emb): Qwen2VLRotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# NF4 quantization config\n",
        "nf4_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "quantization_config=BitsAndBytesConfig(\n",
        "        load_in_8bit=True,\n",
        "        llm_int8_threshold=6.0\n",
        "    )\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")\n",
        "\n",
        "\n",
        "\n",
        "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
        "    \"Qwen/Qwen2-VL-7B-Instruct\",\n",
        "    quantization_config=quantization_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "print(model.device)\n",
        "model.eval()\n",
        "# ok finally it lopads the model in cuda\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37cdb611-9c0b-43bf-b80e-869da4804630",
      "metadata": {
        "id": "37cdb611-9c0b-43bf-b80e-869da4804630",
        "outputId": "0ffc4d83-7af3-463b-f34c-325c0d10b81f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>Provide surrounding boxes for all pawns in this image and respond in <|bbox|> format<|im_end|>\\n<|im_start|>assistant\\n'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# dummy run as before\n",
        "messages = [\n",
        "{\n",
        "    \"role\":\"user\",\n",
        "    \"content\":[\n",
        "        {\n",
        "            \"type\":\"image\",\n",
        "            \"image\":\"https://images.chesscomfiles.com/uploads/v1/images_users/tiny_mce/Ognian_Mikov/php2nnXz9.png\",\n",
        "\n",
        "        },\n",
        "        {\n",
        "            \"type\":\"text\",\"text\":\"Provide surrounding boxes for all pawns in this image and respond in <|bbox|> format\"\n",
        "        },\n",
        "    ]\n",
        "}\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "text = processor.apply_chat_template(\n",
        "    messages, tokenize=False, add_generation_prompt=True\n",
        ")\n",
        "text\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68add023-ebd6-4e1b-b91b-2872b696432d",
      "metadata": {
        "id": "68add023-ebd6-4e1b-b91b-2872b696432d"
      },
      "outputs": [],
      "source": [
        "\n",
        "image_inputs,video_inputs = process_vision_info(messages)\n",
        "inputs = processor(text=[text],images=image_inputs,videos=video_inputs,padding=True,return_tensors=\"pt\")\n",
        "inputs = inputs.to(model.device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e389fdac-3a28-455a-8aea-06f81c45864e",
      "metadata": {
        "id": "e389fdac-3a28-455a-8aea-06f81c45864e",
        "outputId": "a084b8c5-7f21-4b0c-a4a1-56f744f9a06e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "allocated: 0.0 GB\n",
            "reserved: 0.0 GB\n",
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "print(\"allocated:\", torch.cuda.memory_allocated()/1024**3, \"GB\")\n",
        "print(\"reserved:\", torch.cuda.memory_reserved()/1024**3, \"GB\")\n",
        "print(model.device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1c57c37-3745-45b0-b0bc-971e44461a56",
      "metadata": {
        "id": "b1c57c37-3745-45b0-b0bc-971e44461a56",
        "outputId": "77b3164f-9dd5-418c-ce9d-380adeef327c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total VRAM: 31.37 GB\n"
          ]
        }
      ],
      "source": [
        "print(f\"Total VRAM: {torch.cuda.get_device_properties(0).total_memory/1024**3:.2f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43b2e600-782c-43fc-9e8d-5803ec4b27a1",
      "metadata": {
        "id": "43b2e600-782c-43fc-9e8d-5803ec4b27a1",
        "outputId": "c0c9fe3d-0ce5-4486-937d-8669b1fb93e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['```json\\n[\\n  {\"bbox\": [12, 599, 72, 780]},\\n  {\"bbox\": [12, 100, 72, 171]},\\n  {\"bbox\": [12, 141, 72, 212]},\\n  {\"bbox\": [12, 243, 72, 314]},\\n  {\"bbox\": [12, 355, 72, 426]},\\n  {\"bbox\": [12, 467, ']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
        "# Step 1: Trimming - Remove the input prompt from output\n",
        "generated_ids_trimmed = [\n",
        "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
        "]\n",
        "\n",
        "output_text = processor.batch_decode(\n",
        "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
        ")\n",
        "print(output_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "690f3cdc-79c7-49fd-a363-a8dfafc75226",
      "metadata": {
        "id": "690f3cdc-79c7-49fd-a363-a8dfafc75226",
        "outputId": "afc6a602-dfbd-4c83-95da-527f10830399"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "allocated: 0.0 GB\n",
            "reserved: 0.0 GB\n"
          ]
        }
      ],
      "source": [
        "print(\"allocated:\", torch.cuda.memory_allocated()/1024**3, \"GB\")\n",
        "print(\"reserved:\", torch.cuda.memory_reserved()/1024**3, \"GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "adb01aa6-e899-4c81-a888-ed9682824061",
      "metadata": {
        "id": "adb01aa6-e899-4c81-a888-ed9682824061",
        "outputId": "c2869a79-5462-4a0e-e023-5e37758e3c45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "print(model.device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "acb11156-ca33-47e5-9ca7-9570f2b13c0a",
      "metadata": {
        "id": "acb11156-ca33-47e5-9ca7-9570f2b13c0a",
        "outputId": "d6c53f2c-e8a9-42e2-bf7a-d9250163cd1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sat Nov 22 00:44:35 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 570.124.06             Driver Version: 570.124.06     CUDA Version: 12.8     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA GeForce RTX 5090        On  |   00000000:01:00.0 Off |                  N/A |\n",
            "|  0%   31C    P8             23W /  575W |   10028MiB /  32607MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|    0   N/A  N/A           12050      C   /usr/local/bin/python                 10018MiB |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6e1321c-1cf3-4596-a536-bfdaea22bb73",
      "metadata": {
        "id": "d6e1321c-1cf3-4596-a536-bfdaea22bb73"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}