{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alfazick/AppliedLLMCourse/blob/main/part3MultiModal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a90cacb0-5507-4110-afce-f718e0af4058",
      "metadata": {
        "id": "a90cacb0-5507-4110-afce-f718e0af4058",
        "outputId": "5fc49f4e-f95d-487f-cd46-e4207adee065"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Skipping transformers as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping trl as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping datasets as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping bitsandbytes as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping peft as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping qwen-vl-utils as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping wandb as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping accelerate as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting transformers==4.46.2\n",
            "  Downloading transformers-4.46.2-py3-none-any.whl.metadata (44 kB)\n",
            "Collecting accelerate==1.0.1\n",
            "  Downloading accelerate-1.0.1-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting peft==0.13.2\n",
            "  Downloading peft-0.13.2-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting bitsandbytes==0.44.1\n",
            "  Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\n",
            "Collecting datasets==3.0.2\n",
            "  Downloading datasets-3.0.2-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting trl==0.11.4\n",
            "  Downloading trl-0.11.4-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting qwen-vl-utils==0.0.8\n",
            "  Downloading qwen_vl_utils-0.0.8-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting wandb==0.18.5\n",
            "  Downloading wandb-0.18.5-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers==4.46.2) (3.20.0)\n",
            "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers==4.46.2)\n",
            "  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.46.2) (2.1.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.46.2) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.46.2) (6.0.3)\n",
            "Collecting regex!=2019.12.17 (from transformers==4.46.2)\n",
            "  Downloading regex-2025.11.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers==4.46.2) (2.32.5)\n",
            "Collecting safetensors>=0.4.1 (from transformers==4.46.2)\n",
            "  Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Collecting tokenizers<0.21,>=0.20 (from transformers==4.46.2)\n",
            "  Downloading tokenizers-0.20.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting tqdm>=4.27 (from transformers==4.46.2)\n",
            "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate==1.0.1) (7.1.0)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from accelerate==1.0.1) (2.8.0+cu128)\n",
            "Collecting pyarrow>=15.0.0 (from datasets==3.0.2)\n",
            "  Downloading pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets==3.0.2)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting pandas (from datasets==3.0.2)\n",
            "  Downloading pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (91 kB)\n",
            "Collecting xxhash (from datasets==3.0.2)\n",
            "  Downloading xxhash-3.6.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets==3.0.2)\n",
            "  Downloading multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets==3.0.2) (2024.6.1)\n",
            "Collecting aiohttp (from datasets==3.0.2)\n",
            "  Downloading aiohttp-3.13.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting tyro>=0.5.11 (from trl==0.11.4)\n",
            "  Downloading tyro-0.9.35-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting av (from qwen-vl-utils==0.0.8)\n",
            "  Downloading av-16.0.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from qwen-vl-utils==0.0.8) (11.0.0)\n",
            "Collecting click!=8.0.0,>=7.1 (from wandb==0.18.5)\n",
            "  Downloading click-8.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting docker-pycreds>=0.4.0 (from wandb==0.18.5)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb==0.18.5)\n",
            "  Downloading gitpython-3.1.45-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb==0.18.5) (4.5.0)\n",
            "Collecting protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 (from wandb==0.18.5)\n",
            "  Downloading protobuf-5.29.5-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
            "Collecting sentry-sdk>=2.0.0 (from wandb==0.18.5)\n",
            "  Downloading sentry_sdk-2.44.0-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Collecting setproctitle (from wandb==0.18.5)\n",
            "  Downloading setproctitle-1.3.7-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from wandb==0.18.5) (80.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.46.2) (4.15.0)\n",
            "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.23.2->transformers==4.46.2)\n",
            "  Downloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.46.2) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.46.2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.46.2) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.46.2) (2025.10.5)\n",
            "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp->datasets==3.0.2)\n",
            "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting aiosignal>=1.4.0 (from aiohttp->datasets==3.0.2)\n",
            "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==3.0.2) (25.4.0)\n",
            "Collecting frozenlist>=1.1.1 (from aiohttp->datasets==3.0.2)\n",
            "  Downloading frozenlist-1.8.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (20 kB)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets==3.0.2)\n",
            "  Downloading multidict-6.7.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
            "Collecting propcache>=0.2.0 (from aiohttp->datasets==3.0.2)\n",
            "  Downloading propcache-0.4.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
            "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets==3.0.2)\n",
            "  Downloading yarl-1.22.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (75 kB)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/lib/python3/dist-packages (from docker-pycreds>=0.4.0->wandb==0.18.5) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb==0.18.5)\n",
            "  Downloading gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb==0.18.5)\n",
            "  Downloading smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==1.0.1) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==1.0.1) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==1.0.1) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==1.0.1) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==1.0.1) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==1.0.1) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==1.0.1) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==1.0.1) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==1.0.1) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==1.0.1) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==1.0.1) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==1.0.1) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==1.0.1) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==1.0.1) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==1.0.1) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==1.0.1) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==1.0.1) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==1.0.1) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.10.0->accelerate==1.0.1) (1.3.0)\n",
            "Collecting docstring-parser>=0.15 (from tyro>=0.5.11->trl==0.11.4)\n",
            "  Downloading docstring_parser-0.17.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting rich>=11.1.0 (from tyro>=0.5.11->trl==0.11.4)\n",
            "  Downloading rich-14.2.0-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl==0.11.4)\n",
            "  Downloading shtab-1.7.2-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting typeguard>=4.0.0 (from tyro>=0.5.11->trl==0.11.4)\n",
            "  Downloading typeguard-4.4.4-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting markdown-it-py>=2.2.0 (from rich>=11.1.0->tyro>=0.5.11->trl==0.11.4)\n",
            "  Downloading markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.11.4) (2.19.2)\n",
            "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl==0.11.4)\n",
            "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.10.0->accelerate==1.0.1) (3.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==3.0.2) (2.9.0.post0)\n",
            "Collecting pytz>=2020.1 (from pandas->datasets==3.0.2)\n",
            "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting tzdata>=2022.7 (from pandas->datasets==3.0.2)\n",
            "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Downloading transformers-4.46.2-py3-none-any.whl (10.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm0:00:01\u001b[0m0:01\u001b[0m\n",
            "\u001b[?25hDownloading accelerate-1.0.1-py3-none-any.whl (330 kB)\n",
            "Downloading peft-0.13.2-py3-none-any.whl (320 kB)\n",
            "Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m76.8 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.0.2-py3-none-any.whl (472 kB)\n",
            "Downloading trl-0.11.4-py3-none-any.whl (316 kB)\n",
            "Downloading qwen_vl_utils-0.0.8-py3-none-any.whl (5.9 kB)\n",
            "Downloading wandb-0.18.5-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "Downloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m75.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
            "Downloading protobuf-5.29.5-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
            "Downloading tokenizers-0.20.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading aiohttp-3.13.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m60.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multidict-6.7.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (256 kB)\n",
            "Downloading yarl-1.22.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (377 kB)\n",
            "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
            "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
            "Downloading click-8.3.0-py3-none-any.whl (107 kB)\n",
            "Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Downloading frozenlist-1.8.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (242 kB)\n",
            "Downloading gitpython-3.1.45-py3-none-any.whl (208 kB)\n",
            "Downloading gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
            "Downloading smmap-5.0.2-py3-none-any.whl (24 kB)\n",
            "Downloading propcache-0.4.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (221 kB)\n",
            "Downloading pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (47.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m95.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m6m0:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading regex-2025.11.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (803 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.5/803.5 kB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n",
            "Downloading sentry_sdk-2.44.0-py2.py3-none-any.whl (402 kB)\n",
            "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
            "Downloading tyro-0.9.35-py3-none-any.whl (132 kB)\n",
            "Downloading docstring_parser-0.17.0-py3-none-any.whl (36 kB)\n",
            "Downloading rich-14.2.0-py3-none-any.whl (243 kB)\n",
            "Downloading markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
            "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
            "Downloading shtab-1.7.2-py3-none-any.whl (14 kB)\n",
            "Downloading typeguard-4.4.4-py3-none-any.whl (34 kB)\n",
            "Downloading av-16.0.1-cp312-cp312-manylinux_2_28_x86_64.whl (40.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 MB\u001b[0m \u001b[31m104.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m61.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
            "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
            "Downloading setproctitle-1.3.7-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (32 kB)\n",
            "Downloading xxhash-3.6.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (193 kB)\n",
            "Installing collected packages: pytz, xxhash, tzdata, typeguard, tqdm, smmap, shtab, setproctitle, sentry-sdk, safetensors, regex, pyarrow, protobuf, propcache, multidict, mdurl, hf-xet, frozenlist, docstring-parser, docker-pycreds, dill, click, av, aiohappyeyeballs, yarl, qwen-vl-utils, pandas, multiprocess, markdown-it-py, huggingface-hub, gitdb, aiosignal, tokenizers, rich, gitpython, aiohttp, wandb, tyro, transformers, bitsandbytes, accelerate, peft, datasets, trl\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44/44\u001b[0m [trl][0m [trl][0m [datasets]e]s]ub]\n",
            "\u001b[1A\u001b[2KSuccessfully installed accelerate-1.0.1 aiohappyeyeballs-2.6.1 aiohttp-3.13.2 aiosignal-1.4.0 av-16.0.1 bitsandbytes-0.44.1 click-8.3.0 datasets-3.0.2 dill-0.3.8 docker-pycreds-0.4.0 docstring-parser-0.17.0 frozenlist-1.8.0 gitdb-4.0.12 gitpython-3.1.45 hf-xet-1.2.0 huggingface-hub-0.36.0 markdown-it-py-4.0.0 mdurl-0.1.2 multidict-6.7.0 multiprocess-0.70.16 pandas-2.3.3 peft-0.13.2 propcache-0.4.1 protobuf-5.29.5 pyarrow-22.0.0 pytz-2025.2 qwen-vl-utils-0.0.8 regex-2025.11.3 rich-14.2.0 safetensors-0.6.2 sentry-sdk-2.44.0 setproctitle-1.3.7 shtab-1.7.2 smmap-5.0.2 tokenizers-0.20.3 tqdm-4.67.1 transformers-4.46.2 trl-0.11.4 typeguard-4.4.4 tyro-0.9.35 tzdata-2025.2 wandb-0.18.5 xxhash-3.6.0 yarl-1.22.0\n"
          ]
        }
      ],
      "source": [
        "# 1. Clean slate\n",
        "!pip uninstall transformers trl datasets bitsandbytes peft qwen-vl-utils wandb accelerate -y\n",
        "\n",
        "# 2. Install stable versions that should work together\n",
        "!pip install transformers==4.46.2 \\\n",
        "             accelerate==1.0.1 \\\n",
        "             peft==0.13.2 \\\n",
        "             bitsandbytes==0.44.1 \\\n",
        "             datasets==3.0.2 \\\n",
        "             trl==0.11.4 \\\n",
        "             qwen-vl-utils==0.0.8 \\\n",
        "             wandb==0.18.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1788bd6c-4c7a-442f-b6db-35322a5a0f9c",
      "metadata": {
        "id": "1788bd6c-4c7a-442f-b6db-35322a5a0f9c",
        "outputId": "b1ff649b-5b7c-4450-ee17-c70a6e954462"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers==4.46.2 in /usr/local/lib/python3.12/dist-packages (4.46.2)\n",
            "Requirement already satisfied: accelerate==1.0.1 in /usr/local/lib/python3.12/dist-packages (1.0.1)\n",
            "Requirement already satisfied: peft==0.13.2 in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
            "Requirement already satisfied: bitsandbytes==0.44.1 in /usr/local/lib/python3.12/dist-packages (0.44.1)\n",
            "Requirement already satisfied: datasets==3.0.2 in /usr/local/lib/python3.12/dist-packages (3.0.2)\n",
            "Requirement already satisfied: trl==0.11.4 in /usr/local/lib/python3.12/dist-packages (0.11.4)\n",
            "Requirement already satisfied: qwen-vl-utils==0.0.8 in /usr/local/lib/python3.12/dist-packages (0.0.8)\n",
            "Requirement already satisfied: wandb==0.18.5 in /usr/local/lib/python3.12/dist-packages (0.18.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers==4.46.2) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.12/dist-packages (from transformers==4.46.2) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.46.2) (2.1.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.46.2) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.46.2) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.46.2) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers==4.46.2) (2.32.5)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.46.2) (0.6.2)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.12/dist-packages (from transformers==4.46.2) (0.20.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers==4.46.2) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate==1.0.1) (7.1.0)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from accelerate==1.0.1) (2.8.0+cu128)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets==3.0.2) (22.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets==3.0.2) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets==3.0.2) (2.3.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets==3.0.2) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets==3.0.2) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets==3.0.2) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from datasets==3.0.2) (3.13.2)\n",
            "Requirement already satisfied: tyro>=0.5.11 in /usr/local/lib/python3.12/dist-packages (from trl==0.11.4) (0.9.35)\n",
            "Requirement already satisfied: av in /usr/local/lib/python3.12/dist-packages (from qwen-vl-utils==0.0.8) (16.0.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from qwen-vl-utils==0.0.8) (11.0.0)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.12/dist-packages (from wandb==0.18.5) (8.3.0)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from wandb==0.18.5) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb==0.18.5) (3.1.45)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb==0.18.5) (4.5.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.12/dist-packages (from wandb==0.18.5) (5.29.5)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb==0.18.5) (2.44.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.12/dist-packages (from wandb==0.18.5) (1.3.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from wandb==0.18.5) (80.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.46.2) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.46.2) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.46.2) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.46.2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.46.2) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.46.2) (2025.10.5)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==3.0.2) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==3.0.2) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==3.0.2) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==3.0.2) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==3.0.2) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==3.0.2) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==3.0.2) (1.22.0)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/lib/python3/dist-packages (from docker-pycreds>=0.4.0->wandb==0.18.5) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb==0.18.5) (4.0.12)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb==0.18.5) (5.0.2)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==1.0.1) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==1.0.1) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==1.0.1) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==1.0.1) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==1.0.1) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==1.0.1) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==1.0.1) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==1.0.1) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==1.0.1) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==1.0.1) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==1.0.1) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==1.0.1) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==1.0.1) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==1.0.1) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==1.0.1) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==1.0.1) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==1.0.1) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==1.0.1) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.10.0->accelerate==1.0.1) (1.3.0)\n",
            "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.12/dist-packages (from tyro>=0.5.11->trl==0.11.4) (0.17.0)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.12/dist-packages (from tyro>=0.5.11->trl==0.11.4) (14.2.0)\n",
            "Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from tyro>=0.5.11->trl==0.11.4) (1.7.2)\n",
            "Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from tyro>=0.5.11->trl==0.11.4) (4.4.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.11.4) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.11.4) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl==0.11.4) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.10.0->accelerate==1.0.1) (3.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==3.0.2) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==3.0.2) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==3.0.2) (2025.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers==4.46.2 \\\n",
        "             accelerate==1.0.1 \\\n",
        "             peft==0.13.2 \\\n",
        "             bitsandbytes==0.44.1 \\\n",
        "             datasets==3.0.2 \\\n",
        "             trl==0.11.4 \\\n",
        "             qwen-vl-utils==0.0.8 \\\n",
        "             wandb==0.18.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b51f76d-5391-41df-ab95-375c9599c8e3",
      "metadata": {
        "id": "1b51f76d-5391-41df-ab95-375c9599c8e3",
        "outputId": "e016ac6d-b6d3-4af8-b098-cbcadf346785"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting matplotlib\n",
            "  Downloading matplotlib-3.10.7-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: IPython in /usr/local/lib/python3.12/dist-packages (9.6.0)\n",
            "Collecting contourpy>=1.0.1 (from matplotlib)\n",
            "  Downloading contourpy-1.3.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.5 kB)\n",
            "Collecting cycler>=0.10 (from matplotlib)\n",
            "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting fonttools>=4.22.0 (from matplotlib)\n",
            "  Downloading fonttools-4.60.1-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (112 kB)\n",
            "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
            "  Downloading kiwisolver-1.4.9-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.1.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=3 in /usr/lib/python3/dist-packages (from matplotlib) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.12/dist-packages (from IPython) (5.2.1)\n",
            "Requirement already satisfied: ipython-pygments-lexers in /usr/local/lib/python3.12/dist-packages (from IPython) (1.1.1)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.12/dist-packages (from IPython) (0.19.2)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.12/dist-packages (from IPython) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.12/dist-packages (from IPython) (4.9.0)\n",
            "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /usr/local/lib/python3.12/dist-packages (from IPython) (3.0.52)\n",
            "Requirement already satisfied: pygments>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from IPython) (2.19.2)\n",
            "Requirement already satisfied: stack_data in /usr/local/lib/python3.12/dist-packages (from IPython) (0.6.3)\n",
            "Requirement already satisfied: traitlets>=5.13.0 in /usr/local/lib/python3.12/dist-packages (from IPython) (5.14.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from prompt_toolkit<3.1.0,>=3.0.41->IPython) (0.2.14)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.12/dist-packages (from jedi>=0.16->IPython) (0.8.5)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.12/dist-packages (from pexpect>4.3->IPython) (0.7.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: executing>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from stack_data->IPython) (2.2.1)\n",
            "Requirement already satisfied: asttokens>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from stack_data->IPython) (3.0.0)\n",
            "Requirement already satisfied: pure-eval in /usr/local/lib/python3.12/dist-packages (from stack_data->IPython) (0.2.3)\n",
            "Downloading matplotlib-3.10.7-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m91.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading contourpy-1.3.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (362 kB)\n",
            "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
            "Downloading fonttools-4.60.1-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (4.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m104.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kiwisolver-1.4.9-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5/5\u001b[0m [matplotlib]5\u001b[0m [matplotlib]\n",
            "\u001b[1A\u001b[2KSuccessfully installed contourpy-1.3.3 cycler-0.12.1 fonttools-4.60.1 kiwisolver-1.4.9 matplotlib-3.10.7\n"
          ]
        }
      ],
      "source": [
        "!pip install -U -q git+https://github.com/huggingface/transformers.git git+https://github.com/huggingface/trl.git datasets bitsandbytes peft qwen-vl-utils wandb accelerate\n",
        "!pip install matplotlib IPython\n",
        "!pip install -q torch==2.4.1+cu121 torchvision==0.19.1+cu121 torchaudio==2.4.1+cu121 --extra-index-url https://download.pytorch.org/whl/cu121"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8bf92b6-3a25-437a-acbc-8d9fbe04666b",
      "metadata": {
        "id": "f8bf92b6-3a25-437a-acbc-8d9fbe04666b",
        "outputId": "df0784a3-3240-4d0c-dd85-acb5b49b478f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: torch 2.4.1+cu121\n",
            "Uninstalling torch-2.4.1+cu121:\n",
            "  Successfully uninstalled torch-2.4.1+cu121\n",
            "Found existing installation: torchvision 0.19.1+cu121\n",
            "Uninstalling torchvision-0.19.1+cu121:\n",
            "  Successfully uninstalled torchvision-0.19.1+cu121\n",
            "Found existing installation: torchaudio 2.4.1+cu121\n",
            "Uninstalling torchaudio-2.4.1+cu121:\n",
            "  Successfully uninstalled torchaudio-2.4.1+cu121\n",
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu121\n",
            "Collecting torch==2.4.1+cu121\n",
            "  Using cached https://download.pytorch.org/whl/cu121/torch-2.4.1%2Bcu121-cp312-cp312-linux_x86_64.whl (798.9 MB)\n",
            "Collecting torchvision==0.19.1+cu121\n",
            "  Using cached https://download.pytorch.org/whl/cu121/torchvision-0.19.1%2Bcu121-cp312-cp312-linux_x86_64.whl (7.1 MB)\n",
            "Collecting torchaudio==2.4.1+cu121\n",
            "  Using cached https://download.pytorch.org/whl/cu121/torchaudio-2.4.1%2Bcu121-cp312-cp312-linux_x86_64.whl (3.4 MB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.4.1+cu121) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.1+cu121) (4.15.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch==2.4.1+cu121) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.4.1+cu121) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.1+cu121) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.4.1+cu121) (2024.6.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.4.1+cu121) (80.9.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.1+cu121) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.1+cu121) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.1+cu121) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.1+cu121) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.1+cu121) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.1+cu121) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.1+cu121) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.1+cu121) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.1+cu121) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.1+cu121) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.1+cu121) (12.1.105)\n",
            "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.1+cu121) (3.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision==0.19.1+cu121) (2.1.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision==0.19.1+cu121) (11.0.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.1+cu121) (12.8.93)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.4.1+cu121) (3.0.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch==2.4.1+cu121) (1.3.0)\n",
            "Installing collected packages: torch, torchvision, torchaudio\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [torchaudio]3\u001b[0m [torchaudio]]\n",
            "\u001b[1A\u001b[2KSuccessfully installed torch-2.4.1+cu121 torchaudio-2.4.1+cu121 torchvision-0.19.1+cu121\n"
          ]
        }
      ],
      "source": [
        "# Cell 1 - Force reinstall PyTorch 2.4.1\n",
        "!pip uninstall torch torchvision torchaudio -y\n",
        "!pip install torch==2.4.1+cu121 torchvision==0.19.1+cu121 torchaudio==2.4.1+cu121 --extra-index-url https://download.pytorch.org/whl/cu121"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbf53d63-2e0b-497e-94b6-fe4c19eb19ea",
      "metadata": {
        "id": "dbf53d63-2e0b-497e-94b6-fe4c19eb19ea",
        "outputId": "013b9e84-1063-42aa-a479-3c973be2114e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: transformers 5.0.0.dev0\n",
            "Uninstalling transformers-5.0.0.dev0:\n",
            "  Successfully uninstalled transformers-5.0.0.dev0\n",
            "Found existing installation: trl 0.26.0.dev0\n",
            "Uninstalling trl-0.26.0.dev0:\n",
            "  Successfully uninstalled trl-0.26.0.dev0\n",
            "Found existing installation: datasets 4.4.1\n",
            "Uninstalling datasets-4.4.1:\n",
            "  Successfully uninstalled datasets-4.4.1\n",
            "Found existing installation: bitsandbytes 0.48.2\n",
            "Uninstalling bitsandbytes-0.48.2:\n",
            "  Successfully uninstalled bitsandbytes-0.48.2\n",
            "Found existing installation: peft 0.18.0\n",
            "Uninstalling peft-0.18.0:\n",
            "  Successfully uninstalled peft-0.18.0\n",
            "Found existing installation: qwen-vl-utils 0.0.14\n",
            "Uninstalling qwen-vl-utils-0.0.14:\n",
            "  Successfully uninstalled qwen-vl-utils-0.0.14\n",
            "Found existing installation: wandb 0.23.0\n",
            "Uninstalling wandb-0.23.0:\n",
            "  Successfully uninstalled wandb-0.23.0\n",
            "Found existing installation: accelerate 1.11.0\n",
            "Uninstalling accelerate-1.11.0:\n",
            "  Successfully uninstalled accelerate-1.11.0\n",
            "Collecting transformers==4.46.2\n",
            "  Using cached transformers-4.46.2-py3-none-any.whl.metadata (44 kB)\n",
            "Collecting accelerate==1.0.1\n",
            "  Using cached accelerate-1.0.1-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting peft==0.13.2\n",
            "  Using cached peft-0.13.2-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting bitsandbytes==0.44.1\n",
            "  Using cached bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\n",
            "Collecting datasets==3.0.2\n",
            "  Using cached datasets-3.0.2-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting trl==0.11.4\n",
            "  Using cached trl-0.11.4-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting qwen-vl-utils==0.0.8\n",
            "  Using cached qwen_vl_utils-0.0.8-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting wandb==0.18.5\n",
            "  Using cached wandb-0.18.5-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers==4.46.2) (3.20.0)\n",
            "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers==4.46.2)\n",
            "  Using cached huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.46.2) (2.1.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.46.2) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.46.2) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.46.2) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers==4.46.2) (2.32.5)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.46.2) (0.6.2)\n",
            "Collecting tokenizers<0.21,>=0.20 (from transformers==4.46.2)\n",
            "  Using cached tokenizers-0.20.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers==4.46.2) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate==1.0.1) (7.1.0)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from accelerate==1.0.1) (2.4.1+cu121)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets==3.0.2) (22.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets==3.0.2) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets==3.0.2) (2.3.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets==3.0.2) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets==3.0.2) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets==3.0.2) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from datasets==3.0.2) (3.13.2)\n",
            "Requirement already satisfied: tyro>=0.5.11 in /usr/local/lib/python3.12/dist-packages (from trl==0.11.4) (0.9.35)\n",
            "Requirement already satisfied: av in /usr/local/lib/python3.12/dist-packages (from qwen-vl-utils==0.0.8) (16.0.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from qwen-vl-utils==0.0.8) (11.0.0)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.12/dist-packages (from wandb==0.18.5) (8.3.0)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from wandb==0.18.5) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb==0.18.5) (3.1.45)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb==0.18.5) (4.5.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.12/dist-packages (from wandb==0.18.5) (5.29.5)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb==0.18.5) (2.44.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.12/dist-packages (from wandb==0.18.5) (1.3.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from wandb==0.18.5) (80.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.46.2) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.46.2) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.46.2) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.46.2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.46.2) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.46.2) (2025.10.5)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==3.0.2) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==3.0.2) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==3.0.2) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==3.0.2) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==3.0.2) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==3.0.2) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==3.0.2) (1.22.0)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/lib/python3/dist-packages (from docker-pycreds>=0.4.0->wandb==0.18.5) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb==0.18.5) (4.0.12)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb==0.18.5) (5.0.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==1.0.1) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==1.0.1) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==1.0.1) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==1.0.1) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==1.0.1) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==1.0.1) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==1.0.1) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==1.0.1) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==1.0.1) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==1.0.1) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==1.0.1) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==1.0.1) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==1.0.1) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==1.0.1) (12.1.105)\n",
            "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==1.0.1) (3.0.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate==1.0.1) (12.8.93)\n",
            "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.12/dist-packages (from tyro>=0.5.11->trl==0.11.4) (0.17.0)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.12/dist-packages (from tyro>=0.5.11->trl==0.11.4) (14.2.0)\n",
            "Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from tyro>=0.5.11->trl==0.11.4) (1.7.2)\n",
            "Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from tyro>=0.5.11->trl==0.11.4) (4.4.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.11.4) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.11.4) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl==0.11.4) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.10.0->accelerate==1.0.1) (3.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==3.0.2) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==3.0.2) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==3.0.2) (2025.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch>=1.10.0->accelerate==1.0.1) (1.3.0)\n",
            "Using cached transformers-4.46.2-py3-none-any.whl (10.0 MB)\n",
            "Using cached accelerate-1.0.1-py3-none-any.whl (330 kB)\n",
            "Using cached peft-0.13.2-py3-none-any.whl (320 kB)\n",
            "Using cached bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\n",
            "Using cached datasets-3.0.2-py3-none-any.whl (472 kB)\n",
            "Using cached trl-0.11.4-py3-none-any.whl (316 kB)\n",
            "Using cached qwen_vl_utils-0.0.8-py3-none-any.whl (5.9 kB)\n",
            "Using cached wandb-0.18.5-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n",
            "Using cached huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
            "Using cached tokenizers-0.20.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "Installing collected packages: qwen-vl-utils, huggingface-hub, wandb, tokenizers, transformers, datasets, bitsandbytes, accelerate, trl, peft\n",
            "\u001b[2K  Attempting uninstall: huggingface-hub\n",
            "\u001b[2K    Found existing installation: huggingface_hub 1.1.4\n",
            "\u001b[2K    Uninstalling huggingface_hub-1.1.4:\n",
            "\u001b[2K      Successfully uninstalled huggingface_hub-1.1.4\n",
            "\u001b[2K  Attempting uninstall: tokenizersm━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/10\u001b[0m [wandb]gface-hub]\n",
            "\u001b[2K    Found existing installation: tokenizers 0.22.1━━━━━━━━━━━━\u001b[0m \u001b[32m 2/10\u001b[0m [wandb]\n",
            "\u001b[2K    Uninstalling tokenizers-0.22.1:━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/10\u001b[0m [wandb]\n",
            "\u001b[2K      Successfully uninstalled tokenizers-0.22.1━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/10\u001b[0m [wandb]\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/10\u001b[0m [peft]2m 9/10\u001b[0m [peft]ndbytes]\n",
            "\u001b[1A\u001b[2KSuccessfully installed accelerate-1.0.1 bitsandbytes-0.44.1 datasets-3.0.2 huggingface-hub-0.36.0 peft-0.13.2 qwen-vl-utils-0.0.8 tokenizers-0.20.3 transformers-4.46.2 trl-0.11.4 wandb-0.18.5\n"
          ]
        }
      ],
      "source": [
        "# 1. Clean slate\n",
        "!pip uninstall transformers trl datasets bitsandbytes peft qwen-vl-utils wandb accelerate -y\n",
        "\n",
        "# 2. Install stable versions that should work together\n",
        "!pip install transformers==4.46.2 \\\n",
        "             accelerate==1.0.1 \\\n",
        "             peft==0.13.2 \\\n",
        "             bitsandbytes==0.44.1 \\\n",
        "             datasets==3.0.2 \\\n",
        "             trl==0.11.4 \\\n",
        "             qwen-vl-utils==0.0.8 \\\n",
        "             wandb==0.18.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a43d22e9-64f5-4808-ba15-e880b60a5abd",
      "metadata": {
        "id": "a43d22e9-64f5-4808-ba15-e880b60a5abd"
      },
      "outputs": [],
      "source": [
        "# Cell 2 - MUST restart kernel\n",
        "import os\n",
        "os._exit(0)  # This forces kernel restart"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1b61219-d49d-46c1-b63f-948a3b227a1e",
      "metadata": {
        "id": "e1b61219-d49d-46c1-b63f-948a3b227a1e",
        "outputId": "6ad3af5d-e196-4016-e54f-8a5fbd328266"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== VERSION CHECK ===\n",
            "⚠️ torch: 2.4.1+cu121 (expected: 2.4.1)\n",
            "⚠️ transformers: 4.46.2 (expected: 4.47.0.dev0)\n",
            "⚠️ trl: 0.11.4 (expected: 0.12.0.dev0)\n",
            "✅ datasets: 3.0.2 (expected: 3.0.2)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:230: UserWarning: \n",
            "NVIDIA GeForce RTX 5090 with CUDA capability sm_120 is not compatible with the current PyTorch installation.\n",
            "The current PyTorch install supports CUDA capabilities sm_50 sm_60 sm_70 sm_75 sm_80 sm_86 sm_90.\n",
            "If you want to use the NVIDIA GeForce RTX 5090 GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
            "\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ bitsandbytes: 0.44.1 (expected: 0.44.1)\n",
            "✅ peft: 0.13.2 (expected: 0.13.2)\n",
            "⚠️ qwen-vl-utils: unknown (expected: 0.0.8)\n",
            "✅ wandb: 0.18.5 (expected: 0.18.5)\n",
            "✅ accelerate: 1.0.1 (expected: 1.0.1)\n"
          ]
        }
      ],
      "source": [
        "import importlib\n",
        "# Cell 3 - Verify exact versions from notebook requirements\n",
        "requirements = {\n",
        "    'torch': '2.4.1',\n",
        "    'transformers': '4.47.0.dev0',\n",
        "    'trl': '0.12.0.dev0',\n",
        "    'datasets': '3.0.2',\n",
        "    'bitsandbytes': '0.44.1',\n",
        "    'peft': '0.13.2',\n",
        "    'qwen-vl-utils': '0.0.8',\n",
        "    'wandb': '0.18.5',\n",
        "    'accelerate': '1.0.1'\n",
        "}\n",
        "\n",
        "print(\"=== VERSION CHECK ===\")\n",
        "for package, expected in requirements.items():\n",
        "    try:\n",
        "        module = importlib.import_module(package.replace('-', '_'))\n",
        "        actual = getattr(module, '__version__', 'unknown')\n",
        "        if 'dev' in expected:\n",
        "            # Dev versions just check if it's dev\n",
        "            status = \"✅\" if 'dev' in actual else \"⚠️\"\n",
        "        else:\n",
        "            status = \"✅\" if actual == expected else \"⚠️\"\n",
        "        print(f\"{status} {package}: {actual} (expected: {expected})\")\n",
        "    except ImportError:\n",
        "        print(f\"❌ {package}: NOT INSTALLED (expected: {expected})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7779749-605a-4621-8744-5992de86c641",
      "metadata": {
        "id": "a7779749-605a-4621-8744-5992de86c641",
        "outputId": "a012826c-ae00-4b0b-aee2-5c0eda605db5"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>.container { width:100% !important; }</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from IPython.display import display, HTML\n",
        "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5f0916c-1371-4a81-a277-850e19782b90",
      "metadata": {
        "id": "c5f0916c-1371-4a81-a277-850e19782b90",
        "outputId": "0ec18508-024b-4362-d35f-28e5bb0c4935"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting hf_transfer\n",
            "  Downloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Downloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: hf_transfer\n",
            "Successfully installed hf_transfer-0.1.9\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install hf_transfer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da4e124d-3107-4bf4-9dc9-576a7ca44603",
      "metadata": {
        "id": "da4e124d-3107-4bf4-9dc9-576a7ca44603"
      },
      "outputs": [],
      "source": [
        "# 15 November\n",
        "\n",
        "# ok my goal to understand how tokenizetion happens\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "567c3bc6-1ee3-4885-b69a-30c4b6d5ae94",
      "metadata": {
        "id": "567c3bc6-1ee3-4885-b69a-30c4b6d5ae94",
        "outputId": "96799814-edce-45ab-eed5-359c3779c66d",
        "colab": {
          "referenced_widgets": [
            "cfc0c96843a441f6b9fc1fcfc137af0c",
            "fd6a935e70ec4fd8b86d7961a1b064fb",
            "6cb14df78eae4df5b2328489760a8000",
            "9a7185e190624ccaa97deb110ac2a3ba",
            "4a49256bcccd4cce955f009580fcb98a",
            "a03926057c8046c196280bdbbfc7269b"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cfc0c96843a441f6b9fc1fcfc137af0c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fd6a935e70ec4fd8b86d7961a1b064fb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00000-of-00002.parquet:   0%|          | 0.00/291M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6cb14df78eae4df5b2328489760a8000",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00001-of-00002.parquet:   0%|          | 0.00/285M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9a7185e190624ccaa97deb110ac2a3ba",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/val-00000-of-00001.parquet:   0%|          | 0.00/64.2M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4a49256bcccd4cce955f009580fcb98a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/1083 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a03926057c8046c196280bdbbfc7269b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating val split:   0%|          | 0/123 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "dataset_id = \"openfoodfacts/nutrition-table-detection\"\n",
        "\n",
        "nutrition_data = load_dataset(dataset_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3db6f868-afd7-4998-b0da-699de13c2a86",
      "metadata": {
        "id": "3db6f868-afd7-4998-b0da-699de13c2a86"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "import torch\n",
        "from transformers import Qwen2VLForConditionalGeneration, Qwen2VLProcessor\n",
        "from qwen_vl_utils import vision_process\n",
        "from qwen_vl_utils import process_vision_info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1e474fe-28a8-47d1-abbf-c3ac4dbd3791",
      "metadata": {
        "id": "a1e474fe-28a8-47d1-abbf-c3ac4dbd3791"
      },
      "outputs": [],
      "source": [
        "from PIL import Image, ImageDraw\n",
        "from matplotlib import pyplot as plt\n",
        "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
        "from qwen_vl_utils import process_vision_info\n",
        "import torch\n",
        "import peft\n",
        "import bitsandbytes as bnb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c07270a-2ab9-4e92-baa2-6f6509c9abf6",
      "metadata": {
        "id": "4c07270a-2ab9-4e92-baa2-6f6509c9abf6",
        "outputId": "86b23d6d-087b-4a86-9cfc-804d4f63d7f8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1675"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Allocated: 0.00 GB\n",
            "Reserved: 0.00 GB\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import gc\n",
        "\n",
        "# Clear any existing models\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "print(f\"Allocated: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")\n",
        "print(f\"Reserved: {torch.cuda.memory_reserved()/1024**3:.2f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "447fe86c-2c2a-4197-a536-2d53a490c24d",
      "metadata": {
        "id": "447fe86c-2c2a-4197-a536-2d53a490c24d"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "{\n",
        "    \"role\":\"user\",\n",
        "    \"content\":[\n",
        "        {\n",
        "            \"type\":\"image\",\n",
        "            \"image\":\"https://images.chesscomfiles.com/uploads/v1/images_users/tiny_mce/Ognian_Mikov/php2nnXz9.png\",\n",
        "\n",
        "        },\n",
        "        {\n",
        "            \"type\":\"text\",\"text\":\"Provide surrounding boxes for all pawns in this image and respond in <|bbox|> format\"\n",
        "        },\n",
        "    ]\n",
        "}\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "400183ac-c8c0-4538-a26a-59b0590468c7",
      "metadata": {
        "id": "400183ac-c8c0-4538-a26a-59b0590468c7",
        "outputId": "e5be0358-7d88-434b-d42e-00b7693b1471",
        "colab": {
          "referenced_widgets": [
            "e5b484f105f741abbaa473e07613d0ad",
            "f7e6d39b695148109e60cbb4ff0822a5",
            "a024fd8745ae48ed9bae105be5bd6dce",
            "522b0148551e4079aae8d9504243607f",
            "74412a2ed15a4041baed52d5dd41796f",
            "0942db92ca924522a35f9c80a1624873"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e5b484f105f741abbaa473e07613d0ad",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/347 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f7e6d39b695148109e60cbb4ff0822a5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a024fd8745ae48ed9bae105be5bd6dce",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "522b0148551e4079aae8d9504243607f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "74412a2ed15a4041baed52d5dd41796f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0942db92ca924522a35f9c80a1624873",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "chat_template.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")\n",
        "\n",
        "\n",
        "text = processor.apply_chat_template(\n",
        "    messages, tokenize=False, add_generation_prompt=True\n",
        ")\n",
        "\n",
        "image_inputs,video_inputs = process_vision_info(messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "214ecd66-0ad5-4613-9521-fab74d138a7b",
      "metadata": {
        "id": "214ecd66-0ad5-4613-9521-fab74d138a7b",
        "outputId": "d014cb42-d488-4f4c-9706-6e66e38620ad"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[<PIL.Image.Image image mode=RGB size=1932x1092>]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ok so I know everything what happens till here\n",
        "image_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b6ebb85-2c88-49ec-bde6-b87b50e13f48",
      "metadata": {
        "id": "1b6ebb85-2c88-49ec-bde6-b87b50e13f48"
      },
      "outputs": [],
      "source": [
        "# ok now I want to understand how tokenization happens\n",
        "# like in next line\n",
        "# inputs = processor(text=[text],images=image_inputs,videos=video_inputs,padding=True,return_tensors=\"pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7211332-d320-4f5e-ab1f-96f225ee6a21",
      "metadata": {
        "id": "e7211332-d320-4f5e-ab1f-96f225ee6a21",
        "outputId": "208f00e1-625b-45c8-80ff-33eeccca93b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'transformers.models.qwen2_vl.processing_qwen2_vl.Qwen2VLProcessor'>\n",
            "['__annotations__', '__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_auto_class', '_create_repo', '_get_arguments_from_pretrained', '_get_files_timestamps', '_merge_kwargs', '_upload_modified_files', 'apply_chat_template', 'attributes', 'batch_decode', 'chat_template', 'decode', 'feature_extractor_class', 'from_args_and_dict', 'from_pretrained', 'get_processor_dict', 'image_processor', 'image_processor_class', 'model_input_names', 'optional_attributes', 'optional_call_args', 'prepare_and_validate_optional_call_args', 'push_to_hub', 'register_for_auto_class', 'save_pretrained', 'to_dict', 'to_json_file', 'to_json_string', 'tokenizer', 'tokenizer_class', 'valid_kwargs', 'validate_init_kwargs']\n"
          ]
        }
      ],
      "source": [
        "print(type(processor))\n",
        "print(dir(processor))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1200521-552e-4a8a-ac50-4a0146346fd5",
      "metadata": {
        "id": "b1200521-552e-4a8a-ac50-4a0146346fd5"
      },
      "outputs": [],
      "source": [
        "inputs = processor(text=[text],images=image_inputs,videos=video_inputs,padding=True,return_tensors=\"pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a909ba4-ea66-4fb2-a052-e66016f3e725",
      "metadata": {
        "id": "8a909ba4-ea66-4fb2-a052-e66016f3e725",
        "outputId": "5905c071-c3b1-4d02-93f6-920759108d87"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[151644,   8948,    198,  ..., 151644,  77091,    198]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]]), 'pixel_values': tensor([[1.6822, 1.6822, 1.6822,  ..., 1.5060, 1.5060, 1.5060],\n",
              "        [1.6822, 1.6822, 1.6822,  ..., 1.5060, 1.5060, 1.5060],\n",
              "        [1.6822, 1.6822, 1.6822,  ..., 1.5060, 1.5060, 1.5060],\n",
              "        ...,\n",
              "        [1.6822, 1.6822, 1.6822,  ..., 1.5060, 1.5060, 1.5060],\n",
              "        [1.6822, 1.6822, 1.6822,  ..., 1.5060, 1.5060, 1.5060],\n",
              "        [1.6822, 1.6822, 1.6822,  ..., 1.5060, 1.5060, 1.5060]]), 'image_grid_thw': tensor([[  1,  78, 138]])}"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bcd80679-ed26-4c0c-ab5e-f414c34b6b43",
      "metadata": {
        "id": "bcd80679-ed26-4c0c-ab5e-f414c34b6b43",
        "outputId": "9639868d-59c8-43ab-efd4-c0a964b7aa97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    def __call__(\n",
            "        self,\n",
            "        images: ImageInput = None,\n",
            "        text: Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]] = None,\n",
            "        videos: VideoInput = None,\n",
            "        **kwargs: Unpack[Qwen2VLProcessorKwargs],\n",
            "    ) -> BatchFeature:\n",
            "        \"\"\"\n",
            "        Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n",
            "        and `kwargs` arguments to Qwen2TokenizerFast's [`~Qwen2TokenizerFast.__call__`] if `text` is not `None` to encode\n",
            "        the text. To prepare the vision inputs, this method forwards the `vision_infos` and `kwrags` arguments to\n",
            "        Qwen2VLImageProcessor's [`~Qwen2VLImageProcessor.__call__`] if `vision_infos` is not `None`.\n",
            "\n",
            "        Args:\n",
            "            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `List[PIL.Image.Image]`, `List[np.ndarray]`, `List[torch.Tensor]`):\n",
            "                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n",
            "                tensor. Both channels-first and channels-last formats are supported.\n",
            "            text (`str`, `List[str]`, `List[List[str]]`):\n",
            "                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n",
            "                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n",
            "                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n",
            "            videos (`np.ndarray`, `torch.Tensor`, `List[np.ndarray]`, `List[torch.Tensor]`):\n",
            "                The image or batch of videos to be prepared. Each video can be a 4D NumPy array or PyTorch\n",
            "                tensor, or a nested list of 3D frames. Both channels-first and channels-last formats are supported.\n",
            "            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n",
            "                If set, will return tensors of a particular framework. Acceptable values are:\n",
            "                - `'tf'`: Return TensorFlow `tf.constant` objects.\n",
            "                - `'pt'`: Return PyTorch `torch.Tensor` objects.\n",
            "                - `'np'`: Return NumPy `np.ndarray` objects.\n",
            "                - `'jax'`: Return JAX `jnp.ndarray` objects.\n",
            "\n",
            "        Returns:\n",
            "            [`BatchFeature`]: A [`BatchFeature`] with the following fields:\n",
            "\n",
            "            - **input_ids** -- List of token ids to be fed to a model. Returned when `text` is not `None`.\n",
            "            - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n",
            "              `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names` and if `text` is not\n",
            "              `None`).\n",
            "            - **pixel_values** -- Pixel values to be fed to a model. Returned when `images` is not `None`.\n",
            "            - **pixel_values_videos** -- Pixel values of videos to be fed to a model. Returned when `videos` is not `None`.\n",
            "            - **image_grid_thw** -- List of image 3D grid in LLM. Returned when `images` is not `None`.\n",
            "            - **video_grid_thw** -- List of video 3D grid in LLM. Returned when `videos` is not `None`.\n",
            "        \"\"\"\n",
            "        output_kwargs = self._merge_kwargs(\n",
            "            Qwen2VLProcessorKwargs,\n",
            "            tokenizer_init_kwargs=self.tokenizer.init_kwargs,\n",
            "            **kwargs,\n",
            "        )\n",
            "        if images is not None:\n",
            "            image_inputs = self.image_processor(images=images, videos=None, **output_kwargs[\"images_kwargs\"])\n",
            "            image_grid_thw = image_inputs[\"image_grid_thw\"]\n",
            "        else:\n",
            "            image_inputs = {}\n",
            "            image_grid_thw = None\n",
            "\n",
            "        if videos is not None:\n",
            "            videos_inputs = self.image_processor(images=None, videos=videos, **output_kwargs[\"videos_kwargs\"])\n",
            "            video_grid_thw = videos_inputs[\"video_grid_thw\"]\n",
            "        else:\n",
            "            videos_inputs = {}\n",
            "            video_grid_thw = None\n",
            "\n",
            "        if not isinstance(text, list):\n",
            "            text = [text]\n",
            "\n",
            "        if image_grid_thw is not None:\n",
            "            merge_length = self.image_processor.merge_size**2\n",
            "            index = 0\n",
            "            for i in range(len(text)):\n",
            "                while \"<|image_pad|>\" in text[i]:\n",
            "                    text[i] = text[i].replace(\n",
            "                        \"<|image_pad|>\", \"<|placeholder|>\" * (image_grid_thw[index].prod() // merge_length), 1\n",
            "                    )\n",
            "                    index += 1\n",
            "                text[i] = text[i].replace(\"<|placeholder|>\", \"<|image_pad|>\")\n",
            "\n",
            "        if video_grid_thw is not None:\n",
            "            merge_length = self.image_processor.merge_size**2\n",
            "            index = 0\n",
            "            for i in range(len(text)):\n",
            "                while \"<|video_pad|>\" in text[i]:\n",
            "                    text[i] = text[i].replace(\n",
            "                        \"<|video_pad|>\", \"<|placeholder|>\" * (video_grid_thw[index].prod() // merge_length), 1\n",
            "                    )\n",
            "                    index += 1\n",
            "                text[i] = text[i].replace(\"<|placeholder|>\", \"<|video_pad|>\")\n",
            "\n",
            "        text_inputs = self.tokenizer(text, **output_kwargs[\"text_kwargs\"])\n",
            "\n",
            "        return BatchFeature(data={**text_inputs, **image_inputs, **videos_inputs})\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Let's look at the __call__ method\n",
        "import inspect\n",
        "\n",
        "# Get the __call__ method source\n",
        "print(inspect.getsource(processor.__class__.__call__))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e4c9793-c9ca-4770-9e67-6a6386b80e2f",
      "metadata": {
        "id": "1e4c9793-c9ca-4770-9e67-6a6386b80e2f"
      },
      "outputs": [],
      "source": [
        "# def __call__(\n",
        "#         self,\n",
        "#         images: ImageInput = None,\n",
        "#         text: Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]] = None,\n",
        "#         videos: VideoInput = None,\n",
        "#         **kwargs: Unpack[Qwen2VLProcessorKwargs],\n",
        "#     ) -> BatchFeature:\n",
        "\n",
        "# ok let's focus on functino signature and explore\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30d44456-ae97-45f9-bab2-b6f6c4bee188",
      "metadata": {
        "id": "30d44456-ae97-45f9-bab2-b6f6c4bee188",
        "outputId": "fff41130-3563-469a-d04e-e6f0f1f0a0f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "typing.Union[ForwardRef('PIL.Image.Image'), numpy.ndarray, ForwardRef('torch.Tensor'), typing.List[ForwardRef('PIL.Image.Image')], typing.List[numpy.ndarray], typing.List[ForwardRef('torch.Tensor')]]\n"
          ]
        }
      ],
      "source": [
        "# Check the module for ImageInput\n",
        "from transformers.models.qwen2_vl.processing_qwen2_vl import ImageInput\n",
        "print(ImageInput)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8324acf5-87b2-49e4-b4c8-805aa6978950",
      "metadata": {
        "id": "8324acf5-87b2-49e4-b4c8-805aa6978950",
        "outputId": "df9bf131-2317-4919-ec9e-1e9c88dc39ea"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ImageInput = Union[\n",
        "#     PIL.Image.Image,           # Single PIL image\n",
        "#     numpy.ndarray,             # Single numpy array\n",
        "#     torch.Tensor,              # Single torch tensor\n",
        "#     List[PIL.Image.Image],     # List of PIL images\n",
        "#     List[numpy.ndarray],       # List of numpy arrays\n",
        "#     List[torch.Tensor]         # List of torch tensors\n",
        "# ]\n",
        "\n",
        "# ok ImageInput is not a real data type\n",
        "# it's just wrapper around Union\n",
        "from typeguard import check_type\n",
        "from typing import Union\n",
        "\n",
        "def process_number(value: Union[int, float, str]):\n",
        "    # Manually check the type\n",
        "    check_type(value, Union[int, float, str])\n",
        "    return value\n",
        "\n",
        "# This will work fine\n",
        "process_number(10)  # OK\n",
        "\n",
        "# This will raise an error\n",
        "#process_number([10])  # TypeError!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b2c64bb-6228-4e06-a686-12657228ee73",
      "metadata": {
        "id": "5b2c64bb-6228-4e06-a686-12657228ee73",
        "outputId": "c04c1917-45e4-4212-d979-1098fdcbff2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TextInput: <class 'str'>\n",
            "PreTokenizedInput: typing.List[str]\n"
          ]
        }
      ],
      "source": [
        "# Try to import and see what they are\n",
        "from transformers.models.qwen2_vl.processing_qwen2_vl import TextInput, PreTokenizedInput\n",
        "print(\"TextInput:\", TextInput)\n",
        "print(\"PreTokenizedInput:\", PreTokenizedInput)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e55dec87-3c12-4a8d-aa32-6d5f10960495",
      "metadata": {
        "id": "e55dec87-3c12-4a8d-aa32-6d5f10960495"
      },
      "outputs": [],
      "source": [
        "# ok so it's all wrappers for Unions to somehow enforce the data type\n",
        "# text: Union[\n",
        "#     str,            # Single string (TextInput)\n",
        "#     List[str],      # Single pre-tokenized sequence (PreTokenizedInput)\n",
        "#     List[str],      # List of strings (List[TextInput])\n",
        "#     List[List[str]] # List of pre-tokenized sequences (List[PreTokenizedInput])\n",
        "# ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c0004ca-3307-40c0-bc86-0ff44299cbf4",
      "metadata": {
        "id": "1c0004ca-3307-40c0-bc86-0ff44299cbf4",
        "outputId": "61ee717f-40de-4967-e94d-69aee3fa25cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Without flag: ['Hello', 'world']\n",
            "Result: {'input_ids': [[9707], [14615]], 'attention_mask': [[1], [1]]}\n",
            "Shape: [[9707], [14615]]\n",
            "\n",
            "==================================================\n",
            "\n",
            "With flag: ['Hello', 'world']\n",
            "Result: {'input_ids': [9707, 14615], 'attention_mask': [1, 1]}\n",
            "Shape: [9707, 14615]\n"
          ]
        }
      ],
      "source": [
        "# Without the flag - treats as MULTIPLE sequences\n",
        "text = [\"Hello\", \"world\"]\n",
        "result = processor.tokenizer(text)  # NO is_split_into_words flag\n",
        "print(f\"Without flag: {text}\")\n",
        "print(f\"Result: {result}\")\n",
        "print(f\"Shape: {result['input_ids']}\")  # Might be 2D - batch of 2 sequences!\n",
        "\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "# With the flag - treats as ONE sequence\n",
        "text = [\"Hello\", \"world\"]\n",
        "result = processor.tokenizer(text, is_split_into_words=True)\n",
        "print(f\"With flag: {text}\")\n",
        "print(f\"Result: {result}\")\n",
        "print(f\"Shape: {result['input_ids']}\")  # 1D - single sequence!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08779c20-fae2-4520-a194-c7fd8fac6c55",
      "metadata": {
        "id": "08779c20-fae2-4520-a194-c7fd8fac6c55"
      },
      "outputs": [],
      "source": [
        "# ok let me reiterate\n",
        "# if no flag it treats each element inside of the list as a separate sentences\n",
        "# if flag it assumes it's one single input ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8f7e786-a502-4ff6-9d84-5278f42ef896",
      "metadata": {
        "id": "d8f7e786-a502-4ff6-9d84-5278f42ef896",
        "outputId": "1ede14e2-87ed-4ced-b8f6-33d600f9ce84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "        Main method to tokenize and prepare for the model one or several sequence(s) or one or several pair(s) of\n",
            "        sequences.\n",
            "\n",
            "        Args:\n",
            "            text (`str`, `List[str]`, `List[List[str]]`, *optional*):\n",
            "                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n",
            "                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n",
            "                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n",
            "            text_pair (`str`, `List[str]`, `List[List[str]]`, *optional*):\n",
            "                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n",
            "                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n",
            "                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n",
            "            text_target (`str`, `List[str]`, `List[List[str]]`, *optional*):\n",
            "                The sequence or batch of sequences to be encoded as target texts. Each sequence can be a string or a\n",
            "                list of strings (pretokenized string). If the sequences are provided as list of strings (pretokenized),\n",
            "                you must set `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n",
            "            text_pair_target (`str`, `List[str]`, `List[List[str]]`, *optional*):\n",
            "                The sequence or batch of sequences to be encoded as target texts. Each sequence can be a string or a\n",
            "                list of strings (pretokenized string). If the sequences are provided as list of strings (pretokenized),\n",
            "                you must set `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n",
            "        \n",
            "            add_special_tokens (`bool`, *optional*, defaults to `True`):\n",
            "                Whether or not to add special tokens when encoding the sequences. This will use the underlying\n",
            "                `PretrainedTokenizerBase.build_inputs_with_special_tokens` function, which defines which tokens are\n",
            "                automatically added to the input ids. This is usefull if you want to add `bos` or `eos` tokens\n",
            "                automatically.\n",
            "            padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `False`):\n",
            "                Activates and controls padding. Accepts the following values:\n",
            "\n",
            "                - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
            "                  sequence if provided).\n",
            "                - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n",
            "                  acceptable input length for the model if that argument is not provided.\n",
            "                - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different\n",
            "                  lengths).\n",
            "            truncation (`bool`, `str` or [`~tokenization_utils_base.TruncationStrategy`], *optional*, defaults to `False`):\n",
            "                Activates and controls truncation. Accepts the following values:\n",
            "\n",
            "                - `True` or `'longest_first'`: Truncate to a maximum length specified with the argument `max_length` or\n",
            "                  to the maximum acceptable input length for the model if that argument is not provided. This will\n",
            "                  truncate token by token, removing a token from the longest sequence in the pair if a pair of\n",
            "                  sequences (or a batch of pairs) is provided.\n",
            "                - `'only_first'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
            "                  maximum acceptable input length for the model if that argument is not provided. This will only\n",
            "                  truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
            "                - `'only_second'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
            "                  maximum acceptable input length for the model if that argument is not provided. This will only\n",
            "                  truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
            "                - `False` or `'do_not_truncate'` (default): No truncation (i.e., can output batch with sequence lengths\n",
            "                  greater than the model maximum admissible input size).\n",
            "            max_length (`int`, *optional*):\n",
            "                Controls the maximum length to use by one of the truncation/padding parameters.\n",
            "\n",
            "                If left unset or set to `None`, this will use the predefined model maximum length if a maximum length\n",
            "                is required by one of the truncation/padding parameters. If the model has no specific maximum input\n",
            "                length (like XLNet) truncation/padding to a maximum length will be deactivated.\n",
            "            stride (`int`, *optional*, defaults to 0):\n",
            "                If set to a number along with `max_length`, the overflowing tokens returned when\n",
            "                `return_overflowing_tokens=True` will contain some tokens from the end of the truncated sequence\n",
            "                returned to provide some overlap between truncated and overflowing sequences. The value of this\n",
            "                argument defines the number of overlapping tokens.\n",
            "            is_split_into_words (`bool`, *optional*, defaults to `False`):\n",
            "                Whether or not the input is already pre-tokenized (e.g., split into words). If set to `True`, the\n",
            "                tokenizer assumes the input is already split into words (for instance, by splitting it on whitespace)\n",
            "                which it will tokenize. This is useful for NER or token classification.\n",
            "            pad_to_multiple_of (`int`, *optional*):\n",
            "                If set will pad the sequence to a multiple of the provided value. Requires `padding` to be activated.\n",
            "                This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability\n",
            "                `>= 7.5` (Volta).\n",
            "            padding_side (`str`, *optional*):\n",
            "                The side on which the model should have padding applied. Should be selected between ['right', 'left'].\n",
            "                Default value is picked from the class attribute of the same name.\n",
            "            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n",
            "                If set, will return tensors instead of list of python integers. Acceptable values are:\n",
            "\n",
            "                - `'tf'`: Return TensorFlow `tf.constant` objects.\n",
            "                - `'pt'`: Return PyTorch `torch.Tensor` objects.\n",
            "                - `'np'`: Return Numpy `np.ndarray` objects.\n",
            "\n",
            "            return_token_type_ids (`bool`, *optional*):\n",
            "                Whether to return token type IDs. If left to the default, will return the token type IDs according to\n",
            "                the specific tokenizer's default, defined by the `return_outputs` attribute.\n",
            "\n",
            "                [What are token type IDs?](../glossary#token-type-ids)\n",
            "            return_attention_mask (`bool`, *optional*):\n",
            "                Whether to return the attention mask. If left to the default, will return the attention mask according\n",
            "                to the specific tokenizer's default, defined by the `return_outputs` attribute.\n",
            "\n",
            "                [What are attention masks?](../glossary#attention-mask)\n",
            "            return_overflowing_tokens (`bool`, *optional*, defaults to `False`):\n",
            "                Whether or not to return overflowing token sequences. If a pair of sequences of input ids (or a batch\n",
            "                of pairs) is provided with `truncation_strategy = longest_first` or `True`, an error is raised instead\n",
            "                of returning overflowing tokens.\n",
            "            return_special_tokens_mask (`bool`, *optional*, defaults to `False`):\n",
            "                Whether or not to return special tokens mask information.\n",
            "            return_offsets_mapping (`bool`, *optional*, defaults to `False`):\n",
            "                Whether or not to return `(char_start, char_end)` for each token.\n",
            "\n",
            "                This is only available on fast tokenizers inheriting from [`PreTrainedTokenizerFast`], if using\n",
            "                Python's tokenizer, this method will raise `NotImplementedError`.\n",
            "            return_length  (`bool`, *optional*, defaults to `False`):\n",
            "                Whether or not to return the lengths of the encoded inputs.\n",
            "            verbose (`bool`, *optional*, defaults to `True`):\n",
            "                Whether or not to print more information and warnings.\n",
            "            **kwargs: passed to the `self.tokenize()` method\n",
            "\n",
            "        Return:\n",
            "            [`BatchEncoding`]: A [`BatchEncoding`] with the following fields:\n",
            "\n",
            "            - **input_ids** -- List of token ids to be fed to a model.\n",
            "\n",
            "              [What are input IDs?](../glossary#input-ids)\n",
            "\n",
            "            - **token_type_ids** -- List of token type ids to be fed to a model (when `return_token_type_ids=True` or\n",
            "              if *\"token_type_ids\"* is in `self.model_input_names`).\n",
            "\n",
            "              [What are token type IDs?](../glossary#token-type-ids)\n",
            "\n",
            "            - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n",
            "              `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names`).\n",
            "\n",
            "              [What are attention masks?](../glossary#attention-mask)\n",
            "\n",
            "            - **overflowing_tokens** -- List of overflowing tokens sequences (when a `max_length` is specified and\n",
            "              `return_overflowing_tokens=True`).\n",
            "            - **num_truncated_tokens** -- Number of tokens truncated (when a `max_length` is specified and\n",
            "              `return_overflowing_tokens=True`).\n",
            "            - **special_tokens_mask** -- List of 0s and 1s, with 1 specifying added special tokens and 0 specifying\n",
            "              regular sequence tokens (when `add_special_tokens=True` and `return_special_tokens_mask=True`).\n",
            "            - **length** -- The length of the inputs (when `return_length=True`)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# See if there's a parameter that controls this behavior\n",
        "print(processor.tokenizer.__call__.__doc__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f73454d6-16f1-4153-898b-2b0502cc257b",
      "metadata": {
        "id": "f73454d6-16f1-4153-898b-2b0502cc257b",
        "outputId": "790e455d-6741-4dda-9579-03a5badced01"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "typing.Union[typing.List[ForwardRef('PIL.Image.Image')], ForwardRef('np.ndarray'), ForwardRef('torch.Tensor'), typing.List[ForwardRef('np.ndarray')], typing.List[ForwardRef('torch.Tensor')], typing.List[typing.List[ForwardRef('PIL.Image.Image')]], typing.List[typing.List[ForwardRef('np.ndarrray')]], typing.List[typing.List[ForwardRef('torch.Tensor')]]]\n"
          ]
        }
      ],
      "source": [
        "# Try to import VideoInput\n",
        "from transformers.models.qwen2_vl.processing_qwen2_vl import VideoInput\n",
        "print(VideoInput)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "600991c2-3dd5-48d3-8dda-ac8fe4603d80",
      "metadata": {
        "id": "600991c2-3dd5-48d3-8dda-ac8fe4603d80"
      },
      "outputs": [],
      "source": [
        "# VideoInput = Union[\n",
        "#     # Single video formats:\n",
        "#     List[PIL.Image.Image],      # Video as list of PIL images (frames)\n",
        "#     np.ndarray,                 # Video as 4D numpy array [frames, H, W, C]\n",
        "#     torch.Tensor,               # Video as 4D torch tensor\n",
        "\n",
        "#     # Batch of videos:\n",
        "#     List[np.ndarray],           # List of video arrays\n",
        "#     List[torch.Tensor],         # List of video tensors\n",
        "#     List[List[PIL.Image.Image]], # List of videos (each is list of frames)\n",
        "#     List[List[np.ndarray]],     # List of videos (each is list of frame arrays)\n",
        "#     List[List[torch.Tensor]]    # List of videos (each is list of frame tensors)\n",
        "# ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02248723-b9e1-4800-b67f-27fe1258fa64",
      "metadata": {
        "id": "02248723-b9e1-4800-b67f-27fe1258fa64"
      },
      "outputs": [],
      "source": [
        "# repport a bug really ?\n",
        "# typing.List[typing.List[ForwardRef('np.ndarrray')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14f99027-bb83-405e-a47c-35912a17cfbf",
      "metadata": {
        "id": "14f99027-bb83-405e-a47c-35912a17cfbf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b5cee71-df75-4ee0-b6ce-96d80ea4e45c",
      "metadata": {
        "id": "0b5cee71-df75-4ee0-b6ce-96d80ea4e45c",
        "outputId": "13d5923e-7eeb-4cc2-ee7c-738d4a51fc75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FOUND TYPO in image_utils.py!\n",
            "Line 90: List[List[\"np.ndarrray\"]],\n",
            "Line 81: \n",
            "Line 82: \n",
            "Line 83: VideoInput = Union[\n",
            "Line 84:     List[\"PIL.Image.Image\"],\n",
            "Line 85:     \"np.ndarray\",\n"
          ]
        }
      ],
      "source": [
        "# Check the transformers.image_utils file\n",
        "with open('/usr/local/lib/python3.12/dist-packages/transformers/image_utils.py', 'r') as f:\n",
        "    content = f.read()\n",
        "\n",
        "# Search for VideoInput definition\n",
        "if 'ndarrray' in content:\n",
        "    print(\"FOUND TYPO in image_utils.py!\")\n",
        "    lines = content.split('\\n')\n",
        "    for i, line in enumerate(lines):\n",
        "        if 'ndarrray' in line:\n",
        "            print(f\"Line {i+1}: {line.strip()}\")\n",
        "\n",
        "# Also look for VideoInput definition\n",
        "lines = content.split('\\n')\n",
        "for i, line in enumerate(lines):\n",
        "    if 'VideoInput' in line and ('=' in line or ':' in line):\n",
        "        # Print context around VideoInput definition\n",
        "        start = max(0, i-2)\n",
        "        end = min(len(lines), i+3)\n",
        "        for j in range(start, end):\n",
        "            print(f\"Line {j+1}: {lines[j]}\")\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44430e90-e6b7-4e6f-bfee-7b5144b695b3",
      "metadata": {
        "id": "44430e90-e6b7-4e6f-bfee-7b5144b695b3",
        "outputId": "3b46ae0f-8a0f-4288-90dc-a5bcaa85390f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Line 83: VideoInput = Union[\n",
            "Line 84:     List[\"PIL.Image.Image\"],\n",
            "Line 85:     \"np.ndarray\",\n",
            "Line 86:     \"torch.Tensor\",\n",
            "Line 87:     List[\"np.ndarray\"],\n",
            "Line 88:     List[\"torch.Tensor\"],\n",
            "Line 89:     List[List[\"PIL.Image.Image\"]],\n",
            "Line 90:     List[List[\"np.ndarrray\"]],\n",
            "Line 91:     List[List[\"torch.Tensor\"]],\n",
            "Line 92: ]  # noqa\n",
            "Line 93: \n",
            "Line 94: \n",
            "Line 95: class ChannelDimension(ExplicitEnum):\n"
          ]
        }
      ],
      "source": [
        "# Get the full VideoInput definition\n",
        "with open('/usr/local/lib/python3.12/dist-packages/transformers/image_utils.py', 'r') as f:\n",
        "    lines = f.readlines()\n",
        "    # Start from line 83 and get the full definition\n",
        "    for i in range(82, min(95, len(lines))):  # Lines 83-95\n",
        "        print(f\"Line {i+1}: {lines[i].rstrip()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "930637ee-f601-4834-9d12-035ecd6e0d43",
      "metadata": {
        "id": "930637ee-f601-4834-9d12-035ecd6e0d43"
      },
      "outputs": [],
      "source": [
        "# https://github.com/huggingface/transformers/blob/66d57110f089789ae285cc9d54d3bf051123246b/src/transformers/video_utils.py#L64\n",
        "# bug is fixed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c2d0277-32d5-41c6-b752-f0f2dd837c49",
      "metadata": {
        "id": "9c2d0277-32d5-41c6-b752-f0f2dd837c49"
      },
      "outputs": [],
      "source": [
        "# https://github.com/huggingface/transformers/commit/ce091b1bda847bc3ba426cd5430a3d71e267cdae\n",
        "# search np.ndarrray could not find who fixed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83094d14-de8f-40d9-898a-b07402854134",
      "metadata": {
        "id": "83094d14-de8f-40d9-898a-b07402854134",
        "outputId": "c41bf19b-3fca-44a2-8a29-16d949ed62d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "class Qwen2VLProcessorKwargs(ProcessingKwargs, total=False):\n",
            "    _defaults = {\n",
            "        \"text_kwargs\": {\n",
            "            \"padding\": False,\n",
            "        },\n",
            "    }\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Find the definition\n",
        "from transformers.models.qwen2_vl.processing_qwen2_vl import Qwen2VLProcessorKwargs\n",
        "source = inspect.getsource(Qwen2VLProcessorKwargs)\n",
        "print(source)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fbf8c775-5a97-4fcc-8a7f-e73ff8670f1c",
      "metadata": {
        "id": "fbf8c775-5a97-4fcc-8a7f-e73ff8670f1c",
        "outputId": "647f30cd-87b5-439f-ff3b-28e596320ac6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "class ProcessingKwargs(TextKwargs, ImagesKwargs, VideosKwargs, AudioKwargs, CommonKwargs, total=False):\n",
            "    \"\"\"\n",
            "    Base class for kwargs passing to processors.\n",
            "    A model should have its own `ModelProcessorKwargs` class that inherits from `ProcessingKwargs` to provide:\n",
            "        1) Additional typed keys and that this model requires to process inputs.\n",
            "        2) Default values for existing keys under a `_defaults` attribute.\n",
            "    New keys have to be defined as follows to ensure type hinting is done correctly.\n",
            "\n",
            "    ```python\n",
            "    # adding a new image kwarg for this model\n",
            "    class ModelImagesKwargs(ImagesKwargs, total=False):\n",
            "        new_image_kwarg: Optional[bool]\n",
            "\n",
            "    class ModelProcessorKwargs(ProcessingKwargs, total=False):\n",
            "        images_kwargs: ModelImagesKwargs\n",
            "        _defaults = {\n",
            "            \"images_kwargs: {\n",
            "                \"new_image_kwarg\": False,\n",
            "            }\n",
            "            \"text_kwargs\": {\n",
            "                \"padding\": \"max_length\",\n",
            "            },\n",
            "        }\n",
            "\n",
            "    ```\n",
            "\n",
            "    For Python 3.8 compatibility, when inheriting from this class and overriding one of the kwargs,\n",
            "    you need to manually update the __annotations__ dictionary. This can be done as follows:\n",
            "\n",
            "    ```python\n",
            "    class CustomProcessorKwargs(ProcessingKwargs, total=False):\n",
            "        images_kwargs: CustomImagesKwargs\n",
            "\n",
            "    CustomProcessorKwargs.__annotations__[\"images_kwargs\"] = CustomImagesKwargs  # python 3.8 compatibility\n",
            "    ```python\n",
            "\n",
            "    \"\"\"\n",
            "\n",
            "    common_kwargs: CommonKwargs = {\n",
            "        **CommonKwargs.__annotations__,\n",
            "    }\n",
            "    text_kwargs: TextKwargs = {\n",
            "        **TextKwargs.__annotations__,\n",
            "    }\n",
            "    images_kwargs: ImagesKwargs = {\n",
            "        **ImagesKwargs.__annotations__,\n",
            "    }\n",
            "    videos_kwargs: VideosKwargs = {\n",
            "        **VideosKwargs.__annotations__,\n",
            "    }\n",
            "    audio_kwargs: AudioKwargs = {\n",
            "        **AudioKwargs.__annotations__,\n",
            "    }\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from transformers.processing_utils import ProcessingKwargs\n",
        "\n",
        "# Get the source\n",
        "import inspect\n",
        "source = inspect.getsource(ProcessingKwargs)\n",
        "print(source)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4c4a838-8b8d-418b-957b-b15ad338af3c",
      "metadata": {
        "id": "a4c4a838-8b8d-418b-957b-b15ad338af3c",
        "outputId": "67ae32fc-b966-463d-9b1d-26bf175c4580"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "TextKwargs:\n",
            "class TextKwargs(TypedDict, total=False):\n",
            "    \"\"\"\n",
            "    Keyword arguments for text processing. For extended documentation, check out tokenization_utils_base methods and\n",
            "    docstrings associated.\n",
            "\n",
            "    Attributes:\n",
            "        add_special_tokens (`bool`, *optional*)\n",
            "            Whether or not to add special tokens when encoding the sequences.\n",
            "        padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*)\n",
            "            Activates and controls padding.\n",
            "        truncation (`bool`, `str` or [`~tokenization_utils_base.TruncationStrategy`], *optional*):\n",
            "            Activates and controls truncation.\n",
            "        max_length (`int`, *optional*):\n",
            "            Controls the maximum length to use by one of the truncation/padding parameters.\n",
            "        stride (`int`, *optional*):\n",
            "            If set, the overflowing tokens will contain some tokens from the end of the truncated sequence.\n",
            "        is_split_into_words (`bool`, *optional*):\n",
            "            Whether or not the input is already pre-tokenized.\n",
            "        pad_to_multiple_of (`int`, *optional*):\n",
            "            If set, will pad the sequence to a multiple of the provided value.\n",
            "        return_token_type_ids (`bool`, *optional*):\n",
            "            Whether to return token type IDs.\n",
            "        return_attention_mask (`bool`, *optional*):\n",
            "            Whether to return the attention mask.\n",
            "        return_overflowing_tokens (`bool`, *optional*):\n",
            "            Whether or not to return overflowing token sequences.\n",
            "        return_special_tokens_mask (`bool`, *optional*):\n",
            "            Whether or not to return special tokens mask information.\n",
            "        return_offsets_mapping (`bool`, *optional*):\n",
            "            Whether or not to return `(char_start, char_end)` for each token.\n",
            "        return_length (`bool`, *optional*):\n",
            "            Whether or not to return the lengths of the encoded inputs.\n",
            "        verbose (`bool`, *optional*):\n",
            "            Whether or not to print more information and warnings.\n",
            "        padding_side (`str`, *optional*):\n",
            "            The side on which padding will be applied.\n",
            "    \"\"\"\n",
            "\n",
            "    text_pair: Optional[Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]]]\n",
            "    text_target: Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]]\n",
            "    text_pair_target: Optional[Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]]]\n",
            "    add_special_tokens: Optional[bool]\n",
            "    padding: Union[bool, str, PaddingStrategy]\n",
            "    truncation: Union[bool, str, TruncationStrategy]\n",
            "    max_length: Optional[int]\n",
            "    stride: Optional[int]\n",
            "    is_split_into_words: Optional[bool]\n",
            "    pad_to_multiple_of: Optional[int]\n",
            "    return_token_type_ids: Optional[bool]\n",
            "    return_attention_mask: Optional[bool]\n",
            "    return_overflowing_tokens: Optional[bool]\n",
            "    return_special_tokens_mask: Optional[bool]\n",
            "    return_offsets_mapping: Optional[bool]\n",
            "    return_length: Optional[bool]\n",
            "    verbose: Optional[bool]\n",
            "    padding_side: Optional[str]\n",
            "\n",
            "\n",
            "ImagesKwargs:\n",
            "class ImagesKwargs(TypedDict, total=False):\n",
            "    \"\"\"\n",
            "    Keyword arguments for image processing. For extended documentation, check the appropriate ImageProcessor\n",
            "    class methods and docstrings.\n",
            "\n",
            "    Attributes:\n",
            "        do_resize (`bool`, *optional*):\n",
            "            Whether to resize the image.\n",
            "        size (`Dict[str, int]`, *optional*):\n",
            "            Resize the shorter side of the input to `size[\"shortest_edge\"]`.\n",
            "        size_divisor (`int`, *optional*):\n",
            "            The size by which to make sure both the height and width can be divided.\n",
            "        crop_size (`Dict[str, int]`, *optional*):\n",
            "            Desired output size when applying center-cropping.\n",
            "        resample (`PILImageResampling`, *optional*):\n",
            "            Resampling filter to use if resizing the image.\n",
            "        do_rescale (`bool`, *optional*):\n",
            "            Whether to rescale the image by the specified scale `rescale_factor`.\n",
            "        rescale_factor (`int` or `float`, *optional*):\n",
            "            Scale factor to use if rescaling the image.\n",
            "        do_normalize (`bool`, *optional*):\n",
            "            Whether to normalize the image.\n",
            "        image_mean (`float` or `List[float]`, *optional*):\n",
            "            Mean to use if normalizing the image.\n",
            "        image_std (`float` or `List[float]`, *optional*):\n",
            "            Standard deviation to use if normalizing the image.\n",
            "        do_pad (`bool`, *optional*):\n",
            "            Whether to pad the image to the `(max_height, max_width)` of the images in the batch.\n",
            "        pad_size (`Dict[str, int]`, *optional*):\n",
            "            The size `{\"height\": int, \"width\" int}` to pad the images to.\n",
            "        do_center_crop (`bool`, *optional*):\n",
            "            Whether to center crop the image.\n",
            "        data_format (`ChannelDimension` or `str`, *optional*):\n",
            "            The channel dimension format for the output image.\n",
            "        input_data_format (`ChannelDimension` or `str`, *optional*):\n",
            "            The channel dimension format for the input image.\n",
            "    \"\"\"\n",
            "\n",
            "    do_resize: Optional[bool]\n",
            "    size: Optional[Dict[str, int]]\n",
            "    size_divisor: Optional[int]\n",
            "    crop_size: Optional[Dict[str, int]]\n",
            "    resample: Optional[Union[\"PILImageResampling\", int]]\n",
            "    do_rescale: Optional[bool]\n",
            "    rescale_factor: Optional[float]\n",
            "    do_normalize: Optional[bool]\n",
            "    image_mean: Optional[Union[float, List[float]]]\n",
            "    image_std: Optional[Union[float, List[float]]]\n",
            "    do_pad: Optional[bool]\n",
            "    pad_size: Optional[Dict[str, int]]\n",
            "    do_center_crop: Optional[bool]\n",
            "    data_format: Optional[ChannelDimension]\n",
            "    input_data_format: Optional[Union[str, ChannelDimension]]\n",
            "\n",
            "\n",
            "VideosKwargs:\n",
            "class VideosKwargs(TypedDict, total=False):\n",
            "    \"\"\"\n",
            "    Keyword arguments for video processing.\n",
            "\n",
            "    Attributes:\n",
            "        do_resize (`bool`):\n",
            "            Whether to resize the image.\n",
            "        size (`Dict[str, int]`, *optional*):\n",
            "            Resize the shorter side of the input to `size[\"shortest_edge\"]`.\n",
            "        size_divisor (`int`, *optional*):\n",
            "            The size by which to make sure both the height and width can be divided.\n",
            "        resample (`PILImageResampling`, *optional*):\n",
            "            Resampling filter to use if resizing the image.\n",
            "        do_rescale (`bool`, *optional*):\n",
            "            Whether to rescale the image by the specified scale `rescale_factor`.\n",
            "        rescale_factor (`int` or `float`, *optional*):\n",
            "            Scale factor to use if rescaling the image.\n",
            "        do_normalize (`bool`, *optional*):\n",
            "            Whether to normalize the image.\n",
            "        image_mean (`float` or `List[float]`, *optional*):\n",
            "            Mean to use if normalizing the image.\n",
            "        image_std (`float` or `List[float]`, *optional*):\n",
            "            Standard deviation to use if normalizing the image.\n",
            "        do_pad (`bool`, *optional*):\n",
            "            Whether to pad the image to the `(max_height, max_width)` of the images in the batch.\n",
            "        do_center_crop (`bool`, *optional*):\n",
            "            Whether to center crop the image.\n",
            "        data_format (`ChannelDimension` or `str`, *optional*):\n",
            "            The channel dimension format for the output image.\n",
            "        input_data_format (`ChannelDimension` or `str`, *optional*):\n",
            "            The channel dimension format for the input image.\n",
            "    \"\"\"\n",
            "\n",
            "    do_resize: Optional[bool]\n",
            "    size: Optional[Dict[str, int]]\n",
            "    size_divisor: Optional[int]\n",
            "    resample: Optional[\"PILImageResampling\"]\n",
            "    do_rescale: Optional[bool]\n",
            "    rescale_factor: Optional[float]\n",
            "    do_normalize: Optional[bool]\n",
            "    image_mean: Optional[Union[float, List[float]]]\n",
            "    image_std: Optional[Union[float, List[float]]]\n",
            "    do_pad: Optional[bool]\n",
            "    do_center_crop: Optional[bool]\n",
            "    data_format: Optional[ChannelDimension]\n",
            "    input_data_format: Optional[Union[str, ChannelDimension]]\n",
            "\n",
            "\n",
            "AudioKwargs:\n",
            "class AudioKwargs(TypedDict, total=False):\n",
            "    \"\"\"\n",
            "    Keyword arguments for audio processing.\n",
            "\n",
            "    Attributes:\n",
            "        sampling_rate (`int`, *optional*):\n",
            "            The sampling rate at which the `raw_speech` input was sampled.\n",
            "        raw_speech (`np.ndarray`, `List[float]`, `List[np.ndarray]`, `List[List[float]]`):\n",
            "            The sequence or batch of sequences to be padded. Each sequence can be a numpy array, a list of float\n",
            "            values, a list of numpy arrays or a list of list of float values. Must be mono channel audio, not\n",
            "            stereo, i.e. single float per timestep.\n",
            "        padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*):\n",
            "            Select a strategy to pad the returned sequences (according to the model's padding side and padding\n",
            "            index) among:\n",
            "\n",
            "            - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
            "                sequence if provided).\n",
            "            - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n",
            "                acceptable input length for the model if that argument is not provided.\n",
            "            - `False` or `'do_not_pad'`\n",
            "        max_length (`int`, *optional*):\n",
            "            Maximum length of the returned list and optionally padding length (see above).\n",
            "        truncation (`bool`, *optional*):\n",
            "            Activates truncation to cut input sequences longer than *max_length* to *max_length*.\n",
            "        pad_to_multiple_of (`int`, *optional*):\n",
            "            If set, will pad the sequence to a multiple of the provided value.\n",
            "        return_attention_mask (`bool`, *optional*):\n",
            "            Whether or not [`~ASTFeatureExtractor.__call__`] should return `attention_mask`.\n",
            "    \"\"\"\n",
            "\n",
            "    sampling_rate: Optional[int]\n",
            "    raw_speech: Optional[Union[\"np.ndarray\", List[float], List[\"np.ndarray\"], List[List[float]]]]\n",
            "    padding: Optional[Union[bool, str, PaddingStrategy]]\n",
            "    max_length: Optional[int]\n",
            "    truncation: Optional[bool]\n",
            "    pad_to_multiple_of: Optional[int]\n",
            "    return_attention_mask: Optional[bool]\n",
            "\n",
            "\n",
            "CommonKwargs:\n",
            "class CommonKwargs(TypedDict, total=False):\n",
            "    return_tensors: Optional[Union[str, TensorType]]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from transformers.processing_utils import TextKwargs, ImagesKwargs, VideosKwargs, AudioKwargs, CommonKwargs\n",
        "\n",
        "# Get all the kwargs\n",
        "for kwarg_class in [TextKwargs, ImagesKwargs, VideosKwargs, AudioKwargs, CommonKwargs]:\n",
        "    print(f\"\\n{kwarg_class.__name__}:\")\n",
        "    print(inspect.getsource(kwarg_class))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bc59d88-f75f-4e8b-98fb-5ba8a7b829e4",
      "metadata": {
        "id": "6bc59d88-f75f-4e8b-98fb-5ba8a7b829e4"
      },
      "outputs": [],
      "source": [
        "from typing import TypedDict, Optional\n",
        "\n",
        "# Regular dict - no type info\n",
        "regular_dict = {\"name\": \"John\", \"age\": 30}\n",
        "\n",
        "# TypedDict - defines structure\n",
        "class Person(TypedDict):\n",
        "    name: str\n",
        "    age: int\n",
        "    email: Optional[str]\n",
        "\n",
        "# Now you have typed dictionary\n",
        "person: Person = {\"name\": \"John\", \"age\": 30, \"email\": None}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61cf72eb-f51a-4ba4-b428-5f8d670eea88",
      "metadata": {
        "id": "61cf72eb-f51a-4ba4-b428-5f8d670eea88",
        "outputId": "e1455dd6-b76f-481e-b2e1-b196778d92a2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'name': 'John', 'age': 30, 'email': None}"
            ]
          },
          "execution_count": 77,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "person"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7b972bd-981f-4b05-9e12-ea40461a4dcd",
      "metadata": {
        "id": "f7b972bd-981f-4b05-9e12-ea40461a4dcd"
      },
      "outputs": [],
      "source": [
        "from typing import TypedDict, Unpack\n",
        "\n",
        "class MyKwargs(TypedDict):\n",
        "    padding: bool\n",
        "    max_length: int\n",
        "\n",
        "def process(**kwargs: Unpack[MyKwargs]):\n",
        "    pass\n",
        "\n",
        "# Type checker sees: kwargs can have 'padding' (bool) and 'max_length' (int)\n",
        "# IDE autocomplete: Will suggest 'padding' and 'max_length'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd3dee2c-83bf-4f2d-834e-d9b93e75f2cc",
      "metadata": {
        "id": "cd3dee2c-83bf-4f2d-834e-d9b93e75f2cc"
      },
      "outputs": [],
      "source": [
        "class TextKwargs(TypedDict):\n",
        "    padding: bool\n",
        "    max_length: int\n",
        "\n",
        "class ImagesKwargs(TypedDict):\n",
        "    do_resize: bool\n",
        "    size: dict\n",
        "\n",
        "# This inheritance DOESN'T create or merge dicts!\n",
        "class ProcessingKwargs(TextKwargs, ImagesKwargs):\n",
        "    pass\n",
        "\n",
        "# ProcessingKwargs just means: \"type checkers, this TypedDict has all fields from both\"\n",
        "# It's like saying: ProcessingKwargs can have: padding, max_length, do_resize, size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90b4131a-9892-474f-85c2-6c7a858d64d2",
      "metadata": {
        "id": "90b4131a-9892-474f-85c2-6c7a858d64d2",
        "outputId": "24b4feca-441b-4958-c61b-f521eef8ed24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'padding': True, 'do_resize': False, 'max_length': 100}\n"
          ]
        }
      ],
      "source": [
        "def process(**kwargs: Unpack[ProcessingKwargs]):\n",
        "    print(kwargs)  # Just ONE regular dict!\n",
        "\n",
        "# When you call:\n",
        "process(padding=True, do_resize=False, max_length=100)\n",
        "\n",
        "# Python creates ONE dict:\n",
        "# kwargs = {'padding': True, 'do_resize': False, 'max_length': 100}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b707f269-9492-43cf-900e-384ad36f0664",
      "metadata": {
        "id": "b707f269-9492-43cf-900e-384ad36f0664",
        "outputId": "bcfb4b86-1eaa-4d0f-fa13-aa6706e8ffd2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "class BatchFeature(UserDict):\n",
            "    r\"\"\"\n",
            "    Holds the output of the [`~SequenceFeatureExtractor.pad`] and feature extractor specific `__call__` methods.\n",
            "\n",
            "    This class is derived from a python dictionary and can be used as a dictionary.\n",
            "\n",
            "    Args:\n",
            "        data (`dict`, *optional*):\n",
            "            Dictionary of lists/arrays/tensors returned by the __call__/pad methods ('input_values', 'attention_mask',\n",
            "            etc.).\n",
            "        tensor_type (`Union[None, str, TensorType]`, *optional*):\n",
            "            You can give a tensor_type here to convert the lists of integers in PyTorch/TensorFlow/Numpy Tensors at\n",
            "            initialization.\n",
            "    \"\"\"\n",
            "\n",
            "    def __init__(self, data: Optional[Dict[str, Any]] = None, tensor_type: Union[None, str, TensorType] = None):\n",
            "        super().__init__(data)\n",
            "        self.convert_to_tensors(tensor_type=tensor_type)\n",
            "\n",
            "    def __getitem__(self, item: str) -> Union[Any]:\n",
            "        \"\"\"\n",
            "        If the key is a string, returns the value of the dict associated to `key` ('input_values', 'attention_mask',\n",
            "        etc.).\n",
            "        \"\"\"\n",
            "        if isinstance(item, str):\n",
            "            return self.data[item]\n",
            "        else:\n",
            "            raise KeyError(\"Indexing with integers is not available when using Python based feature extractors\")\n",
            "\n",
            "    def __getattr__(self, item: str):\n",
            "        try:\n",
            "            return self.data[item]\n",
            "        except KeyError:\n",
            "            raise AttributeError\n",
            "\n",
            "    def __getstate__(self):\n",
            "        return {\"data\": self.data}\n",
            "\n",
            "    def __setstate__(self, state):\n",
            "        if \"data\" in state:\n",
            "            self.data = state[\"data\"]\n",
            "\n",
            "    # Copied from transformers.tokenization_utils_base.BatchEncoding.keys\n",
            "    def keys(self):\n",
            "        return self.data.keys()\n",
            "\n",
            "    # Copied from transformers.tokenization_utils_base.BatchEncoding.values\n",
            "    def values(self):\n",
            "        return self.data.values()\n",
            "\n",
            "    # Copied from transformers.tokenization_utils_base.BatchEncoding.items\n",
            "    def items(self):\n",
            "        return self.data.items()\n",
            "\n",
            "    def _get_is_as_tensor_fns(self, tensor_type: Optional[Union[str, TensorType]] = None):\n",
            "        if tensor_type is None:\n",
            "            return None, None\n",
            "\n",
            "        # Convert to TensorType\n",
            "        if not isinstance(tensor_type, TensorType):\n",
            "            tensor_type = TensorType(tensor_type)\n",
            "\n",
            "        # Get a function reference for the correct framework\n",
            "        if tensor_type == TensorType.TENSORFLOW:\n",
            "            if not is_tf_available():\n",
            "                raise ImportError(\n",
            "                    \"Unable to convert output to TensorFlow tensors format, TensorFlow is not installed.\"\n",
            "                )\n",
            "            import tensorflow as tf\n",
            "\n",
            "            as_tensor = tf.constant\n",
            "            is_tensor = tf.is_tensor\n",
            "        elif tensor_type == TensorType.PYTORCH:\n",
            "            if not is_torch_available():\n",
            "                raise ImportError(\"Unable to convert output to PyTorch tensors format, PyTorch is not installed.\")\n",
            "            import torch  # noqa\n",
            "\n",
            "            def as_tensor(value):\n",
            "                if isinstance(value, (list, tuple)) and len(value) > 0:\n",
            "                    if isinstance(value[0], np.ndarray):\n",
            "                        value = np.array(value)\n",
            "                    elif (\n",
            "                        isinstance(value[0], (list, tuple))\n",
            "                        and len(value[0]) > 0\n",
            "                        and isinstance(value[0][0], np.ndarray)\n",
            "                    ):\n",
            "                        value = np.array(value)\n",
            "                if isinstance(value, np.ndarray):\n",
            "                    return torch.from_numpy(value)\n",
            "                else:\n",
            "                    return torch.tensor(value)\n",
            "\n",
            "            is_tensor = torch.is_tensor\n",
            "        elif tensor_type == TensorType.JAX:\n",
            "            if not is_flax_available():\n",
            "                raise ImportError(\"Unable to convert output to JAX tensors format, JAX is not installed.\")\n",
            "            import jax.numpy as jnp  # noqa: F811\n",
            "\n",
            "            as_tensor = jnp.array\n",
            "            is_tensor = is_jax_tensor\n",
            "        else:\n",
            "\n",
            "            def as_tensor(value, dtype=None):\n",
            "                if isinstance(value, (list, tuple)) and isinstance(value[0], (list, tuple, np.ndarray)):\n",
            "                    value_lens = [len(val) for val in value]\n",
            "                    if len(set(value_lens)) > 1 and dtype is None:\n",
            "                        # we have a ragged list so handle explicitly\n",
            "                        value = as_tensor([np.asarray(val) for val in value], dtype=object)\n",
            "                return np.asarray(value, dtype=dtype)\n",
            "\n",
            "            is_tensor = is_numpy_array\n",
            "        return is_tensor, as_tensor\n",
            "\n",
            "    def convert_to_tensors(self, tensor_type: Optional[Union[str, TensorType]] = None):\n",
            "        \"\"\"\n",
            "        Convert the inner content to tensors.\n",
            "\n",
            "        Args:\n",
            "            tensor_type (`str` or [`~utils.TensorType`], *optional*):\n",
            "                The type of tensors to use. If `str`, should be one of the values of the enum [`~utils.TensorType`]. If\n",
            "                `None`, no modification is done.\n",
            "        \"\"\"\n",
            "        if tensor_type is None:\n",
            "            return self\n",
            "\n",
            "        is_tensor, as_tensor = self._get_is_as_tensor_fns(tensor_type)\n",
            "\n",
            "        # Do the tensor conversion in batch\n",
            "        for key, value in self.items():\n",
            "            try:\n",
            "                if not is_tensor(value):\n",
            "                    tensor = as_tensor(value)\n",
            "\n",
            "                    self[key] = tensor\n",
            "            except:  # noqa E722\n",
            "                if key == \"overflowing_values\":\n",
            "                    raise ValueError(\"Unable to create tensor returning overflowing values of different lengths. \")\n",
            "                raise ValueError(\n",
            "                    \"Unable to create tensor, you should probably activate padding \"\n",
            "                    \"with 'padding=True' to have batched tensors with the same length.\"\n",
            "                )\n",
            "\n",
            "        return self\n",
            "\n",
            "    def to(self, *args, **kwargs) -> \"BatchFeature\":\n",
            "        \"\"\"\n",
            "        Send all values to device by calling `v.to(*args, **kwargs)` (PyTorch only). This should support casting in\n",
            "        different `dtypes` and sending the `BatchFeature` to a different `device`.\n",
            "\n",
            "        Args:\n",
            "            args (`Tuple`):\n",
            "                Will be passed to the `to(...)` function of the tensors.\n",
            "            kwargs (`Dict`, *optional*):\n",
            "                Will be passed to the `to(...)` function of the tensors.\n",
            "\n",
            "        Returns:\n",
            "            [`BatchFeature`]: The same instance after modification.\n",
            "        \"\"\"\n",
            "        requires_backends(self, [\"torch\"])\n",
            "        import torch  # noqa\n",
            "\n",
            "        new_data = {}\n",
            "        device = kwargs.get(\"device\")\n",
            "        # Check if the args are a device or a dtype\n",
            "        if device is None and len(args) > 0:\n",
            "            # device should be always the first argument\n",
            "            arg = args[0]\n",
            "            if is_torch_dtype(arg):\n",
            "                # The first argument is a dtype\n",
            "                pass\n",
            "            elif isinstance(arg, str) or is_torch_device(arg) or isinstance(arg, int):\n",
            "                device = arg\n",
            "            else:\n",
            "                # it's something else\n",
            "                raise ValueError(f\"Attempting to cast a BatchFeature to type {str(arg)}. This is not supported.\")\n",
            "        # We cast only floating point tensors to avoid issues with tokenizers casting `LongTensor` to `FloatTensor`\n",
            "        for k, v in self.items():\n",
            "            # check if v is a floating point\n",
            "            if isinstance(v, torch.Tensor) and torch.is_floating_point(v):\n",
            "                # cast and send to device\n",
            "                new_data[k] = v.to(*args, **kwargs)\n",
            "            elif isinstance(v, torch.Tensor) and device is not None:\n",
            "                new_data[k] = v.to(device=device)\n",
            "            else:\n",
            "                new_data[k] = v\n",
            "        self.data = new_data\n",
            "        return self\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ok so last piece before inspecting the code\n",
        "from transformers import BatchFeature\n",
        "print(inspect.getsource(BatchFeature))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "312340ed-bac8-45db-9746-4b8e902ad4c8",
      "metadata": {
        "id": "312340ed-bac8-45db-9746-4b8e902ad4c8"
      },
      "outputs": [],
      "source": [
        "# ok seems to me nothing interesting\n",
        "# big dictionary, the mire important how it gets populated\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0aef321f-17b6-403d-a2df-0883367f8bf1",
      "metadata": {
        "id": "0aef321f-17b6-403d-a2df-0883367f8bf1",
        "outputId": "42a391de-8e1b-4e53-c640-28063552aaf7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    def __call__(\n",
            "        self,\n",
            "        images: ImageInput = None,\n",
            "        text: Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]] = None,\n",
            "        videos: VideoInput = None,\n",
            "        **kwargs: Unpack[Qwen2VLProcessorKwargs],\n",
            "    ) -> BatchFeature:\n",
            "        \"\"\"\n",
            "        Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n",
            "        and `kwargs` arguments to Qwen2TokenizerFast's [`~Qwen2TokenizerFast.__call__`] if `text` is not `None` to encode\n",
            "        the text. To prepare the vision inputs, this method forwards the `vision_infos` and `kwrags` arguments to\n",
            "        Qwen2VLImageProcessor's [`~Qwen2VLImageProcessor.__call__`] if `vision_infos` is not `None`.\n",
            "\n",
            "        Args:\n",
            "            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `List[PIL.Image.Image]`, `List[np.ndarray]`, `List[torch.Tensor]`):\n",
            "                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n",
            "                tensor. Both channels-first and channels-last formats are supported.\n",
            "            text (`str`, `List[str]`, `List[List[str]]`):\n",
            "                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n",
            "                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n",
            "                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n",
            "            videos (`np.ndarray`, `torch.Tensor`, `List[np.ndarray]`, `List[torch.Tensor]`):\n",
            "                The image or batch of videos to be prepared. Each video can be a 4D NumPy array or PyTorch\n",
            "                tensor, or a nested list of 3D frames. Both channels-first and channels-last formats are supported.\n",
            "            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n",
            "                If set, will return tensors of a particular framework. Acceptable values are:\n",
            "                - `'tf'`: Return TensorFlow `tf.constant` objects.\n",
            "                - `'pt'`: Return PyTorch `torch.Tensor` objects.\n",
            "                - `'np'`: Return NumPy `np.ndarray` objects.\n",
            "                - `'jax'`: Return JAX `jnp.ndarray` objects.\n",
            "\n",
            "        Returns:\n",
            "            [`BatchFeature`]: A [`BatchFeature`] with the following fields:\n",
            "\n",
            "            - **input_ids** -- List of token ids to be fed to a model. Returned when `text` is not `None`.\n",
            "            - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n",
            "              `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names` and if `text` is not\n",
            "              `None`).\n",
            "            - **pixel_values** -- Pixel values to be fed to a model. Returned when `images` is not `None`.\n",
            "            - **pixel_values_videos** -- Pixel values of videos to be fed to a model. Returned when `videos` is not `None`.\n",
            "            - **image_grid_thw** -- List of image 3D grid in LLM. Returned when `images` is not `None`.\n",
            "            - **video_grid_thw** -- List of video 3D grid in LLM. Returned when `videos` is not `None`.\n",
            "        \"\"\"\n",
            "        output_kwargs = self._merge_kwargs(\n",
            "            Qwen2VLProcessorKwargs,\n",
            "            tokenizer_init_kwargs=self.tokenizer.init_kwargs,\n",
            "            **kwargs,\n",
            "        )\n",
            "        if images is not None:\n",
            "            image_inputs = self.image_processor(images=images, videos=None, **output_kwargs[\"images_kwargs\"])\n",
            "            image_grid_thw = image_inputs[\"image_grid_thw\"]\n",
            "        else:\n",
            "            image_inputs = {}\n",
            "            image_grid_thw = None\n",
            "\n",
            "        if videos is not None:\n",
            "            videos_inputs = self.image_processor(images=None, videos=videos, **output_kwargs[\"videos_kwargs\"])\n",
            "            video_grid_thw = videos_inputs[\"video_grid_thw\"]\n",
            "        else:\n",
            "            videos_inputs = {}\n",
            "            video_grid_thw = None\n",
            "\n",
            "        if not isinstance(text, list):\n",
            "            text = [text]\n",
            "\n",
            "        if image_grid_thw is not None:\n",
            "            merge_length = self.image_processor.merge_size**2\n",
            "            index = 0\n",
            "            for i in range(len(text)):\n",
            "                while \"<|image_pad|>\" in text[i]:\n",
            "                    text[i] = text[i].replace(\n",
            "                        \"<|image_pad|>\", \"<|placeholder|>\" * (image_grid_thw[index].prod() // merge_length), 1\n",
            "                    )\n",
            "                    index += 1\n",
            "                text[i] = text[i].replace(\"<|placeholder|>\", \"<|image_pad|>\")\n",
            "\n",
            "        if video_grid_thw is not None:\n",
            "            merge_length = self.image_processor.merge_size**2\n",
            "            index = 0\n",
            "            for i in range(len(text)):\n",
            "                while \"<|video_pad|>\" in text[i]:\n",
            "                    text[i] = text[i].replace(\n",
            "                        \"<|video_pad|>\", \"<|placeholder|>\" * (video_grid_thw[index].prod() // merge_length), 1\n",
            "                    )\n",
            "                    index += 1\n",
            "                text[i] = text[i].replace(\"<|placeholder|>\", \"<|video_pad|>\")\n",
            "\n",
            "        text_inputs = self.tokenizer(text, **output_kwargs[\"text_kwargs\"])\n",
            "\n",
            "        return BatchFeature(data={**text_inputs, **image_inputs, **videos_inputs})\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import inspect\n",
        "\n",
        "# Get the source code of the __call__ method\n",
        "source = inspect.getsource(processor.__call__)\n",
        "print(source)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1987d8f6-4cbd-4ff3-b276-ff4834c9fd72",
      "metadata": {
        "id": "1987d8f6-4cbd-4ff3-b276-ff4834c9fd72",
        "outputId": "576c6537-0dd5-47f5-ea8e-8041c04a9134"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Line 76: the text. To prepare the vision inputs, this method forwards the `vision_infos` and `kwrags` arguments to\n"
          ]
        }
      ],
      "source": [
        "# ok another typo # Check in the source file\n",
        "with open('/usr/local/lib/python3.12/dist-packages/transformers/models/qwen2_vl/processing_qwen2_vl.py', 'r') as f:\n",
        "    lines = f.readlines()\n",
        "    for i, line in enumerate(lines, 1):\n",
        "        if 'kwrags' in line:\n",
        "            print(f\"Line {i}: {line.strip()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d678c335-5026-43c0-a0b1-8a68de88bafb",
      "metadata": {
        "id": "d678c335-5026-43c0-a0b1-8a68de88bafb"
      },
      "outputs": [],
      "source": [
        "# https://github.com/huggingface/transformers/blob/main/src/transformers/models/qwen2_vl/processing_qwen2_vl.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21361f96-3eed-4999-babc-a79162c524d2",
      "metadata": {
        "id": "21361f96-3eed-4999-babc-a79162c524d2"
      },
      "outputs": [],
      "source": [
        "# ok so it seems it works like orchestrator\n",
        "# text goes to Qwen2TokenizerFast.__call__\n",
        "# and images go Qwen2VLImageProcessor.__call__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "370f4f5d-b9af-4b24-9758-a24ad292c593",
      "metadata": {
        "id": "370f4f5d-b9af-4b24-9758-a24ad292c593",
        "outputId": "2e592d04-0df6-4912-9b4f-9655650baeb5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    def _merge_kwargs(\n",
            "        self,\n",
            "        ModelProcessorKwargs: ProcessingKwargs,\n",
            "        tokenizer_init_kwargs: Optional[Dict] = None,\n",
            "        **kwargs,\n",
            "    ) -> Dict[str, Dict]:\n",
            "        \"\"\"\n",
            "        Method to merge dictionaries of kwargs cleanly separated by modality within a Processor instance.\n",
            "        The order of operations is as follows:\n",
            "            1) kwargs passed as before have highest priority to preserve BC.\n",
            "                ```python\n",
            "                high_priority_kwargs = {\"crop_size\" = {\"height\": 222, \"width\": 222}, \"padding\" = \"max_length\"}\n",
            "                processor(..., **high_priority_kwargs)\n",
            "                ```\n",
            "            2) kwargs passed as modality-specific kwargs have second priority. This is the recommended API.\n",
            "                ```python\n",
            "                processor(..., text_kwargs={\"padding\": \"max_length\"}, images_kwargs={\"crop_size\": {\"height\": 222, \"width\": 222}}})\n",
            "                ```\n",
            "            3) kwargs passed during instantiation of a modality processor have fourth priority.\n",
            "                ```python\n",
            "                tokenizer = tokenizer_class(..., {\"padding\": \"max_length\"})\n",
            "                image_processor = image_processor_class(...)\n",
            "                processor(tokenizer, image_processor) # will pass max_length unless overriden by kwargs at call\n",
            "                ```\n",
            "            4) defaults kwargs specified at processor level have lowest priority.\n",
            "                ```python\n",
            "                class MyProcessingKwargs(ProcessingKwargs, CommonKwargs, TextKwargs, ImagesKwargs, total=False):\n",
            "                    _defaults = {\n",
            "                        \"text_kwargs\": {\n",
            "                            \"padding\": \"max_length\",\n",
            "                            \"max_length\": 64,\n",
            "                        },\n",
            "                    }\n",
            "                ```\n",
            "        Args:\n",
            "            ModelProcessorKwargs (`ProcessingKwargs`):\n",
            "                Typed dictionary of kwargs specifically required by the model passed.\n",
            "            tokenizer_init_kwargs (`Dict`, *optional*):\n",
            "                Dictionary of kwargs the tokenizer was instantiated with and need to take precedence over defaults.\n",
            "\n",
            "        Returns:\n",
            "            output_kwargs (`Dict`):\n",
            "                Dictionary of per-modality kwargs to be passed to each modality-specific processor.\n",
            "\n",
            "        \"\"\"\n",
            "        # Initialize dictionaries\n",
            "        output_kwargs = {\n",
            "            \"text_kwargs\": {},\n",
            "            \"images_kwargs\": {},\n",
            "            \"audio_kwargs\": {},\n",
            "            \"videos_kwargs\": {},\n",
            "            \"common_kwargs\": {},\n",
            "        }\n",
            "\n",
            "        default_kwargs = {\n",
            "            \"text_kwargs\": {},\n",
            "            \"images_kwargs\": {},\n",
            "            \"audio_kwargs\": {},\n",
            "            \"videos_kwargs\": {},\n",
            "            \"common_kwargs\": {},\n",
            "        }\n",
            "\n",
            "        used_keys = set()\n",
            "\n",
            "        # get defaults from set model processor kwargs if they exist\n",
            "        for modality in default_kwargs:\n",
            "            default_kwargs[modality] = ModelProcessorKwargs._defaults.get(modality, {}).copy()\n",
            "            # update defaults with arguments from tokenizer init\n",
            "            for modality_key in ModelProcessorKwargs.__annotations__[modality].__annotations__.keys():\n",
            "                # init with tokenizer init kwargs if necessary\n",
            "                if modality_key in tokenizer_init_kwargs:\n",
            "                    value = (\n",
            "                        getattr(self.tokenizer, modality_key)\n",
            "                        if hasattr(self.tokenizer, modality_key)\n",
            "                        else tokenizer_init_kwargs[modality_key]\n",
            "                    )\n",
            "                    default_kwargs[modality][modality_key] = value\n",
            "        # now defaults kwargs are updated with the tokenizers defaults.\n",
            "        # pass defaults to output dictionary\n",
            "        output_kwargs.update(default_kwargs)\n",
            "\n",
            "        # update modality kwargs with passed kwargs\n",
            "        non_modality_kwargs = set(kwargs) - set(output_kwargs)\n",
            "        for modality in output_kwargs:\n",
            "            for modality_key in ModelProcessorKwargs.__annotations__[modality].__annotations__.keys():\n",
            "                # check if we received a structured kwarg dict or not to handle it correctly\n",
            "                if modality in kwargs:\n",
            "                    kwarg_value = kwargs[modality].pop(modality_key, \"__empty__\")\n",
            "                    # check if this key was passed as a flat kwarg.\n",
            "                    if kwarg_value != \"__empty__\" and modality_key in non_modality_kwargs:\n",
            "                        raise ValueError(\n",
            "                            f\"Keyword argument {modality_key} was passed two times:\\n\"\n",
            "                            f\"in a dictionary for {modality} and as a **kwarg.\"\n",
            "                        )\n",
            "                elif modality_key in kwargs:\n",
            "                    # we get a modality_key instead of popping it because modality-specific processors\n",
            "                    # can have overlapping kwargs\n",
            "                    kwarg_value = kwargs.get(modality_key, \"__empty__\")\n",
            "                else:\n",
            "                    kwarg_value = \"__empty__\"\n",
            "                if kwarg_value != \"__empty__\":\n",
            "                    output_kwargs[modality][modality_key] = kwarg_value\n",
            "                    used_keys.add(modality_key)\n",
            "\n",
            "        # Determine if kwargs is a flat dictionary or contains nested dictionaries\n",
            "        if any(key in default_kwargs for key in kwargs):\n",
            "            # kwargs is dictionary-based, and some keys match modality names\n",
            "            for modality, subdict in kwargs.items():\n",
            "                if modality in default_kwargs:\n",
            "                    for subkey, subvalue in subdict.items():\n",
            "                        if subkey not in used_keys:\n",
            "                            output_kwargs[modality][subkey] = subvalue\n",
            "                            used_keys.add(subkey)\n",
            "        else:\n",
            "            # kwargs is a flat dictionary\n",
            "            for key in kwargs:\n",
            "                if key not in used_keys:\n",
            "                    output_kwargs[\"common_kwargs\"][key] = kwargs[key]\n",
            "\n",
            "        # all modality-specific kwargs are updated with common kwargs\n",
            "        for modality in output_kwargs:\n",
            "            output_kwargs[modality].update(output_kwargs[\"common_kwargs\"])\n",
            "        return output_kwargs\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# first # First, let's see what this method looks like\n",
        "import inspect\n",
        "print(inspect.getsource(processor._merge_kwargs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d486dd57-066e-49af-a546-26a9e8b55c2f",
      "metadata": {
        "id": "d486dd57-066e-49af-a546-26a9e8b55c2f"
      },
      "outputs": [],
      "source": [
        "# ok so _merge_kwargs, works as a separator router\n",
        "\n",
        "# ok now is a meaty part\n",
        "\n",
        "# if images is not None:\n",
        "#     image_inputs = self.image_processor(images=images, videos=None, **output_kwargs[\"images_kwargs\"])\n",
        "#     image_grid_thw = image_inputs[\"image_grid_thw\"]\n",
        "# else:\n",
        "#     image_inputs = {}\n",
        "#     image_grid_thw = None\n",
        "\n",
        "# ok let's go down into rabbit hole"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93328a3c-1225-4bf4-b664-468767cbbbea",
      "metadata": {
        "id": "93328a3c-1225-4bf4-b664-468767cbbbea",
        "outputId": "6ff247b6-4c86-448c-ff5d-02d0c9149621"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Image processor __call__ method:\n",
            "    def __call__(self, images, **kwargs) -> BatchFeature:\n",
            "        \"\"\"Preprocess an image or a batch of images.\"\"\"\n",
            "        return self.preprocess(images, **kwargs)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "source = inspect.getsource(processor.image_processor.__call__)\n",
        "print(\"Image processor __call__ method:\")\n",
        "print(source)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14c6d3d7-a2a5-4c2e-856a-db2e6ac488d7",
      "metadata": {
        "id": "14c6d3d7-a2a5-4c2e-856a-db2e6ac488d7",
        "outputId": "374021d6-6ea5-4cb1-b7c0-e7a83b31c38d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    def preprocess(\n",
            "        self,\n",
            "        images: ImageInput,\n",
            "        videos: VideoInput = None,\n",
            "        do_resize: bool = None,\n",
            "        size: Dict[str, int] = None,\n",
            "        resample: PILImageResampling = None,\n",
            "        do_rescale: bool = None,\n",
            "        rescale_factor: float = None,\n",
            "        do_normalize: bool = None,\n",
            "        image_mean: Optional[Union[float, List[float]]] = None,\n",
            "        image_std: Optional[Union[float, List[float]]] = None,\n",
            "        do_convert_rgb: bool = None,\n",
            "        return_tensors: Optional[Union[str, TensorType]] = None,\n",
            "        data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n",
            "        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n",
            "    ):\n",
            "        \"\"\"\n",
            "        Args:\n",
            "            images (`ImageInput`):\n",
            "                Image to preprocess. Expects a single or batch of images with pixel values ranging from 0 to 255. If\n",
            "                passing in images with pixel values between 0 and 1, set `do_rescale=False`.\n",
            "            videos (`VideoInput`):\n",
            "                Video to preprocess. Expects a single or batch of videos with pixel values ranging from 0 to 255. If\n",
            "                passing in videos with pixel values between 0 and 1, set `do_rescale=False`.\n",
            "            do_resize (`bool`, *optional*, defaults to `self.do_resize`):\n",
            "                Whether to resize the image.\n",
            "            size (`Dict[str, int]`, *optional*, defaults to `self.size`):\n",
            "                Size of the image after resizing. Shortest edge of the image is resized to size[\"shortest_edge\"], with\n",
            "                the longest edge resized to keep the input aspect ratio.\n",
            "            resample (`int`, *optional*, defaults to `self.resample`):\n",
            "                Resampling filter to use if resizing the image. This can be one of the enum `PILImageResampling`. Only\n",
            "                has an effect if `do_resize` is set to `True`.\n",
            "            do_rescale (`bool`, *optional*, defaults to `self.do_rescale`):\n",
            "                Whether to rescale the image.\n",
            "            rescale_factor (`float`, *optional*, defaults to `self.rescale_factor`):\n",
            "                Rescale factor to rescale the image by if `do_rescale` is set to `True`.\n",
            "            do_normalize (`bool`, *optional*, defaults to `self.do_normalize`):\n",
            "                Whether to normalize the image.\n",
            "            image_mean (`float` or `List[float]`, *optional*, defaults to `self.image_mean`):\n",
            "                Image mean to use for normalization. Only has an effect if `do_normalize` is set to `True`.\n",
            "            image_std (`float` or `List[float]`, *optional*, defaults to `self.image_std`):\n",
            "                Image standard deviation to use for normalization. Only has an effect if `do_normalize` is set to\n",
            "                `True`.\n",
            "            do_convert_rgb (`bool`, *optional*, defaults to `self.do_convert_rgb`):\n",
            "                Whether to convert the image to RGB.\n",
            "            return_tensors (`str` or `TensorType`, *optional*):\n",
            "                The type of tensors to return. Can be one of:\n",
            "                - Unset: Return a list of `np.ndarray`.\n",
            "                - `TensorType.TENSORFLOW` or `'tf'`: Return a batch of type `tf.Tensor`.\n",
            "                - `TensorType.PYTORCH` or `'pt'`: Return a batch of type `torch.Tensor`.\n",
            "                - `TensorType.NUMPY` or `'np'`: Return a batch of type `np.ndarray`.\n",
            "                - `TensorType.JAX` or `'jax'`: Return a batch of type `jax.numpy.ndarray`.\n",
            "            data_format (`ChannelDimension` or `str`, *optional*, defaults to `ChannelDimension.FIRST`):\n",
            "                The channel dimension format for the output image. Can be one of:\n",
            "                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n",
            "                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n",
            "                - Unset: Use the channel dimension format of the input image.\n",
            "            input_data_format (`ChannelDimension` or `str`, *optional*):\n",
            "                The channel dimension format for the input image. If unset, the channel dimension format is inferred\n",
            "                from the input image. Can be one of:\n",
            "                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n",
            "                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n",
            "                - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n",
            "\n",
            "        \"\"\"\n",
            "        do_resize = do_resize if do_resize is not None else self.do_resize\n",
            "        size = size if size is not None else self.size\n",
            "        resample = resample if resample is not None else self.resample\n",
            "        do_rescale = do_rescale if do_rescale is not None else self.do_rescale\n",
            "        rescale_factor = rescale_factor if rescale_factor is not None else self.rescale_factor\n",
            "        do_normalize = do_normalize if do_normalize is not None else self.do_normalize\n",
            "        image_mean = image_mean if image_mean is not None else self.image_mean\n",
            "        image_std = image_std if image_std is not None else self.image_std\n",
            "        do_convert_rgb = do_convert_rgb if do_convert_rgb is not None else self.do_convert_rgb\n",
            "\n",
            "        if images is not None:\n",
            "            images = make_batched_images(images)\n",
            "        if videos is not None:\n",
            "            videos = make_batched_videos(videos)\n",
            "\n",
            "        if images is not None and not valid_images(images):\n",
            "            raise ValueError(\n",
            "                \"Invalid image type. Must be of type PIL.Image.Image, numpy.ndarray, \"\n",
            "                \"torch.Tensor, tf.Tensor or jax.ndarray.\"\n",
            "            )\n",
            "\n",
            "        validate_preprocess_arguments(\n",
            "            rescale_factor=rescale_factor,\n",
            "            do_normalize=do_normalize,\n",
            "            image_mean=image_mean,\n",
            "            image_std=image_std,\n",
            "            do_resize=do_resize,\n",
            "            size=size,\n",
            "            resample=resample,\n",
            "        )\n",
            "\n",
            "        if images is not None:\n",
            "            pixel_values, vision_grid_thws = [], []\n",
            "            for image in images:\n",
            "                patches, image_grid_thw = self._preprocess(\n",
            "                    image,\n",
            "                    do_resize=do_resize,\n",
            "                    resample=resample,\n",
            "                    do_rescale=do_rescale,\n",
            "                    rescale_factor=rescale_factor,\n",
            "                    do_normalize=do_normalize,\n",
            "                    image_mean=image_mean,\n",
            "                    image_std=image_std,\n",
            "                    data_format=data_format,\n",
            "                    do_convert_rgb=do_convert_rgb,\n",
            "                    input_data_format=input_data_format,\n",
            "                )\n",
            "                pixel_values.extend(patches)\n",
            "                vision_grid_thws.append(image_grid_thw)\n",
            "            pixel_values = np.array(pixel_values)\n",
            "            vision_grid_thws = np.array(vision_grid_thws)\n",
            "            data = {\"pixel_values\": pixel_values, \"image_grid_thw\": vision_grid_thws}\n",
            "\n",
            "        if videos is not None:\n",
            "            pixel_values, vision_grid_thws = [], []\n",
            "            for images in videos:\n",
            "                patches, video_grid_thw = self._preprocess(\n",
            "                    images,\n",
            "                    do_resize=do_resize,\n",
            "                    resample=resample,\n",
            "                    do_rescale=do_rescale,\n",
            "                    rescale_factor=rescale_factor,\n",
            "                    do_normalize=do_normalize,\n",
            "                    image_mean=image_mean,\n",
            "                    image_std=image_std,\n",
            "                    data_format=data_format,\n",
            "                    do_convert_rgb=do_convert_rgb,\n",
            "                    input_data_format=input_data_format,\n",
            "                )\n",
            "                pixel_values.extend(patches)\n",
            "                vision_grid_thws.append(video_grid_thw)\n",
            "            pixel_values = np.array(pixel_values)\n",
            "            vision_grid_thws = np.array(vision_grid_thws)\n",
            "            data = {\"pixel_values_videos\": pixel_values, \"video_grid_thw\": vision_grid_thws}\n",
            "\n",
            "        return BatchFeature(data=data, tensor_type=return_tensors)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# oh my god it's another wrapper\n",
        "source = inspect.getsource(processor.image_processor.preprocess)\n",
        "print(source)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef42f937-040a-4931-97e2-672643eab871",
      "metadata": {
        "id": "ef42f937-040a-4931-97e2-672643eab871"
      },
      "outputs": [],
      "source": [
        "# okey there is another real _preprocess function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a8864cd-5be8-4423-93f0-09b66f08ee61",
      "metadata": {
        "id": "0a8864cd-5be8-4423-93f0-09b66f08ee61",
        "outputId": "3e8bbaa7-9447-452d-87de-4cc56fc48fe1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    def _preprocess(\n",
            "        self,\n",
            "        images: Union[ImageInput, VideoInput],\n",
            "        do_resize: bool = None,\n",
            "        resample: PILImageResampling = None,\n",
            "        do_rescale: bool = None,\n",
            "        rescale_factor: float = None,\n",
            "        do_normalize: bool = None,\n",
            "        image_mean: Optional[Union[float, List[float]]] = None,\n",
            "        image_std: Optional[Union[float, List[float]]] = None,\n",
            "        do_convert_rgb: bool = None,\n",
            "        data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n",
            "        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n",
            "    ):\n",
            "        \"\"\"\n",
            "        Preprocess an image or batch of images. Copy of the `preprocess` method from `CLIPImageProcessor`.\n",
            "\n",
            "        Args:\n",
            "            images (`ImageInput`):\n",
            "                Image or batch of images to preprocess. Expects pixel values ranging from 0 to 255. If pixel values range from 0 to 1, set `do_rescale=False`.\n",
            "            vision_info (`List[Dict]`, *optional*):\n",
            "                Optional list of dictionaries containing additional information about vision inputs.\n",
            "            do_resize (`bool`, *optional*, defaults to `self.do_resize`):\n",
            "                Whether to resize the image.\n",
            "            resample (`PILImageResampling`, *optional*, defaults to `self.resample`):\n",
            "                Resampling filter to use if resizing the image. This can be one of the `PILImageResampling` enums.\n",
            "            do_rescale (`bool`, *optional*, defaults to `self.do_rescale`):\n",
            "                Whether to rescale the image.\n",
            "            rescale_factor (`float`, *optional*, defaults to `self.rescale_factor`):\n",
            "                Scale factor to use if rescaling the image.\n",
            "            do_normalize (`bool`, *optional*, defaults to `self.do_normalize`):\n",
            "                Whether to normalize the image.\n",
            "            image_mean (`float` or `List[float]`, *optional*, defaults to `self.image_mean`):\n",
            "                Mean to use if normalizing the image. Can be a float or a list of floats corresponding to the number of channels in the image.\n",
            "            image_std (`float` or `List[float]`, *optional*, defaults to `self.image_std`):\n",
            "                Standard deviation to use if normalizing the image. Can be a float or a list of floats corresponding to the number of channels in the image.\n",
            "            do_convert_rgb (`bool`, *optional*, defaults to `self.do_convert_rgb`):\n",
            "                Whether to convert the image to RGB.\n",
            "            data_format (`ChannelDimension`, *optional*, defaults to `ChannelDimension.FIRST`):\n",
            "                The channel dimension format for the output image. Can be one of:\n",
            "                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n",
            "                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n",
            "                - Unset: Use the channel dimension format of the input image.\n",
            "            input_data_format (`ChannelDimension` or `str`, *optional*):\n",
            "                The channel dimension format for the input image. Can be one of:\n",
            "                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n",
            "                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n",
            "                - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.   - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n",
            "        \"\"\"\n",
            "        images = make_list_of_images(images)\n",
            "\n",
            "        if do_convert_rgb:\n",
            "            images = [convert_to_rgb(image) for image in images]\n",
            "\n",
            "        # All transformations expect numpy arrays.\n",
            "        images = [to_numpy_array(image) for image in images]\n",
            "\n",
            "        if is_scaled_image(images[0]) and do_rescale:\n",
            "            logger.warning_once(\n",
            "                \"It looks like you are trying to rescale already rescaled images. If the input\"\n",
            "                \" images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\"\n",
            "            )\n",
            "        if input_data_format is None:\n",
            "            # We assume that all images have the same channel dimension format.\n",
            "            input_data_format = infer_channel_dimension_format(images[0])\n",
            "\n",
            "        height, width = get_image_size(images[0], channel_dim=input_data_format)\n",
            "        resized_height, resized_width = height, width\n",
            "        processed_images = []\n",
            "        for image in images:\n",
            "            if do_resize:\n",
            "                resized_height, resized_width = smart_resize(\n",
            "                    height,\n",
            "                    width,\n",
            "                    factor=self.patch_size * self.merge_size,\n",
            "                    min_pixels=self.min_pixels,\n",
            "                    max_pixels=self.max_pixels,\n",
            "                )\n",
            "                image = resize(\n",
            "                    image, size=(resized_height, resized_width), resample=resample, input_data_format=input_data_format\n",
            "                )\n",
            "\n",
            "            if do_rescale:\n",
            "                image = self.rescale(image, scale=rescale_factor, input_data_format=input_data_format)\n",
            "\n",
            "            if do_normalize:\n",
            "                image = self.normalize(\n",
            "                    image=image, mean=image_mean, std=image_std, input_data_format=input_data_format\n",
            "                )\n",
            "\n",
            "            image = to_channel_dimension_format(image, data_format, input_channel_dim=input_data_format)\n",
            "            processed_images.append(image)\n",
            "\n",
            "        patches = np.array(processed_images)\n",
            "        if data_format == ChannelDimension.LAST:\n",
            "            patches = patches.transpose(0, 3, 1, 2)\n",
            "        if patches.shape[0] == 1:\n",
            "            patches = np.tile(patches, (self.temporal_patch_size, 1, 1, 1))\n",
            "        channel = patches.shape[1]\n",
            "        grid_t = patches.shape[0] // self.temporal_patch_size\n",
            "        grid_h, grid_w = resized_height // self.patch_size, resized_width // self.patch_size\n",
            "        patches = patches.reshape(\n",
            "            grid_t,\n",
            "            self.temporal_patch_size,\n",
            "            channel,\n",
            "            grid_h // self.merge_size,\n",
            "            self.merge_size,\n",
            "            self.patch_size,\n",
            "            grid_w // self.merge_size,\n",
            "            self.merge_size,\n",
            "            self.patch_size,\n",
            "        )\n",
            "        patches = patches.transpose(0, 3, 6, 4, 7, 2, 1, 5, 8)\n",
            "        flatten_patches = patches.reshape(\n",
            "            grid_t * grid_h * grid_w, channel * self.temporal_patch_size * self.patch_size * self.patch_size\n",
            "        )\n",
            "\n",
            "        return flatten_patches, (grid_t, grid_h, grid_w)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "source = inspect.getsource(processor.image_processor._preprocess)\n",
        "print(source)  # First part to see what it does"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c45acd1e-2770-4a4d-b55e-d62847e9b18c",
      "metadata": {
        "id": "c45acd1e-2770-4a4d-b55e-d62847e9b18c"
      },
      "outputs": [],
      "source": [
        "# ok it seems that we catched a really big fish :))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9944ab5e-e21c-4b42-a7a3-64f18ccd03d2",
      "metadata": {
        "id": "9944ab5e-e21c-4b42-a7a3-64f18ccd03d2",
        "outputId": "e5d77c1b-31d6-4c96-fc8f-6263a1286aee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "patch_size: 14\n",
            "merge_size: 2\n",
            "temporal_patch_size: 2\n",
            "Factor images must divide by: 28\n"
          ]
        }
      ],
      "source": [
        "print(f\"patch_size: {processor.image_processor.patch_size}\")\n",
        "print(f\"merge_size: {processor.image_processor.merge_size}\")\n",
        "print(f\"temporal_patch_size: {processor.image_processor.temporal_patch_size}\")\n",
        "print(f\"Factor images must divide by: {processor.image_processor.patch_size * processor.image_processor.merge_size}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e6af20a-a207-47e3-a91e-d8ba6be763ed",
      "metadata": {
        "id": "1e6af20a-a207-47e3-a91e-d8ba6be763ed",
        "outputId": "1e66bd1b-51b2-46eb-d53c-cfe51b9132d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "def make_list_of_images(images, expected_ndims: int = 3) -> List[ImageInput]:\n",
            "    \"\"\"\n",
            "    Ensure that the input is a list of images. If the input is a single image, it is converted to a list of length 1.\n",
            "    If the input is a batch of images, it is converted to a list of images.\n",
            "\n",
            "    Args:\n",
            "        images (`ImageInput`):\n",
            "            Image of images to turn into a list of images.\n",
            "        expected_ndims (`int`, *optional*, defaults to 3):\n",
            "            Expected number of dimensions for a single input image. If the input image has a different number of\n",
            "            dimensions, an error is raised.\n",
            "    \"\"\"\n",
            "    if is_batched(images):\n",
            "        return images\n",
            "\n",
            "    # Either the input is a single image, in which case we create a list of length 1\n",
            "    if isinstance(images, PIL.Image.Image):\n",
            "        # PIL images are never batched\n",
            "        return [images]\n",
            "\n",
            "    if is_valid_image(images):\n",
            "        if images.ndim == expected_ndims + 1:\n",
            "            # Batch of images\n",
            "            images = list(images)\n",
            "        elif images.ndim == expected_ndims:\n",
            "            # Single image\n",
            "            images = [images]\n",
            "        else:\n",
            "            raise ValueError(\n",
            "                f\"Invalid image shape. Expected either {expected_ndims + 1} or {expected_ndims} dimensions, but got\"\n",
            "                f\" {images.ndim} dimensions.\"\n",
            "            )\n",
            "        return images\n",
            "    raise ValueError(\n",
            "        \"Invalid image type. Expected either PIL.Image.Image, numpy.ndarray, torch.Tensor, tf.Tensor or \"\n",
            "        f\"jax.ndarray, but got {type(images)}.\"\n",
            "    )\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Let's see the actual implementation\n",
        "from transformers.image_utils import make_list_of_images\n",
        "import inspect\n",
        "\n",
        "source = inspect.getsource(make_list_of_images)\n",
        "print(source)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0cb8af3-43fc-4196-b3e9-5d354ed9bc4d",
      "metadata": {
        "id": "a0cb8af3-43fc-4196-b3e9-5d354ed9bc4d",
        "outputId": "3deb6da5-8386-4d17-9304-bb4d5bf47900"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "def is_batched(img):\n",
            "    if isinstance(img, (list, tuple)):\n",
            "        return is_valid_image(img[0])\n",
            "    return False\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Let's find is_batched\n",
        "from transformers.image_utils import is_batched\n",
        "import inspect\n",
        "\n",
        "source = inspect.getsource(is_batched)\n",
        "print(source)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fc6c059-88cc-460c-998b-91d9506e92a4",
      "metadata": {
        "id": "6fc6c059-88cc-460c-998b-91d9506e92a4",
        "outputId": "509ad855-d3c1-4060-e01e-f2cdc9e7ef79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "def is_valid_image(img):\n",
            "    return is_pil_image(img) or is_numpy_array(img) or is_torch_tensor(img) or is_tf_tensor(img) or is_jax_tensor(img)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from transformers.image_utils import is_valid_image\n",
        "import inspect\n",
        "\n",
        "source = inspect.getsource(is_valid_image)\n",
        "print(source)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52245a9a-bd16-4fa2-bd17-01efe681fc19",
      "metadata": {
        "id": "52245a9a-bd16-4fa2-bd17-01efe681fc19",
        "outputId": "985a10bf-80b6-422e-dbc6-0abaa09e25e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "Function: is_pil_image\n",
            "==================================================\n",
            "def is_pil_image(img):\n",
            "    return is_vision_available() and isinstance(img, PIL.Image.Image)\n",
            "\n",
            "\n",
            "==================================================\n",
            "Function: is_numpy_array\n",
            "==================================================\n",
            "def is_numpy_array(x):\n",
            "    \"\"\"\n",
            "    Tests if `x` is a numpy array or not.\n",
            "    \"\"\"\n",
            "    return _is_numpy(x)\n",
            "\n",
            "\n",
            "==================================================\n",
            "Function: is_torch_tensor\n",
            "==================================================\n",
            "def is_torch_tensor(x):\n",
            "    \"\"\"\n",
            "    Tests if `x` is a torch tensor or not. Safe to call even if torch is not installed.\n",
            "    \"\"\"\n",
            "    return False if not is_torch_available() else _is_torch(x)\n",
            "\n",
            "\n",
            "==================================================\n",
            "Function: is_tf_tensor\n",
            "==================================================\n",
            "def is_tf_tensor(x):\n",
            "    \"\"\"\n",
            "    Tests if `x` is a tensorflow tensor or not. Safe to call even if tensorflow is not installed.\n",
            "    \"\"\"\n",
            "    return False if not is_tf_available() else _is_tensorflow(x)\n",
            "\n",
            "\n",
            "==================================================\n",
            "Function: is_jax_tensor\n",
            "==================================================\n",
            "def is_jax_tensor(x):\n",
            "    \"\"\"\n",
            "    Tests if `x` is a Jax tensor or not. Safe to call even if jax is not installed.\n",
            "    \"\"\"\n",
            "    return False if not is_flax_available() else _is_jax(x)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from transformers.image_utils import (\n",
        "    is_pil_image,\n",
        "    is_numpy_array,\n",
        "    is_torch_tensor,\n",
        "    is_tf_tensor,\n",
        "    is_jax_tensor\n",
        ")\n",
        "import inspect\n",
        "\n",
        "# Check each validation function\n",
        "checks = [is_pil_image, is_numpy_array, is_torch_tensor, is_tf_tensor, is_jax_tensor]\n",
        "\n",
        "for check_func in checks:\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Function: {check_func.__name__}\")\n",
        "    print('='*50)\n",
        "    print(inspect.getsource(check_func))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd474b01-17f2-4f22-95a5-14b5e8edeb59",
      "metadata": {
        "id": "fd474b01-17f2-4f22-95a5-14b5e8edeb59",
        "outputId": "fddca5c9-19e4-4664-9ba3-1f500d045d3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "Function: is_vision_available\n",
            "==================================================\n",
            "@lru_cache\n",
            "def is_vision_available():\n",
            "    _pil_available = importlib.util.find_spec(\"PIL\") is not None\n",
            "    if _pil_available:\n",
            "        try:\n",
            "            package_version = importlib.metadata.version(\"Pillow\")\n",
            "        except importlib.metadata.PackageNotFoundError:\n",
            "            try:\n",
            "                package_version = importlib.metadata.version(\"Pillow-SIMD\")\n",
            "            except importlib.metadata.PackageNotFoundError:\n",
            "                return False\n",
            "        logger.debug(f\"Detected PIL version {package_version}\")\n",
            "    return _pil_available\n",
            "\n",
            "\n",
            "==================================================\n",
            "Function: is_torch_available\n",
            "==================================================\n",
            "def is_torch_available():\n",
            "    return _torch_available\n",
            "\n",
            "\n",
            "==================================================\n",
            "Function: is_tf_available\n",
            "==================================================\n",
            "def is_tf_available():\n",
            "    return _tf_available\n",
            "\n",
            "\n",
            "==================================================\n",
            "Function: is_flax_available\n",
            "==================================================\n",
            "def is_flax_available():\n",
            "    return _flax_available\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from transformers.utils import is_vision_available, is_torch_available, is_tf_available, is_flax_available\n",
        "\n",
        "availability_checks = [is_vision_available, is_torch_available, is_tf_available, is_flax_available]\n",
        "\n",
        "for func in availability_checks:\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Function: {func.__name__}\")\n",
        "    print('='*50)\n",
        "    print(inspect.getsource(func))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fcebf039-d8d9-4e5b-9b51-94942d2b1684",
      "metadata": {
        "id": "fcebf039-d8d9-4e5b-9b51-94942d2b1684",
        "outputId": "05bbd989-bda9-4670-923c-f5d4e4b6960d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "def convert_to_rgb(image: ImageInput) -> ImageInput:\n",
            "    \"\"\"\n",
            "    Converts an image to RGB format. Only converts if the image is of type PIL.Image.Image, otherwise returns the image\n",
            "    as is.\n",
            "    Args:\n",
            "        image (Image):\n",
            "            The image to convert.\n",
            "    \"\"\"\n",
            "    requires_backends(convert_to_rgb, [\"vision\"])\n",
            "\n",
            "    if not isinstance(image, PIL.Image.Image):\n",
            "        return image\n",
            "\n",
            "    if image.mode == \"RGB\":\n",
            "        return image\n",
            "\n",
            "    image = image.convert(\"RGB\")\n",
            "    return image\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Now we know where it's from\n",
        "from transformers.image_transforms import convert_to_rgb\n",
        "import inspect\n",
        "\n",
        "source = inspect.getsource(convert_to_rgb)\n",
        "print(source)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1f63d10-b266-42fe-a5c3-e84cc52a164d",
      "metadata": {
        "id": "f1f63d10-b266-42fe-a5c3-e84cc52a164d",
        "outputId": "6f292b88-4e0d-4fcd-8aa1-ed86438b3dcb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "def to_numpy_array(img) -> np.ndarray:\n",
            "    if not is_valid_image(img):\n",
            "        raise ValueError(f\"Invalid image type: {type(img)}\")\n",
            "\n",
            "    if is_vision_available() and isinstance(img, PIL.Image.Image):\n",
            "        return np.array(img)\n",
            "    return to_numpy(img)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Next function in the pipeline\n",
        "from transformers.image_utils import to_numpy_array\n",
        "import inspect\n",
        "\n",
        "source = inspect.getsource(to_numpy_array)\n",
        "print(source)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a43daf6-d5de-4f51-989c-4a2e96558b80",
      "metadata": {
        "id": "1a43daf6-d5de-4f51-989c-4a2e96558b80",
        "outputId": "e6f80f9c-c19c-4a71-9dc0-b269e4f7232f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "def to_numpy(obj):\n",
            "    \"\"\"\n",
            "    Convert a TensorFlow tensor, PyTorch tensor, Numpy array or python list to a Numpy array.\n",
            "    \"\"\"\n",
            "\n",
            "    framework_to_numpy = {\n",
            "        \"pt\": lambda obj: obj.detach().cpu().numpy(),\n",
            "        \"tf\": lambda obj: obj.numpy(),\n",
            "        \"jax\": lambda obj: np.asarray(obj),\n",
            "        \"np\": lambda obj: obj,\n",
            "    }\n",
            "\n",
            "    if isinstance(obj, (dict, UserDict)):\n",
            "        return {k: to_numpy(v) for k, v in obj.items()}\n",
            "    elif isinstance(obj, (list, tuple)):\n",
            "        return np.array(obj)\n",
            "\n",
            "    # This gives us a smart order to test the frameworks with the corresponding tests.\n",
            "    framework_to_test_func = _get_frameworks_and_test_func(obj)\n",
            "    for framework, test_func in framework_to_test_func.items():\n",
            "        if test_func(obj):\n",
            "            return framework_to_numpy[framework](obj)\n",
            "\n",
            "    return obj\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Let's see to_numpy which handles tensor conversion\n",
        "from transformers.utils import to_numpy\n",
        "import inspect\n",
        "\n",
        "source = inspect.getsource(to_numpy)\n",
        "print(source)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68abfa3f-79f7-40a4-a129-8850cea3fe5a",
      "metadata": {
        "id": "68abfa3f-79f7-40a4-a129-8850cea3fe5a",
        "outputId": "e5d75a05-4de8-4f12-dc4e-98488daa8d77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "def is_scaled_image(image: np.ndarray) -> bool:\n",
            "    \"\"\"\n",
            "    Checks to see whether the pixel values have already been rescaled to [0, 1].\n",
            "    \"\"\"\n",
            "    if image.dtype == np.uint8:\n",
            "        return False\n",
            "\n",
            "    # It's possible the image has pixel values in [0, 255] but is of floating type\n",
            "    return np.min(image) >= 0 and np.max(image) <= 1\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Next is the check for scaled images\n",
        "from transformers.image_utils import is_scaled_image\n",
        "import inspect\n",
        "\n",
        "source = inspect.getsource(is_scaled_image)\n",
        "print(source)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85521d0e-256a-4b30-bd1d-6cd77c4136a2",
      "metadata": {
        "id": "85521d0e-256a-4b30-bd1d-6cd77c4136a2",
        "outputId": "eba76a47-28b9-4571-a4c6-1a840e92b31d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "def infer_channel_dimension_format(\n",
            "    image: np.ndarray, num_channels: Optional[Union[int, Tuple[int, ...]]] = None\n",
            ") -> ChannelDimension:\n",
            "    \"\"\"\n",
            "    Infers the channel dimension format of `image`.\n",
            "\n",
            "    Args:\n",
            "        image (`np.ndarray`):\n",
            "            The image to infer the channel dimension of.\n",
            "        num_channels (`int` or `Tuple[int, ...]`, *optional*, defaults to `(1, 3)`):\n",
            "            The number of channels of the image.\n",
            "\n",
            "    Returns:\n",
            "        The channel dimension of the image.\n",
            "    \"\"\"\n",
            "    num_channels = num_channels if num_channels is not None else (1, 3)\n",
            "    num_channels = (num_channels,) if isinstance(num_channels, int) else num_channels\n",
            "\n",
            "    if image.ndim == 3:\n",
            "        first_dim, last_dim = 0, 2\n",
            "    elif image.ndim == 4:\n",
            "        first_dim, last_dim = 1, 3\n",
            "    else:\n",
            "        raise ValueError(f\"Unsupported number of image dimensions: {image.ndim}\")\n",
            "\n",
            "    if image.shape[first_dim] in num_channels and image.shape[last_dim] in num_channels:\n",
            "        logger.warning(\n",
            "            f\"The channel dimension is ambiguous. Got image shape {image.shape}. Assuming channels are the first dimension.\"\n",
            "        )\n",
            "        return ChannelDimension.FIRST\n",
            "    elif image.shape[first_dim] in num_channels:\n",
            "        return ChannelDimension.FIRST\n",
            "    elif image.shape[last_dim] in num_channels:\n",
            "        return ChannelDimension.LAST\n",
            "    raise ValueError(\"Unable to infer channel dimension format\")\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Next function that infers the channel layout\n",
        "from transformers.image_utils import infer_channel_dimension_format\n",
        "import inspect\n",
        "\n",
        "source = inspect.getsource(infer_channel_dimension_format)\n",
        "print(source)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fdb5e44e-217b-4672-a133-837ed310de42",
      "metadata": {
        "id": "fdb5e44e-217b-4672-a133-837ed310de42"
      },
      "outputs": [],
      "source": [
        "# # PyTorch expects: (C, H, W) - channels first\n",
        "# torch_image = torch.randn(3, 224, 224)  # (3 channels, 224 height, 224 width)\n",
        "\n",
        "# # TensorFlow/Keras expects: (H, W, C) - channels last\n",
        "# tf_image = tf.random.normal([224, 224, 3])  # (224 height, 224 width, 3 channels)\n",
        "\n",
        "# # PIL also uses: (H, W, C)\n",
        "# pil_array = np.array(pil_image)  # Shape: (height, width, 3)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8cb5091-4f81-4535-9d05-187b4071d81a",
      "metadata": {
        "id": "e8cb5091-4f81-4535-9d05-187b4071d81a",
        "outputId": "cc5f42cc-1832-4baa-dc2f-fa3ad96b11ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "def get_image_size(image: np.ndarray, channel_dim: ChannelDimension = None) -> Tuple[int, int]:\n",
            "    \"\"\"\n",
            "    Returns the (height, width) dimensions of the image.\n",
            "\n",
            "    Args:\n",
            "        image (`np.ndarray`):\n",
            "            The image to get the dimensions of.\n",
            "        channel_dim (`ChannelDimension`, *optional*):\n",
            "            Which dimension the channel dimension is in. If `None`, will infer the channel dimension from the image.\n",
            "\n",
            "    Returns:\n",
            "        A tuple of the image's height and width.\n",
            "    \"\"\"\n",
            "    if channel_dim is None:\n",
            "        channel_dim = infer_channel_dimension_format(image)\n",
            "\n",
            "    if channel_dim == ChannelDimension.FIRST:\n",
            "        return image.shape[-2], image.shape[-1]\n",
            "    elif channel_dim == ChannelDimension.LAST:\n",
            "        return image.shape[-3], image.shape[-2]\n",
            "    else:\n",
            "        raise ValueError(f\"Unsupported data format: {channel_dim}\")\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Get the get_image_size function\n",
        "from transformers.image_utils import get_image_size\n",
        "import inspect\n",
        "\n",
        "source = inspect.getsource(get_image_size)\n",
        "print(source)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4d12af0-679f-468d-96d8-6db11f03ab50",
      "metadata": {
        "id": "b4d12af0-679f-468d-96d8-6db11f03ab50"
      },
      "outputs": [],
      "source": [
        "# so like it just returns weight and height based on the data format\n",
        "# and the data format of image is decided where channel is\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "adea046a-4baf-45d1-8497-c84327f18d5c",
      "metadata": {
        "id": "adea046a-4baf-45d1-8497-c84327f18d5c",
        "outputId": "d2675356-3ad8-4559-df04-b279a2d36329"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Channel dimension inference:\n",
            "image_CHW shape (3, 224, 256) → format: ChannelDimension.FIRST\n",
            "image_HWC shape (224, 256, 3) → format: ChannelDimension.LAST\n",
            "\n",
            "Size extraction:\n",
            "image_CHW (3, 224, 256) → size: (224, 256)\n",
            "image_HWC (224, 256, 3) → size: (224, 256)\n",
            "\n",
            "Size extraction (auto-inferred):\n",
            "image_CHW → size: (224, 256)\n",
            "image_HWC → size: (224, 256)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from transformers.image_utils import infer_channel_dimension_format, get_image_size\n",
        "\n",
        "# Create test images\n",
        "image_CHW = np.ones((3, 224, 256))   # Channels first\n",
        "image_HWC = np.ones((224, 256, 3))   # Channels last\n",
        "\n",
        "# Test inference\n",
        "format_CHW = infer_channel_dimension_format(image_CHW)\n",
        "format_HWC = infer_channel_dimension_format(image_HWC)\n",
        "\n",
        "print(\"Channel dimension inference:\")\n",
        "print(f\"image_CHW shape {image_CHW.shape} → format: {format_CHW}\")\n",
        "print(f\"image_HWC shape {image_HWC.shape} → format: {format_HWC}\")\n",
        "\n",
        "# Test size extraction\n",
        "size_CHW = get_image_size(image_CHW, channel_dim=format_CHW)\n",
        "size_HWC = get_image_size(image_HWC, channel_dim=format_HWC)\n",
        "\n",
        "print(\"\\nSize extraction:\")\n",
        "print(f\"image_CHW {image_CHW.shape} → size: {size_CHW}\")\n",
        "print(f\"image_HWC {image_HWC.shape} → size: {size_HWC}\")\n",
        "\n",
        "# Also test without providing channel_dim (it should infer)\n",
        "size_CHW_auto = get_image_size(image_CHW)\n",
        "size_HWC_auto = get_image_size(image_HWC)\n",
        "\n",
        "print(\"\\nSize extraction (auto-inferred):\")\n",
        "print(f\"image_CHW → size: {size_CHW_auto}\")\n",
        "print(f\"image_HWC → size: {size_HWC_auto}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d94e2fd7-ed30-45f8-b39c-89b23e9dc503",
      "metadata": {
        "id": "d94e2fd7-ed30-45f8-b39c-89b23e9dc503",
        "outputId": "0ff4c5da-b9bb-408f-9d15-230f09324065"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    def _preprocess(\n",
            "        self,\n",
            "        images: Union[ImageInput, VideoInput],\n",
            "        do_resize: bool = None,\n",
            "        resample: PILImageResampling = None,\n",
            "        do_rescale: bool = None,\n",
            "        rescale_factor: float = None,\n",
            "        do_normalize: bool = None,\n",
            "        image_mean: Optional[Union[float, List[float]]] = None,\n",
            "        image_std: Optional[Union[float, List[float]]] = None,\n",
            "        do_convert_rgb: bool = None,\n",
            "        data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n",
            "        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n",
            "    ):\n",
            "        \"\"\"\n",
            "        Preprocess an image or batch of images. Copy of the `preprocess` method from `CLIPImageProcessor`.\n",
            "\n",
            "        Args:\n",
            "            images (`ImageInput`):\n",
            "                Image or batch of images to preprocess. Expects pixel values ranging from 0 to 255. If pixel values range from 0 to 1, set `do_rescale=False`.\n",
            "            vision_info (`List[Dict]`, *optional*):\n",
            "                Optional list of dictionaries containing additional information about vision inputs.\n",
            "            do_resize (`bool`, *optional*, defaults to `self.do_resize`):\n",
            "                Whether to resize the image.\n",
            "            resample (`PILImageResampling`, *optional*, defaults to `self.resample`):\n",
            "                Resampling filter to use if resizing the image. This can be one of the `PILImageResampling` enums.\n",
            "            do_rescale (`bool`, *optional*, defaults to `self.do_rescale`):\n",
            "                Whether to rescale the image.\n",
            "            rescale_factor (`float`, *optional*, defaults to `self.rescale_factor`):\n",
            "                Scale factor to use if rescaling the image.\n",
            "            do_normalize (`bool`, *optional*, defaults to `self.do_normalize`):\n",
            "                Whether to normalize the image.\n",
            "            image_mean (`float` or `List[float]`, *optional*, defaults to `self.image_mean`):\n",
            "                Mean to use if normalizing the image. Can be a float or a list of floats corresponding to the number of channels in the image.\n",
            "            image_std (`float` or `List[float]`, *optional*, defaults to `self.image_std`):\n",
            "                Standard deviation to use if normalizing the image. Can be a float or a list of floats corresponding to the number of channels in the image.\n",
            "            do_convert_rgb (`bool`, *optional*, defaults to `self.do_convert_rgb`):\n",
            "                Whether to convert the image to RGB.\n",
            "            data_format (`ChannelDimension`, *optional*, defaults to `ChannelDimension.FIRST`):\n",
            "                The channel dimension format for the output image. Can be one of:\n",
            "                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n",
            "                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n",
            "                - Unset: Use the channel dimension format of the input image.\n",
            "            input_data_format (`ChannelDimension` or `str`, *optional*):\n",
            "                The channel dimension format for the input image. Can be one of:\n",
            "                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n",
            "                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n",
            "                - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.   - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n",
            "        \"\"\"\n",
            "        images = make_list_of_images(images)\n",
            "\n",
            "        if do_convert_rgb:\n",
            "            images = [convert_to_rgb(image) for image in images]\n",
            "\n",
            "        # All transformations expect numpy arrays.\n",
            "        images = [to_numpy_array(image) for image in images]\n",
            "\n",
            "        if is_scaled_image(images[0]) and do_rescale:\n",
            "            logger.warning_once(\n",
            "                \"It looks like you are trying to rescale already rescaled images. If the input\"\n",
            "                \" images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\"\n",
            "            )\n",
            "        if input_data_format is None:\n",
            "            # We assume that all images have the same channel dimension format.\n",
            "            input_data_format = infer_channel_dimension_format(images[0])\n",
            "\n",
            "        height, width = get_image_size(images[0], channel_dim=input_data_format)\n",
            "        resized_height, resized_width = height, width\n",
            "        processed_images = []\n",
            "        for image in images:\n",
            "            if do_resize:\n",
            "                resized_height, resized_width = smart_resize(\n",
            "                    height,\n",
            "                    width,\n",
            "                    factor=self.patch_size * self.merge_size,\n",
            "                    min_pixels=self.min_pixels,\n",
            "                    max_pixels=self.max_pixels,\n",
            "                )\n",
            "                image = resize(\n",
            "                    image, size=(resized_height, resized_width), resample=resample, input_data_format=input_data_format\n",
            "                )\n",
            "\n",
            "            if do_rescale:\n",
            "                image = self.rescale(image, scale=rescale_factor, input_data_format=input_data_format)\n",
            "\n",
            "            if do_normalize:\n",
            "                image = self.normalize(\n",
            "                    image=image, mean=image_mean, std=image_std, input_data_format=input_data_format\n",
            "                )\n",
            "\n",
            "            image = to_channel_dimension_format(image, data_format, input_channel_dim=input_data_format)\n",
            "            processed_images.append(image)\n",
            "\n",
            "        patches = np.array(processed_images)\n",
            "        if data_format == ChannelDimension.LAST:\n",
            "            patches = patches.transpose(0, 3, 1, 2)\n",
            "        if patches.shape[0] == 1:\n",
            "            patches = np.tile(patches, (self.temporal_patch_size, 1, 1, 1))\n",
            "        channel = patches.shape[1]\n",
            "        grid_t = patches.shape[0] // self.temporal_patch_size\n",
            "        grid_h, grid_w = resized_height // self.patch_size, resized_width // self.patch_size\n",
            "        patches = patches.reshape(\n",
            "            grid_t,\n",
            "            self.temporal_patch_size,\n",
            "            channel,\n",
            "            grid_h // self.merge_size,\n",
            "            self.merge_size,\n",
            "            self.patch_size,\n",
            "            grid_w // self.merge_size,\n",
            "            self.merge_size,\n",
            "            self.patch_size,\n",
            "        )\n",
            "        patches = patches.transpose(0, 3, 6, 4, 7, 2, 1, 5, 8)\n",
            "        flatten_patches = patches.reshape(\n",
            "            grid_t * grid_h * grid_w, channel * self.temporal_patch_size * self.patch_size * self.patch_size\n",
            "        )\n",
            "\n",
            "        return flatten_patches, (grid_t, grid_h, grid_w)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Let's see the _preprocess method again, focusing on where we left off\n",
        "source = inspect.getsource(processor.image_processor._preprocess)\n",
        "print(source)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc3ff56c-b97c-424b-92cf-f663a3a09dd4",
      "metadata": {
        "id": "cc3ff56c-b97c-424b-92cf-f663a3a09dd4"
      },
      "outputs": [],
      "source": [
        "# ok we saw smart_resize before but let's understand it better\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62812da7-c3d0-4f79-a63c-dffa10b61394",
      "metadata": {
        "id": "62812da7-c3d0-4f79-a63c-dffa10b61394",
        "outputId": "74d638cc-9bb1-4324-bbdc-858a4a42dd84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "def smart_resize(\n",
            "    height: int, width: int, factor: int = 28, min_pixels: int = 56 * 56, max_pixels: int = 14 * 14 * 4 * 1280\n",
            "):\n",
            "    \"\"\"Rescales the image so that the following conditions are met:\n",
            "\n",
            "    1. Both dimensions (height and width) are divisible by 'factor'.\n",
            "\n",
            "    2. The total number of pixels is within the range ['min_pixels', 'max_pixels'].\n",
            "\n",
            "    3. The aspect ratio of the image is maintained as closely as possible.\n",
            "\n",
            "    \"\"\"\n",
            "    if height < factor or width < factor:\n",
            "        raise ValueError(f\"height:{height} or width:{width} must be larger than factor:{factor}\")\n",
            "    elif max(height, width) / min(height, width) > 200:\n",
            "        raise ValueError(\n",
            "            f\"absolute aspect ratio must be smaller than 200, got {max(height, width) / min(height, width)}\"\n",
            "        )\n",
            "    h_bar = round(height / factor) * factor\n",
            "    w_bar = round(width / factor) * factor\n",
            "    if h_bar * w_bar > max_pixels:\n",
            "        beta = math.sqrt((height * width) / max_pixels)\n",
            "        h_bar = math.floor(height / beta / factor) * factor\n",
            "        w_bar = math.floor(width / beta / factor) * factor\n",
            "    elif h_bar * w_bar < min_pixels:\n",
            "        beta = math.sqrt(min_pixels / (height * width))\n",
            "        h_bar = math.ceil(height * beta / factor) * factor\n",
            "        w_bar = math.ceil(width * beta / factor) * factor\n",
            "    return h_bar, w_bar\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Let's inspect smart_resize\n",
        "from transformers.models.qwen2_vl.image_processing_qwen2_vl import smart_resize\n",
        "import inspect\n",
        "\n",
        "source = inspect.getsource(smart_resize)\n",
        "print(source)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a925b0be-ef09-4df1-b5cb-8c72fbd3255f",
      "metadata": {
        "id": "a925b0be-ef09-4df1-b5cb-8c72fbd3255f",
        "outputId": "22961fcb-633f-4b5d-bb49-63d3cb38778b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found in qwen_vl_utils!\n",
            "def smart_resize(\n",
            "    height: int, width: int, factor: int = IMAGE_FACTOR, min_pixels: int = MIN_PIXELS, max_pixels: int = MAX_PIXELS\n",
            ") -> tuple[int, int]:\n",
            "    \"\"\"\n",
            "    Rescales the image so that the following conditions are met:\n",
            "\n",
            "    1. Both dimensions (height and width) are divisible by 'factor'.\n",
            "\n",
            "    2. The total number of pixels is within the range ['min_pixels', 'max_pixels'].\n",
            "\n",
            "    3. The aspect ratio of the image is maintained as closely as possible.\n",
            "    \"\"\"\n",
            "    if max(height, width) / min(height, width) > MAX_RATIO:\n",
            "        raise ValueError(\n",
            "            f\"absolute aspect ratio must be smaller than {MAX_RATIO}, got {max(height, width) / min(height, width)}\"\n",
            "        )\n",
            "    h_bar = max(factor, round_by_factor(height, factor))\n",
            "    w_bar = max(factor, round_by_factor(width, factor))\n",
            "    if h_bar * w_bar > max_pixels:\n",
            "        beta = math.sqrt((height * width) / max_pixels)\n",
            "        h_bar = floor_by_factor(height / beta, factor)\n",
            "        w_bar = floor_by_factor(width / beta, factor)\n",
            "    elif h_bar * w_bar < min_pixels:\n",
            "        beta = math.sqrt(min_pixels / (height * width))\n",
            "        h_bar = ceil_by_factor(height * beta, factor)\n",
            "        w_bar = ceil_by_factor(width * beta, factor)\n",
            "    return h_bar, w_bar\n",
            "\n",
            "\n",
            "Transformers smart_resize location: /usr/local/lib/python3.12/dist-packages/transformers/models/qwen2_vl/image_processing_qwen2_vl.py\n"
          ]
        }
      ],
      "source": [
        "# Try importing from the path you mentioned\n",
        "try:\n",
        "    from qwen_vl_utils.vision_process import smart_resize as qwen_smart_resize\n",
        "    import inspect\n",
        "    print(\"Found in qwen_vl_utils!\")\n",
        "    print(inspect.getsource(qwen_smart_resize))\n",
        "except ImportError as e:\n",
        "    print(f\"ImportError: {e}\")\n",
        "    print(\"\\nThis module might not be installed or might be from a different package\")\n",
        "\n",
        "# Let's also check where our current smart_resize comes from\n",
        "from transformers.models.qwen2_vl.image_processing_qwen2_vl import smart_resize as transformers_smart_resize\n",
        "print(f\"\\nTransformers smart_resize location: {inspect.getfile(transformers_smart_resize)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9471920d-47db-493e-b26f-ba8511bd0dc5",
      "metadata": {
        "id": "9471920d-47db-493e-b26f-ba8511bd0dc5",
        "outputId": "fc890ffd-fade-4a16-83aa-dd1569f883ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original image size: (500, 300)\n",
            "\n",
            "--- Step 1: process_vision_info (qwen_vl_utils) ---\n",
            "After process_vision_info: (504, 308)\n",
            "\n",
            "--- Step 2: processor (transformers) ---\n",
            "Final pixel_values shape: torch.Size([792, 1176])\n",
            "Grid dimensions (THW): tensor([[ 1, 22, 36]])\n",
            "\n",
            "--- Detailed resize trace ---\n",
            "qwen_vl_utils smart_resize: (300, 500) → (308, 504)\n",
            "transformers smart_resize: (308, 504) → (308, 504)\n",
            "Same dimensions, but still processed twice!\n"
          ]
        }
      ],
      "source": [
        "# ok I don't like it, absolutely don't like it\n",
        "# is it double work ???\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from qwen_vl_utils import process_vision_info\n",
        "from transformers import AutoProcessor\n",
        "\n",
        "# Create a test image\n",
        "test_image = Image.new('RGB', (500, 300), color='red')\n",
        "print(f\"Original image size: {test_image.size}\")\n",
        "\n",
        "# Create a message with this image\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"image\", \"image\": test_image},\n",
        "            {\"type\": \"text\", \"text\": \"What's in this image?\"}\n",
        "        ]\n",
        "    }\n",
        "]\n",
        "\n",
        "# Step 1: process_vision_info (qwen_vl_utils) - FIRST RESIZE\n",
        "print(\"\\n--- Step 1: process_vision_info (qwen_vl_utils) ---\")\n",
        "image_inputs, video_inputs = process_vision_info(messages)\n",
        "print(f\"After process_vision_info: {image_inputs[0].size}\")\n",
        "\n",
        "# Step 2: Let's manually check what the processor will do - SECOND RESIZE\n",
        "print(\"\\n--- Step 2: processor (transformers) ---\")\n",
        "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")\n",
        "\n",
        "# Apply chat template\n",
        "text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "# Process with the already-resized image\n",
        "inputs = processor(text=[text], images=image_inputs, videos=video_inputs, padding=True, return_tensors=\"pt\")\n",
        "\n",
        "# Check the final dimensions\n",
        "print(f\"Final pixel_values shape: {inputs['pixel_values'].shape}\")\n",
        "print(f\"Grid dimensions (THW): {inputs['image_grid_thw']}\")\n",
        "\n",
        "# Let's trace the exact resize operations\n",
        "print(\"\\n--- Detailed resize trace ---\")\n",
        "from qwen_vl_utils.vision_process import smart_resize as qwen_smart_resize\n",
        "from transformers.models.qwen2_vl.image_processing_qwen2_vl import smart_resize as tf_smart_resize\n",
        "\n",
        "# What qwen_vl_utils calculated\n",
        "h1, w1 = qwen_smart_resize(300, 500)\n",
        "print(f\"qwen_vl_utils smart_resize: (300, 500) → ({h1}, {w1})\")\n",
        "\n",
        "# What transformers would calculate on the already-resized image\n",
        "h2, w2 = tf_smart_resize(h1, w1)\n",
        "print(f\"transformers smart_resize: ({h1}, {w1}) → ({h2}, {w2})\")\n",
        "\n",
        "print(\"\\nDouble resizing confirmed!\" if (h1, w1) != (h2, w2) else \"Same dimensions, but still processed twice!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21a04d88-5161-45f9-95a8-57f8eb293a25",
      "metadata": {
        "id": "21a04d88-5161-45f9-95a8-57f8eb293a25",
        "outputId": "155f1dcb-df03-447b-caf1-78f486af8286"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Environment ===\n",
            "transformers version: 4.46.2\n",
            "qwen_vl_utils version: unknown\n",
            "torch version: 2.4.1+cu121\n",
            "PIL version: 11.0.0\n"
          ]
        }
      ],
      "source": [
        "# Document versions\n",
        "import transformers\n",
        "import qwen_vl_utils\n",
        "import torch\n",
        "import PIL\n",
        "\n",
        "print(\"=== Environment ===\")\n",
        "print(f\"transformers version: {transformers.__version__}\")\n",
        "print(f\"qwen_vl_utils version: {qwen_vl_utils.__version__ if hasattr(qwen_vl_utils, '__version__') else 'unknown'}\")\n",
        "print(f\"torch version: {torch.__version__}\")\n",
        "print(f\"PIL version: {PIL.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf8fbf68-8d78-47ae-835b-2cea070e60a8",
      "metadata": {
        "id": "bf8fbf68-8d78-47ae-835b-2cea070e60a8",
        "outputId": "58462534-54d8-4b06-b395-cb328d015ede"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Double Resize Demonstration ===\n",
            "Original image size: (500, 300)\n",
            "\n",
            "--- Resize #1: qwen_vl_utils.process_vision_info ---\n",
            "After process_vision_info: (504, 308)\n",
            "\n",
            "--- Resize #2: transformers.processor ---\n",
            "Processor will resize to: (308, 504)\n",
            "Final tensor shape: torch.Size([792, 1176])\n"
          ]
        }
      ],
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "from qwen_vl_utils import process_vision_info\n",
        "from transformers import AutoProcessor\n",
        "\n",
        "print(\"\\n=== Double Resize Demonstration ===\")\n",
        "\n",
        "# Create test image with known size\n",
        "test_image = Image.new('RGB', (500, 300), color='red')\n",
        "print(f\"Original image size: {test_image.size}\")\n",
        "\n",
        "# Prepare message\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"image\", \"image\": test_image},\n",
        "            {\"type\": \"text\", \"text\": \"Test\"}\n",
        "        ]\n",
        "    }\n",
        "]\n",
        "\n",
        "# Track resize #1 - qwen_vl_utils\n",
        "print(\"\\n--- Resize #1: qwen_vl_utils.process_vision_info ---\")\n",
        "image_inputs, _ = process_vision_info(messages)\n",
        "print(f\"After process_vision_info: {image_inputs[0].size}\")\n",
        "\n",
        "# Track resize #2 - transformers\n",
        "print(\"\\n--- Resize #2: transformers.processor ---\")\n",
        "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")\n",
        "text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "# Show that processor will resize again\n",
        "from transformers.models.qwen2_vl.image_processing_qwen2_vl import smart_resize\n",
        "h, w = image_inputs[0].size[1], image_inputs[0].size[0]  # PIL uses (width, height)\n",
        "new_h, new_w = smart_resize(h, w, factor=28)\n",
        "print(f\"Processor will resize to: ({new_h}, {new_w})\")\n",
        "\n",
        "inputs = processor(text=[text], images=image_inputs, padding=True, return_tensors=\"pt\")\n",
        "print(f\"Final tensor shape: {inputs['pixel_values'].shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc5b7c12-f21e-4d7f-8899-0daa51e3e811",
      "metadata": {
        "id": "fc5b7c12-f21e-4d7f-8899-0daa51e3e811",
        "outputId": "347bdf01-5392-4f73-adee-49d6807adc68"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Performance Impact ===\n",
            "Double resize (official): 0.083s for 10 iterations\n",
            "Single resize (processor only): 0.066s for 10 iterations\n",
            "Overhead: 26.5%\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "print(\"\\n=== Performance Impact ===\")\n",
        "\n",
        "# Time the double resize approach (official)\n",
        "start = time.time()\n",
        "for _ in range(10):\n",
        "    image_inputs, _ = process_vision_info(messages)\n",
        "    inputs = processor(text=[text], images=image_inputs, padding=True, return_tensors=\"pt\")\n",
        "time_double = time.time() - start\n",
        "\n",
        "# Time single resize (processor only)\n",
        "start = time.time()\n",
        "for _ in range(10):\n",
        "    # Extract image without qwen_vl_utils resize\n",
        "    raw_image = messages[0][\"content\"][0][\"image\"]\n",
        "    inputs = processor(text=[text], images=[raw_image], padding=True, return_tensors=\"pt\")\n",
        "time_single = time.time() - start\n",
        "\n",
        "print(f\"Double resize (official): {time_double:.3f}s for 10 iterations\")\n",
        "print(f\"Single resize (processor only): {time_single:.3f}s for 10 iterations\")\n",
        "print(f\"Overhead: {((time_double - time_single) / time_single * 100):.1f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02adeb47-a784-4339-96c1-c7a69be5acb1",
      "metadata": {
        "id": "02adeb47-a784-4339-96c1-c7a69be5acb1",
        "outputId": "1be79a0d-d4b4-4f96-fb8f-2864e670b748"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Resize Logic Duplication ===\n",
            "Input (224, 224):\n",
            "  qwen_vl_utils: (224, 224)\n",
            "  transformers:  (224, 224)\n",
            "  chained:       (224, 224) -> (224, 224)\n",
            "Input (500, 300):\n",
            "  qwen_vl_utils: (504, 308)\n",
            "  transformers:  (504, 308)\n",
            "  chained:       (504, 308) -> (504, 308)\n",
            "Input (1024, 768):\n",
            "  qwen_vl_utils: (1036, 756)\n",
            "  transformers:  (1036, 756)\n",
            "  chained:       (1036, 756) -> (1036, 756)\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n=== Resize Logic Duplication ===\")\n",
        "\n",
        "# Show both smart_resize implementations\n",
        "from qwen_vl_utils.vision_process import smart_resize as qwen_smart_resize\n",
        "from transformers.models.qwen2_vl.image_processing_qwen2_vl import smart_resize as tf_smart_resize\n",
        "\n",
        "# Test with various sizes\n",
        "test_sizes = [(224, 224), (500, 300), (1024, 768)]\n",
        "\n",
        "for h, w in test_sizes:\n",
        "    qwen_h, qwen_w = qwen_smart_resize(h, w)\n",
        "    tf_h, tf_w = tf_smart_resize(h, w)\n",
        "    print(f\"Input ({h}, {w}):\")\n",
        "    print(f\"  qwen_vl_utils: ({qwen_h}, {qwen_w})\")\n",
        "    print(f\"  transformers:  ({tf_h}, {tf_w})\")\n",
        "\n",
        "    # Then show what happens when chained\n",
        "    tf_h2, tf_w2 = tf_smart_resize(qwen_h, qwen_w)\n",
        "    print(f\"  chained:       ({qwen_h}, {qwen_w}) -> ({tf_h2}, {tf_w2})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24d6acfb-bec6-4078-ad99-7637046855d1",
      "metadata": {
        "id": "24d6acfb-bec6-4078-ad99-7637046855d1",
        "outputId": "bee2796b-cf2b-4853-a19f-6bfd2b9c5b11"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Summary for Bug Report ===\n",
            "\n",
            "ISSUE: Official example causes double image resizing\n",
            "\n",
            "EVIDENCE:\n",
            "1. qwen_vl_utils.fetch_image() resizes images using smart_resize\n",
            "2. transformers.processor() resizes the already-resized images again\n",
            "3. Both use similar but different smart_resize implementations\n",
            "\n",
            "IMPACT:\n",
            "- Unnecessary computation (shown above)\n",
            "- Potential image quality degradation from multiple interpolations\n",
            "- Confusion about which parameters control final image size\n",
            "\n",
            "QUESTION:\n",
            "Is this intentional? If not, which resize should be kept?\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n=== Summary for Bug Report ===\")\n",
        "print(\"\"\"\n",
        "ISSUE: Official example causes double image resizing\n",
        "\n",
        "EVIDENCE:\n",
        "1. qwen_vl_utils.fetch_image() resizes images using smart_resize\n",
        "2. transformers.processor() resizes the already-resized images again\n",
        "3. Both use similar but different smart_resize implementations\n",
        "\n",
        "IMPACT:\n",
        "- Unnecessary computation (shown above)\n",
        "- Potential image quality degradation from multiple interpolations\n",
        "- Confusion about which parameters control final image size\n",
        "\n",
        "QUESTION:\n",
        "Is this intentional? If not, which resize should be kept?\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "835f5dc8-0853-4149-94a1-cd0439f98707",
      "metadata": {
        "id": "835f5dc8-0853-4149-94a1-cd0439f98707",
        "outputId": "eefc327f-c45a-4a58-81e4-cd229342e108"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Verifying resize is actually skipped ===\n",
            "\n",
            "After qwen_vl_utils: (588, 392)\n",
            "\n",
            "Time with do_resize=True: 0.119s\n",
            "Time with do_resize=False: 0.105s\n",
            "Speedup: 1.13x\n",
            "Time saved: 11.2%\n"
          ]
        }
      ],
      "source": [
        "print(\"=== Verifying resize is actually skipped ===\\n\")\n",
        "\n",
        "import time\n",
        "from PIL import Image\n",
        "from qwen_vl_utils import process_vision_info\n",
        "\n",
        "# Create test image\n",
        "test_img = Image.new('RGB', (600, 400), 'white')\n",
        "messages = [{\"role\": \"user\", \"content\": [{\"type\": \"image\", \"image\": test_img}, {\"type\": \"text\", \"text\": \"Test\"}]}]\n",
        "\n",
        "# Process through qwen_vl_utils first\n",
        "image_inputs, _ = process_vision_info(messages)\n",
        "print(f\"After qwen_vl_utils: {image_inputs[0].size}\\n\")\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")\n",
        "text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "# Time with do_resize=True (double resize)\n",
        "start = time.time()\n",
        "for _ in range(20):\n",
        "    inputs = processor(text=[text], images=image_inputs, do_resize=True, padding=True, return_tensors=\"pt\")\n",
        "time_with_resize = time.time() - start\n",
        "\n",
        "# Time with do_resize=False (skip second resize)\n",
        "start = time.time()\n",
        "for _ in range(20):\n",
        "    inputs = processor(text=[text], images=image_inputs, do_resize=False, padding=True, return_tensors=\"pt\")\n",
        "time_without_resize = time.time() - start\n",
        "\n",
        "print(f\"Time with do_resize=True: {time_with_resize:.3f}s\")\n",
        "print(f\"Time with do_resize=False: {time_without_resize:.3f}s\")\n",
        "print(f\"Speedup: {time_with_resize/time_without_resize:.2f}x\")\n",
        "print(f\"Time saved: {((time_with_resize - time_without_resize) / time_with_resize * 100):.1f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba9b337f-0a1a-4593-9156-c627c8059c46",
      "metadata": {
        "id": "ba9b337f-0a1a-4593-9156-c627c8059c46",
        "outputId": "74429830-c828-43f8-a7e5-88b9b64efc15"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Tracing resize calls ===\n",
            "Image after qwen_vl_utils: (588, 392)\n",
            "\n",
            "With do_resize=True:\n",
            "  Total: 0 resize calls\n",
            "\n",
            "With do_resize=False:\n",
            "  Total: 0 resize calls\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n=== Tracing resize calls ===\")\n",
        "\n",
        "# Monkey-patch to trace calls\n",
        "from transformers import image_transforms\n",
        "original_resize = image_transforms.resize\n",
        "\n",
        "resize_calls = []\n",
        "\n",
        "def traced_resize(image, size, **kwargs):\n",
        "    resize_calls.append(size)\n",
        "    print(f\"  resize() called with size={size}, image shape={image.shape if hasattr(image, 'shape') else 'PIL'}\")\n",
        "    return original_resize(image, size, **kwargs)\n",
        "\n",
        "image_transforms.resize = traced_resize\n",
        "\n",
        "# Test with fresh image\n",
        "test_img = Image.new('RGB', (600, 400), 'white')\n",
        "messages = [{\"role\": \"user\", \"content\": [{\"type\": \"image\", \"image\": test_img}, {\"type\": \"text\", \"text\": \"Test\"}]}]\n",
        "image_inputs, _ = process_vision_info(messages)\n",
        "text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "print(f\"Image after qwen_vl_utils: {image_inputs[0].size}\\n\")\n",
        "\n",
        "print(\"With do_resize=True:\")\n",
        "resize_calls = []\n",
        "inputs = processor(text=[text], images=image_inputs, do_resize=True, padding=True, return_tensors=\"pt\")\n",
        "print(f\"  Total: {len(resize_calls)} resize calls\\n\")\n",
        "\n",
        "print(\"With do_resize=False:\")\n",
        "resize_calls = []\n",
        "inputs = processor(text=[text], images=image_inputs, do_resize=False, padding=True, return_tensors=\"pt\")\n",
        "print(f\"  Total: {len(resize_calls)} resize calls\")\n",
        "\n",
        "# Restore\n",
        "image_transforms.resize = original_resize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9242f363-cb3f-43ac-8c73-bb6ebd0c34e6",
      "metadata": {
        "id": "9242f363-cb3f-43ac-8c73-bb6ebd0c34e6",
        "outputId": "75a42f1f-a33d-485c-f4ae-5208d07ff65b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== ACTUAL RESIZE LOGIC IN _preprocess ===\n",
            "\n",
            "69:         for image in images:\n",
            "70:             if do_resize:\n",
            "71:                 resized_height, resized_width = smart_resize(\n",
            "72:                     height,\n",
            "73:                     width,\n",
            "74:                     factor=self.patch_size * self.merge_size,\n",
            "75:                     min_pixels=self.min_pixels,\n",
            "76:                     max_pixels=self.max_pixels,\n",
            "77:                 )\n",
            "78:                 image = resize(\n",
            "79:                     image, size=(resized_height, resized_width), resample=resample, input_data_format=input_data_format\n",
            "80:                 )\n",
            "81: \n",
            "82:             if do_rescale:\n",
            "83:                 image = self.rescale(image, scale=rescale_factor, input_data_format=input_data_format)\n",
            "84: \n",
            "85:             if do_normalize:\n",
            "86:                 image = self.normalize(\n",
            "87:                     image=image, mean=image_mean, std=image_std, input_data_format=input_data_format\n",
            "88:                 )\n"
          ]
        }
      ],
      "source": [
        "import inspect\n",
        "\n",
        "# Get the _preprocess source code\n",
        "source = inspect.getsource(processor.image_processor._preprocess)\n",
        "lines = source.split('\\n')\n",
        "\n",
        "# Find and show the resize logic\n",
        "print(\"=== ACTUAL RESIZE LOGIC IN _preprocess ===\\n\")\n",
        "for i, line in enumerate(lines):\n",
        "    if 'for image in images:' in line:\n",
        "        # Show the entire loop that processes each image\n",
        "        for j in range(i, min(i+20, len(lines))):\n",
        "            print(f\"{j}: {lines[j]}\")\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8bd55eee-7d8b-4d67-a887-152af6a490cf",
      "metadata": {
        "id": "8bd55eee-7d8b-4d67-a887-152af6a490cf",
        "outputId": "47df22d2-fa1d-4abf-d4e7-a0210d1abe83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== RESIZE FUNCTION SOURCE ===\n",
            "\n",
            "def resize(\n",
            "    image: np.ndarray,\n",
            "    size: Tuple[int, int],\n",
            "    resample: \"PILImageResampling\" = None,\n",
            "    reducing_gap: Optional[int] = None,\n",
            "    data_format: Optional[ChannelDimension] = None,\n",
            "    return_numpy: bool = True,\n",
            "    input_data_format: Optional[Union[str, ChannelDimension]] = None,\n",
            ") -> np.ndarray:\n",
            "    \"\"\"\n",
            "    Resizes `image` to `(height, width)` specified by `size` using the PIL library.\n",
            "\n",
            "    Args:\n",
            "        image (`np.ndarray`):\n",
            "            The image to resize.\n",
            "        size (`Tuple[int, int]`):\n",
            "            The size to use for resizing the image.\n",
            "        resample (`int`, *optional*, defaults to `PILImageResampling.BILINEAR`):\n",
            "            The filter to user for resampling.\n",
            "        reducing_gap (`int`, *optional*):\n",
            "            Apply optimization by resizing the image in two steps. The bigger `reducing_gap`, the closer the result to\n",
            "            the fair resampling. See corresponding Pillow documentation for more details.\n",
            "        data_format (`ChannelDimension`, *optional*):\n",
            "            The channel dimension format of the output image. If unset, will use the inferred format from the input.\n",
            "        return_numpy (`bool`, *optional*, defaults to `True`):\n",
            "            Whether or not to return the resized image as a numpy array. If False a `PIL.Image.Image` object is\n",
            "            returned.\n",
            "        input_data_format (`ChannelDimension`, *optional*):\n",
            "            The channel dimension format of the input image. If unset, will use the inferred format from the input.\n",
            "\n",
            "    Returns:\n",
            "        `np.ndarray`: The resized image.\n",
            "    \"\"\"\n",
            "    requires_backends(resize, [\"vision\"])\n",
            "\n",
            "    resample = resample if resample is not None else PILImageResampling.BILINEAR\n",
            "\n",
            "    if not len(size) == 2:\n",
            "        raise ValueError(\"size must have 2 elements\")\n",
            "\n",
            "    # For all transformations, we want to keep the same data format as the input image unless otherwise specified.\n",
            "    # The resized image from PIL will always have channels last, so find the input format first.\n",
            "    if input_data_format is None:\n",
            "        input_data_format = infer_channel_dimension_format(image)\n",
            "    data_format = input_data_format if data_format is None else data_format\n",
            "\n",
            "    # To maintain backwards compatibility with the resizing done in previous image feature extractors, we use\n",
            "    # the pillow library to resize the image and then convert back to numpy\n",
            "    do_rescale = False\n",
            "    if not isinstance(image, PIL.Image.Image):\n",
            "        do_rescale = _rescale_for_pil_conversion(image)\n",
            "        image = to_pil_image(image, do_rescale=do_rescale, input_data_format=input_data_format)\n",
            "    height, width = size\n",
            "    # PIL images are in the format (width, height)\n",
            "    resized_image = image.resize((width, height), resample=resample, reducing_gap=reducing_gap)\n",
            "\n",
            "    if return_numpy:\n",
            "        resized_image = np.array(resized_image)\n",
            "        # If the input image channel dimension was of size 1, then it is dropped when converting to a PIL image\n",
            "        # so we need to add it back if necessary.\n",
            "        resized_image = np.expand_dims(resized_image, axis=-1) if resized_image.ndim == 2 else resized_image\n",
            "        # The image is always in channels last format after converting from a PIL image\n",
            "        resized_image = to_channel_dimension_format(\n",
            "            resized_image, data_format, input_channel_dim=ChannelDimension.LAST\n",
            "        )\n",
            "        # If an image was rescaled to be in the range [0, 255] before converting to a PIL image, then we need to\n",
            "        # rescale it back to the original range.\n",
            "        resized_image = rescale(resized_image, 1 / 255) if do_rescale else resized_image\n",
            "    return resized_image\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"=== RESIZE FUNCTION SOURCE ===\\n\")\n",
        "from transformers.image_transforms import resize\n",
        "import inspect\n",
        "\n",
        "source = inspect.getsource(resize)\n",
        "print(source)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a08a2cb-0ede-454d-a555-5640706869c1",
      "metadata": {
        "id": "9a08a2cb-0ede-454d-a555-5640706869c1",
        "outputId": "ce98bffd-895d-4872-9dcb-63f6c5aa1a23"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nAccurate benchmark of double resize issue\\n'"
            ]
          },
          "execution_count": 148,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== ACCURATE BENCHMARK ===\n",
            "Running 20 iterations each...\n",
            "\n",
            "With double resize: 0.149s for 20 iterations\n",
            "With single resize: 0.136s for 20 iterations\n",
            "Average per iteration:\n",
            "  Double resize: 7.4ms\n",
            "  Single resize: 6.8ms\n",
            "Performance improvement: 8.4%\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Accurate benchmark of double resize issue\n",
        "\"\"\"\n",
        "\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import time\n",
        "from qwen_vl_utils import process_vision_info\n",
        "from transformers import AutoProcessor\n",
        "\n",
        "# Pre-load processor to avoid initialization bias\n",
        "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")\n",
        "\n",
        "# Create test image\n",
        "test_image = Image.new('RGB', (600, 400), color='white')\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"image\", \"image\": test_image},\n",
        "            {\"type\": \"text\", \"text\": \"Test\"}\n",
        "        ]\n",
        "    }\n",
        "]\n",
        "\n",
        "# Pre-process once to warm up\n",
        "text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "image_inputs, video_inputs = process_vision_info(messages)\n",
        "\n",
        "# Now benchmark properly with multiple iterations\n",
        "iterations = 20\n",
        "\n",
        "print(\"=== ACCURATE BENCHMARK ===\")\n",
        "print(f\"Running {iterations} iterations each...\\n\")\n",
        "\n",
        "# Test WITH double resize (official way)\n",
        "start = time.time()\n",
        "for _ in range(iterations):\n",
        "    inputs = processor(\n",
        "        text=[text],\n",
        "        images=image_inputs,\n",
        "        videos=video_inputs,\n",
        "        padding=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )  # do_resize=True (default)\n",
        "time_with_double_resize = time.time() - start\n",
        "\n",
        "# Test WITHOUT double resize (fixed)\n",
        "start = time.time()\n",
        "for _ in range(iterations):\n",
        "    inputs = processor(\n",
        "        text=[text],\n",
        "        images=image_inputs,\n",
        "        videos=video_inputs,\n",
        "        do_resize=False,  # Skip redundant resize\n",
        "        padding=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "time_single_resize = time.time() - start\n",
        "\n",
        "print(f\"With double resize: {time_with_double_resize:.3f}s for {iterations} iterations\")\n",
        "print(f\"With single resize: {time_single_resize:.3f}s for {iterations} iterations\")\n",
        "print(f\"Average per iteration:\")\n",
        "print(f\"  Double resize: {time_with_double_resize/iterations*1000:.1f}ms\")\n",
        "print(f\"  Single resize: {time_single_resize/iterations*1000:.1f}ms\")\n",
        "print(f\"Performance improvement: {((time_with_double_resize - time_single_resize) / time_with_double_resize * 100):.1f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40ca5df5-7808-4399-983e-49e091653079",
      "metadata": {
        "id": "40ca5df5-7808-4399-983e-49e091653079",
        "outputId": "4f900a65-ccac-4c15-bfb5-0dfdfccc193a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== RESIZE PARAMETERS ===\n",
            "patch_size: 14\n",
            "merge_size: 2\n",
            "factor: 28\n",
            "min_pixels: 3136\n",
            "max_pixels: 12845056\n"
          ]
        }
      ],
      "source": [
        "print(\"=== RESIZE PARAMETERS ===\")\n",
        "print(f\"patch_size: {processor.image_processor.patch_size}\")\n",
        "print(f\"merge_size: {processor.image_processor.merge_size}\")\n",
        "print(f\"factor: {processor.image_processor.patch_size * processor.image_processor.merge_size}\")\n",
        "print(f\"min_pixels: {processor.image_processor.min_pixels}\")\n",
        "print(f\"max_pixels: {processor.image_processor.max_pixels}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79429b76-54c0-474a-84fe-7417e08abe5f",
      "metadata": {
        "id": "79429b76-54c0-474a-84fe-7417e08abe5f",
        "outputId": "615e5540-6123-4c2e-e1ba-11bdc61d85b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== COMPLETE _preprocess FUNCTION ===\n",
            "\n",
            "    def _preprocess(\n",
            "        self,\n",
            "        images: Union[ImageInput, VideoInput],\n",
            "        do_resize: bool = None,\n",
            "        resample: PILImageResampling = None,\n",
            "        do_rescale: bool = None,\n",
            "        rescale_factor: float = None,\n",
            "        do_normalize: bool = None,\n",
            "        image_mean: Optional[Union[float, List[float]]] = None,\n",
            "        image_std: Optional[Union[float, List[float]]] = None,\n",
            "        do_convert_rgb: bool = None,\n",
            "        data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n",
            "        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n",
            "    ):\n",
            "        \"\"\"\n",
            "        Preprocess an image or batch of images. Copy of the `preprocess` method from `CLIPImageProcessor`.\n",
            "\n",
            "        Args:\n",
            "            images (`ImageInput`):\n",
            "                Image or batch of images to preprocess. Expects pixel values ranging from 0 to 255. If pixel values range from 0 to 1, set `do_rescale=False`.\n",
            "            vision_info (`List[Dict]`, *optional*):\n",
            "                Optional list of dictionaries containing additional information about vision inputs.\n",
            "            do_resize (`bool`, *optional*, defaults to `self.do_resize`):\n",
            "                Whether to resize the image.\n",
            "            resample (`PILImageResampling`, *optional*, defaults to `self.resample`):\n",
            "                Resampling filter to use if resizing the image. This can be one of the `PILImageResampling` enums.\n",
            "            do_rescale (`bool`, *optional*, defaults to `self.do_rescale`):\n",
            "                Whether to rescale the image.\n",
            "            rescale_factor (`float`, *optional*, defaults to `self.rescale_factor`):\n",
            "                Scale factor to use if rescaling the image.\n",
            "            do_normalize (`bool`, *optional*, defaults to `self.do_normalize`):\n",
            "                Whether to normalize the image.\n",
            "            image_mean (`float` or `List[float]`, *optional*, defaults to `self.image_mean`):\n",
            "                Mean to use if normalizing the image. Can be a float or a list of floats corresponding to the number of channels in the image.\n",
            "            image_std (`float` or `List[float]`, *optional*, defaults to `self.image_std`):\n",
            "                Standard deviation to use if normalizing the image. Can be a float or a list of floats corresponding to the number of channels in the image.\n",
            "            do_convert_rgb (`bool`, *optional*, defaults to `self.do_convert_rgb`):\n",
            "                Whether to convert the image to RGB.\n",
            "            data_format (`ChannelDimension`, *optional*, defaults to `ChannelDimension.FIRST`):\n",
            "                The channel dimension format for the output image. Can be one of:\n",
            "                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n",
            "                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n",
            "                - Unset: Use the channel dimension format of the input image.\n",
            "            input_data_format (`ChannelDimension` or `str`, *optional*):\n",
            "                The channel dimension format for the input image. Can be one of:\n",
            "                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n",
            "                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n",
            "                - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.   - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n",
            "        \"\"\"\n",
            "        images = make_list_of_images(images)\n",
            "\n",
            "        if do_convert_rgb:\n",
            "            images = [convert_to_rgb(image) for image in images]\n",
            "\n",
            "        # All transformations expect numpy arrays.\n",
            "        images = [to_numpy_array(image) for image in images]\n",
            "\n",
            "        if is_scaled_image(images[0]) and do_rescale:\n",
            "            logger.warning_once(\n",
            "                \"It looks like you are trying to rescale already rescaled images. If the input\"\n",
            "                \" images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\"\n",
            "            )\n",
            "        if input_data_format is None:\n",
            "            # We assume that all images have the same channel dimension format.\n",
            "            input_data_format = infer_channel_dimension_format(images[0])\n",
            "\n",
            "        height, width = get_image_size(images[0], channel_dim=input_data_format)\n",
            "        resized_height, resized_width = height, width\n",
            "        processed_images = []\n",
            "        for image in images:\n",
            "            if do_resize:\n",
            "                resized_height, resized_width = smart_resize(\n",
            "                    height,\n",
            "                    width,\n",
            "                    factor=self.patch_size * self.merge_size,\n",
            "                    min_pixels=self.min_pixels,\n",
            "                    max_pixels=self.max_pixels,\n",
            "                )\n",
            "                image = resize(\n",
            "                    image, size=(resized_height, resized_width), resample=resample, input_data_format=input_data_format\n",
            "                )\n",
            "\n",
            "            if do_rescale:\n",
            "                image = self.rescale(image, scale=rescale_factor, input_data_format=input_data_format)\n",
            "\n",
            "            if do_normalize:\n",
            "                image = self.normalize(\n",
            "                    image=image, mean=image_mean, std=image_std, input_data_format=input_data_format\n",
            "                )\n",
            "\n",
            "            image = to_channel_dimension_format(image, data_format, input_channel_dim=input_data_format)\n",
            "            processed_images.append(image)\n",
            "\n",
            "        patches = np.array(processed_images)\n",
            "        if data_format == ChannelDimension.LAST:\n",
            "            patches = patches.transpose(0, 3, 1, 2)\n",
            "        if patches.shape[0] == 1:\n",
            "            patches = np.tile(patches, (self.temporal_patch_size, 1, 1, 1))\n",
            "        channel = patches.shape[1]\n",
            "        grid_t = patches.shape[0] // self.temporal_patch_size\n",
            "        grid_h, grid_w = resized_height // self.patch_size, resized_width // self.patch_size\n",
            "        patches = patches.reshape(\n",
            "            grid_t,\n",
            "            self.temporal_patch_size,\n",
            "            channel,\n",
            "            grid_h // self.merge_size,\n",
            "            self.merge_size,\n",
            "            self.patch_size,\n",
            "            grid_w // self.merge_size,\n",
            "            self.merge_size,\n",
            "            self.patch_size,\n",
            "        )\n",
            "        patches = patches.transpose(0, 3, 6, 4, 7, 2, 1, 5, 8)\n",
            "        flatten_patches = patches.reshape(\n",
            "            grid_t * grid_h * grid_w, channel * self.temporal_patch_size * self.patch_size * self.patch_size\n",
            "        )\n",
            "\n",
            "        return flatten_patches, (grid_t, grid_h, grid_w)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import inspect\n",
        "\n",
        "# Print the ENTIRE _preprocess function\n",
        "source = inspect.getsource(processor.image_processor._preprocess)\n",
        "print(\"=== COMPLETE _preprocess FUNCTION ===\\n\")\n",
        "print(source)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8fea8edc-7c2a-44df-8b6a-23c9da12c51b",
      "metadata": {
        "id": "8fea8edc-7c2a-44df-8b6a-23c9da12c51b"
      },
      "outputs": [],
      "source": [
        "# ok so like I am a bit confused so if images have a different dimenison\n",
        "# why resizing happens based only on first image ????\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1bd8fb4-8750-4692-943a-9b467b3b1dde",
      "metadata": {
        "id": "c1bd8fb4-8750-4692-943a-9b467b3b1dde",
        "outputId": "e1c3fca5-a45e-4f24-ab1e-eca73396dee9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "temporal_patch_size = 2\n",
            "patch_size = 14\n",
            "merge_size = 2\n"
          ]
        }
      ],
      "source": [
        "# Check the actual value\n",
        "print(f\"temporal_patch_size = {processor.image_processor.temporal_patch_size}\")\n",
        "\n",
        "# Let's also check related parameters\n",
        "print(f\"patch_size = {processor.image_processor.patch_size}\")\n",
        "print(f\"merge_size = {processor.image_processor.merge_size}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "602b1510-d400-4f44-8a25-d38eabadd24a",
      "metadata": {
        "id": "602b1510-d400-4f44-8a25-d38eabadd24a",
        "outputId": "3e80b094-dad4-4db8-8585-8801c0384c9a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== COMPLETE preprocess FUNCTION ===\n",
            "\n",
            "    def preprocess(\n",
            "        self,\n",
            "        images: ImageInput,\n",
            "        videos: VideoInput = None,\n",
            "        do_resize: bool = None,\n",
            "        size: Dict[str, int] = None,\n",
            "        resample: PILImageResampling = None,\n",
            "        do_rescale: bool = None,\n",
            "        rescale_factor: float = None,\n",
            "        do_normalize: bool = None,\n",
            "        image_mean: Optional[Union[float, List[float]]] = None,\n",
            "        image_std: Optional[Union[float, List[float]]] = None,\n",
            "        do_convert_rgb: bool = None,\n",
            "        return_tensors: Optional[Union[str, TensorType]] = None,\n",
            "        data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n",
            "        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n",
            "    ):\n",
            "        \"\"\"\n",
            "        Args:\n",
            "            images (`ImageInput`):\n",
            "                Image to preprocess. Expects a single or batch of images with pixel values ranging from 0 to 255. If\n",
            "                passing in images with pixel values between 0 and 1, set `do_rescale=False`.\n",
            "            videos (`VideoInput`):\n",
            "                Video to preprocess. Expects a single or batch of videos with pixel values ranging from 0 to 255. If\n",
            "                passing in videos with pixel values between 0 and 1, set `do_rescale=False`.\n",
            "            do_resize (`bool`, *optional*, defaults to `self.do_resize`):\n",
            "                Whether to resize the image.\n",
            "            size (`Dict[str, int]`, *optional*, defaults to `self.size`):\n",
            "                Size of the image after resizing. Shortest edge of the image is resized to size[\"shortest_edge\"], with\n",
            "                the longest edge resized to keep the input aspect ratio.\n",
            "            resample (`int`, *optional*, defaults to `self.resample`):\n",
            "                Resampling filter to use if resizing the image. This can be one of the enum `PILImageResampling`. Only\n",
            "                has an effect if `do_resize` is set to `True`.\n",
            "            do_rescale (`bool`, *optional*, defaults to `self.do_rescale`):\n",
            "                Whether to rescale the image.\n",
            "            rescale_factor (`float`, *optional*, defaults to `self.rescale_factor`):\n",
            "                Rescale factor to rescale the image by if `do_rescale` is set to `True`.\n",
            "            do_normalize (`bool`, *optional*, defaults to `self.do_normalize`):\n",
            "                Whether to normalize the image.\n",
            "            image_mean (`float` or `List[float]`, *optional*, defaults to `self.image_mean`):\n",
            "                Image mean to use for normalization. Only has an effect if `do_normalize` is set to `True`.\n",
            "            image_std (`float` or `List[float]`, *optional*, defaults to `self.image_std`):\n",
            "                Image standard deviation to use for normalization. Only has an effect if `do_normalize` is set to\n",
            "                `True`.\n",
            "            do_convert_rgb (`bool`, *optional*, defaults to `self.do_convert_rgb`):\n",
            "                Whether to convert the image to RGB.\n",
            "            return_tensors (`str` or `TensorType`, *optional*):\n",
            "                The type of tensors to return. Can be one of:\n",
            "                - Unset: Return a list of `np.ndarray`.\n",
            "                - `TensorType.TENSORFLOW` or `'tf'`: Return a batch of type `tf.Tensor`.\n",
            "                - `TensorType.PYTORCH` or `'pt'`: Return a batch of type `torch.Tensor`.\n",
            "                - `TensorType.NUMPY` or `'np'`: Return a batch of type `np.ndarray`.\n",
            "                - `TensorType.JAX` or `'jax'`: Return a batch of type `jax.numpy.ndarray`.\n",
            "            data_format (`ChannelDimension` or `str`, *optional*, defaults to `ChannelDimension.FIRST`):\n",
            "                The channel dimension format for the output image. Can be one of:\n",
            "                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n",
            "                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n",
            "                - Unset: Use the channel dimension format of the input image.\n",
            "            input_data_format (`ChannelDimension` or `str`, *optional*):\n",
            "                The channel dimension format for the input image. If unset, the channel dimension format is inferred\n",
            "                from the input image. Can be one of:\n",
            "                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n",
            "                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n",
            "                - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n",
            "\n",
            "        \"\"\"\n",
            "        do_resize = do_resize if do_resize is not None else self.do_resize\n",
            "        size = size if size is not None else self.size\n",
            "        resample = resample if resample is not None else self.resample\n",
            "        do_rescale = do_rescale if do_rescale is not None else self.do_rescale\n",
            "        rescale_factor = rescale_factor if rescale_factor is not None else self.rescale_factor\n",
            "        do_normalize = do_normalize if do_normalize is not None else self.do_normalize\n",
            "        image_mean = image_mean if image_mean is not None else self.image_mean\n",
            "        image_std = image_std if image_std is not None else self.image_std\n",
            "        do_convert_rgb = do_convert_rgb if do_convert_rgb is not None else self.do_convert_rgb\n",
            "\n",
            "        if images is not None:\n",
            "            images = make_batched_images(images)\n",
            "        if videos is not None:\n",
            "            videos = make_batched_videos(videos)\n",
            "\n",
            "        if images is not None and not valid_images(images):\n",
            "            raise ValueError(\n",
            "                \"Invalid image type. Must be of type PIL.Image.Image, numpy.ndarray, \"\n",
            "                \"torch.Tensor, tf.Tensor or jax.ndarray.\"\n",
            "            )\n",
            "\n",
            "        validate_preprocess_arguments(\n",
            "            rescale_factor=rescale_factor,\n",
            "            do_normalize=do_normalize,\n",
            "            image_mean=image_mean,\n",
            "            image_std=image_std,\n",
            "            do_resize=do_resize,\n",
            "            size=size,\n",
            "            resample=resample,\n",
            "        )\n",
            "\n",
            "        if images is not None:\n",
            "            pixel_values, vision_grid_thws = [], []\n",
            "            for image in images:\n",
            "                patches, image_grid_thw = self._preprocess(\n",
            "                    image,\n",
            "                    do_resize=do_resize,\n",
            "                    resample=resample,\n",
            "                    do_rescale=do_rescale,\n",
            "                    rescale_factor=rescale_factor,\n",
            "                    do_normalize=do_normalize,\n",
            "                    image_mean=image_mean,\n",
            "                    image_std=image_std,\n",
            "                    data_format=data_format,\n",
            "                    do_convert_rgb=do_convert_rgb,\n",
            "                    input_data_format=input_data_format,\n",
            "                )\n",
            "                pixel_values.extend(patches)\n",
            "                vision_grid_thws.append(image_grid_thw)\n",
            "            pixel_values = np.array(pixel_values)\n",
            "            vision_grid_thws = np.array(vision_grid_thws)\n",
            "            data = {\"pixel_values\": pixel_values, \"image_grid_thw\": vision_grid_thws}\n",
            "\n",
            "        if videos is not None:\n",
            "            pixel_values, vision_grid_thws = [], []\n",
            "            for images in videos:\n",
            "                patches, video_grid_thw = self._preprocess(\n",
            "                    images,\n",
            "                    do_resize=do_resize,\n",
            "                    resample=resample,\n",
            "                    do_rescale=do_rescale,\n",
            "                    rescale_factor=rescale_factor,\n",
            "                    do_normalize=do_normalize,\n",
            "                    image_mean=image_mean,\n",
            "                    image_std=image_std,\n",
            "                    data_format=data_format,\n",
            "                    do_convert_rgb=do_convert_rgb,\n",
            "                    input_data_format=input_data_format,\n",
            "                )\n",
            "                pixel_values.extend(patches)\n",
            "                vision_grid_thws.append(video_grid_thw)\n",
            "            pixel_values = np.array(pixel_values)\n",
            "            vision_grid_thws = np.array(vision_grid_thws)\n",
            "            data = {\"pixel_values_videos\": pixel_values, \"video_grid_thw\": vision_grid_thws}\n",
            "\n",
            "        return BatchFeature(data=data, tensor_type=return_tensors)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Look at the actual preprocess function\n",
        "import inspect\n",
        "source = inspect.getsource(processor.image_processor.preprocess)\n",
        "print(\"=== COMPLETE preprocess FUNCTION ===\\n\")\n",
        "print(source)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe0b07ca-9966-4f44-9a3f-3cc4ffbdf3bb",
      "metadata": {
        "id": "fe0b07ca-9966-4f44-9a3f-3cc4ffbdf3bb"
      },
      "outputs": [],
      "source": [
        "#ok this is really strange but they send only one image at a time to preprocess\n",
        "# for image in images:\n",
        "    # patches, image_grid_thw = self._preprocess(\n",
        "    #     image,\n",
        "    #     do_resize=do_resize,\n",
        "    #     resample=resample,\n",
        "    #     do_rescale=do_rescale,\n",
        "    #     rescale_factor=rescale_factor,\n",
        "    #     do_normalize=do_normalize,\n",
        "    #     image_mean=image_mean,\n",
        "    #     image_std=image_std,\n",
        "    #     data_format=data_format,\n",
        "    #     do_convert_rgb=do_convert_rgb,\n",
        "    #     input_data_format=input_data_format,\n",
        "    # )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a253b3a9-148f-41f2-852a-85fd8109d154",
      "metadata": {
        "id": "a253b3a9-148f-41f2-852a-85fd8109d154"
      },
      "outputs": [],
      "source": [
        "# it's a total mess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "601adfd5-de41-4611-96b3-8584827cb955",
      "metadata": {
        "id": "601adfd5-de41-4611-96b3-8584827cb955",
        "outputId": "d0025851-329a-4890-ee9b-2d2b52a76bdc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== CONCRETE EXAMPLE: 28x28 RGB Image → Patches ===\n",
            "\n",
            "Step 0 - Original: shape=(1, 3, 28, 28)\n",
            "  This is 1 image, 3 channels, 28x28 pixels\n",
            "\n",
            "Step 1 - Add temporal: shape=(2, 3, 28, 28)\n",
            "  Duplicated image: (2, 3, 28, 28)\n",
            "\n",
            "Step 2 - Grid dimensions:\n",
            "  grid_t=1, grid_h=2, grid_w=2\n",
            "  Image will be split into 2x2 = 4 patches\n",
            "\n",
            "Step 3 - Reshape to 9D: shape=(1, 2, 3, 1, 2, 14, 1, 2, 14)\n",
            "  Dimensions represent: (t_grid, t_size, channels, h_merged, merge, patch_h, w_merged, merge, patch_w)\n",
            "\n",
            "Step 4 - Transpose: shape=(1, 1, 1, 2, 2, 3, 2, 14, 14)\n",
            "  Rearranged to group patches\n",
            "\n",
            "Step 5 - Final flatten: shape=(4, 1176)\n",
            "  Result: 4 patches, each is a 1176-dimensional vector\n",
            "  Each patch contains: 14x14 pixels × 3 channels × 2 temporal = 1176 values\n",
            "\n",
            "=== VISUAL REPRESENTATION ===\n",
            "\n",
            "Original 28x28 image:\n",
            "+-------+-------+\n",
            "|Patch0 |Patch1 | 14x14 each\n",
            "+-------+-------+\n",
            "|Patch2 |Patch3 |\n",
            "+-------+-------+\n",
            "\n",
            "After processing:\n",
            "Row 0: [1176 values from Patch0]\n",
            "Row 1: [1176 values from Patch1]  \n",
            "Row 2: [1176 values from Patch2]\n",
            "Row 3: [1176 values from Patch3]\n",
            "\n",
            "Shape: (4, 1176)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Let's use a tiny example image for clarity\n",
        "# Imagine a 28x28 image (much smaller than real, but easier to follow)\n",
        "\n",
        "print(\"=== CONCRETE EXAMPLE: 28x28 RGB Image → Patches ===\\n\")\n",
        "\n",
        "# Parameters\n",
        "patch_size = 14\n",
        "merge_size = 2\n",
        "temporal_patch_size = 2\n",
        "channels = 3\n",
        "\n",
        "# Start with a simple image\n",
        "H, W = 28, 28\n",
        "image = np.arange(H * W * channels).reshape(1, channels, H, W)\n",
        "print(f\"Step 0 - Original: shape={image.shape}\")\n",
        "print(f\"  This is 1 image, 3 channels, 28x28 pixels\\n\")\n",
        "\n",
        "# Step 1: Handle temporal (duplicate for images)\n",
        "patches = np.tile(image, (temporal_patch_size, 1, 1, 1))\n",
        "print(f\"Step 1 - Add temporal: shape={patches.shape}\")\n",
        "print(f\"  Duplicated image: (2, 3, 28, 28)\\n\")\n",
        "\n",
        "# Step 2: Calculate grid\n",
        "grid_t = 2 // temporal_patch_size  # = 1\n",
        "grid_h = H // patch_size            # = 2\n",
        "grid_w = W // patch_size            # = 2\n",
        "print(f\"Step 2 - Grid dimensions:\")\n",
        "print(f\"  grid_t={grid_t}, grid_h={grid_h}, grid_w={grid_w}\")\n",
        "print(f\"  Image will be split into {grid_h}x{grid_w} = 4 patches\\n\")\n",
        "\n",
        "# Step 3: Reshape to separate patches\n",
        "patches = patches.reshape(\n",
        "    grid_t,                    # 1\n",
        "    temporal_patch_size,       # 2\n",
        "    channels,                  # 3\n",
        "    grid_h // merge_size,      # 1\n",
        "    merge_size,                # 2\n",
        "    patch_size,                # 14\n",
        "    grid_w // merge_size,      # 1\n",
        "    merge_size,                # 2\n",
        "    patch_size,                # 14\n",
        ")\n",
        "print(f\"Step 3 - Reshape to 9D: shape={patches.shape}\")\n",
        "print(f\"  Dimensions represent: (t_grid, t_size, channels, h_merged, merge, patch_h, w_merged, merge, patch_w)\\n\")\n",
        "\n",
        "# Step 4: Transpose\n",
        "patches = patches.transpose(0, 3, 6, 4, 7, 2, 1, 5, 8)\n",
        "print(f\"Step 4 - Transpose: shape={patches.shape}\")\n",
        "print(f\"  Rearranged to group patches\\n\")\n",
        "\n",
        "# Step 5: Flatten\n",
        "flatten_patches = patches.reshape(\n",
        "    grid_t * grid_h * grid_w,  # 1*2*2 = 4 patches\n",
        "    channels * temporal_patch_size * patch_size * patch_size  # 3*2*14*14 = 1176\n",
        ")\n",
        "print(f\"Step 5 - Final flatten: shape={flatten_patches.shape}\")\n",
        "print(f\"  Result: 4 patches, each is a 1176-dimensional vector\")\n",
        "print(f\"  Each patch contains: 14x14 pixels × 3 channels × 2 temporal = 1176 values\\n\")\n",
        "\n",
        "print(\"=== VISUAL REPRESENTATION ===\")\n",
        "print(\"\"\"\n",
        "Original 28x28 image:\n",
        "+-------+-------+\n",
        "|Patch0 |Patch1 | 14x14 each\n",
        "+-------+-------+\n",
        "|Patch2 |Patch3 |\n",
        "+-------+-------+\n",
        "\n",
        "After processing:\n",
        "Row 0: [1176 values from Patch0]\n",
        "Row 1: [1176 values from Patch1]\n",
        "Row 2: [1176 values from Patch2]\n",
        "Row 3: [1176 values from Patch3]\n",
        "\n",
        "Shape: (4, 1176)\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77ebb113-f542-4a9b-9d0b-f35838817c05",
      "metadata": {
        "id": "77ebb113-f542-4a9b-9d0b-f35838817c05",
        "outputId": "2318ee4a-a59a-4549-ab98-85acfc5693f8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "588"
            ]
          },
          "execution_count": 164,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ok this is really weird\n",
        "# 1) why they are creating a duplicate of the same image ?\n",
        "# like to addreess frames if it's video?\n",
        "# Every patch has 2 temporal copies of the same 14×14 region\n",
        "14*14*3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f90ac2bc-2be0-4847-8912-306f1f79cd1b",
      "metadata": {
        "id": "f90ac2bc-2be0-4847-8912-306f1f79cd1b",
        "outputId": "18e7e5ba-3b63-49b6-bea5-41e825250645"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== GRID DIMENSIONS CALCULATION ===\n",
            "\n",
            "Image: 308×392 pixels\n",
            "Patch size: 14×14\n",
            "Grid: 22×28 patches\n",
            "\n",
            "Image divided into patches:\n",
            "+-------+-------+-------+-------+-------+-------+ ... (28 columns total)\n",
            "| 14×14 | 14×14 | 14×14 | 14×14 | 14×14 | 14×14 | ...\n",
            "+-------+-------+-------+-------+-------+-------+ ...\n",
            "| 14×14 | 14×14 | 14×14 | 14×14 | 14×14 | 14×14 | ...\n",
            "+-------+-------+-------+-------+-------+-------+ ...\n",
            "| 14×14 | 14×14 | 14×14 | 14×14 | 14×14 | 14×14 | ...\n",
            "+-------+-------+-------+-------+-------+-------+ ...\n",
            "... (22 rows total)\n",
            "\n",
            "Total patches: 22 × 28 = 616\n"
          ]
        }
      ],
      "source": [
        "print(\"=== GRID DIMENSIONS CALCULATION ===\\n\")\n",
        "\n",
        "# Example image: 308 x 392 pixels\n",
        "H, W = 308, 392\n",
        "patch_size = 14\n",
        "\n",
        "# Calculate grid dimensions\n",
        "grid_h = H // patch_size  # 308 ÷ 14 = 22\n",
        "grid_w = W // patch_size  # 392 ÷ 14 = 28\n",
        "\n",
        "print(f\"Image: {H}×{W} pixels\")\n",
        "print(f\"Patch size: {patch_size}×{patch_size}\")\n",
        "print(f\"Grid: {grid_h}×{grid_w} patches\\n\")\n",
        "\n",
        "# Visualize\n",
        "print(\"Image divided into patches:\")\n",
        "print(\"+\" + \"-\"*7 + (\"+\" + \"-\"*7)*5 + \"+ ... (28 columns total)\")\n",
        "for i in range(3):\n",
        "    print(\"|\" + \" 14×14 |\" * 6 + \" ...\")\n",
        "    print(\"+\" + \"-\"*7 + (\"+\" + \"-\"*7)*5 + \"+ ...\")\n",
        "print(\"... (22 rows total)\\n\")\n",
        "\n",
        "print(f\"Total patches: {grid_h} × {grid_w} = {grid_h * grid_w}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e4dced1-33df-4d75-915f-d7859b47b1e7",
      "metadata": {
        "id": "2e4dced1-33df-4d75-915f-d7859b47b1e7"
      },
      "outputs": [],
      "source": [
        "# ok next step is insane"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f9509df-965f-46f0-943a-a6934dcae740",
      "metadata": {
        "id": "2f9509df-965f-46f0-943a-a6934dcae740"
      },
      "outputs": [],
      "source": [
        "# patches = patches.reshape(\n",
        "#     grid_t,               # Dim 0: = 1 (temporal grid cells)\n",
        "#     temporal_patch_size,  # Dim 1: = 2 (frames per temporal patch)\n",
        "#     channels,             # Dim 2: = 3 (RGB)\n",
        "#     grid_h // merge_size, # Dim 3: = 1 (vertical groups of patches)\n",
        "#     merge_size,           # Dim 4: = 2 (patches per vertical group)\n",
        "#     patch_size,           # Dim 5: = 14 (pixel height per patch)\n",
        "#     grid_w // merge_size, # Dim 6: = 1 (horizontal groups of patches)\n",
        "#     merge_size,           # Dim 7: = 2 (patches per horizontal group)\n",
        "#     patch_size,           # Dim 8: = 14 (pixel width per patch)\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "576ee91a-370b-428b-9fc2-3620a96d0d8e",
      "metadata": {
        "id": "576ee91a-370b-428b-9fc2-3620a96d0d8e",
        "outputId": "cafc011a-3677-485c-f055-da9eeaf88eeb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original 4x4 image:\n",
            "[[ 1  2  3  4]\n",
            " [ 5  6  7  8]\n",
            " [ 9 10 11 12]\n",
            " [13 14 15 16]]\n",
            "\n",
            "==================================================\n",
            "\n",
            "❌ WRONG WAY: Just flatten and reshape\n",
            "\n",
            "Flattened: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16]\n",
            "\n",
            "Wrong patches:\n",
            "Patch 0: [1 2 3 4]\n",
            "Patch 1: [5 6 7 8]\n",
            "Patch 2: [ 9 10 11 12]\n",
            "Patch 3: [13 14 15 16]\n",
            "\n",
            "⚠️  These are NOT spatial regions! Patch 0 has pixels 1,2,3,4\n",
            "   which are all from the TOP ROW, not a 2x2 region!\n",
            "\n",
            "==================================================\n",
            "\n",
            "✅ RIGHT WAY: Reshape to group spatial regions\n",
            "\n",
            "After reshape(2, 2, 2, 2):\n",
            "(n_patches_h, patch_height, n_patches_w, patch_width)\n",
            "\n",
            "Correct patches:\n",
            "Patch 0: [1 2 5 6]\n",
            "Patch 1: [3 4 7 8]\n",
            "Patch 2: [ 9 10 13 14]\n",
            "Patch 3: [11 12 15 16]\n",
            "\n",
            "✅ These ARE spatial regions!\n",
            "   Patch 0: [1,2,5,6] = top-left 2x2 region\n",
            "   Patch 1: [3,4,7,8] = top-right 2x2 region\n",
            "   Patch 2: [9,10,13,14] = bottom-left 2x2 region\n",
            "   Patch 3: [11,12,15,16] = bottom-right 2x2 region\n",
            "\n",
            "==================================================\n",
            "\n",
            "Visual proof - each letter represents a patch region:\n",
            "[['A' 'A' 'B' 'B']\n",
            " ['A' 'A' 'B' 'B']\n",
            " ['C' 'C' 'D' 'D']\n",
            " ['C' 'C' 'D' 'D']]\n",
            "\n",
            "Extracted patches:\n",
            "Patch 0: ['A' 'A' 'A' 'A'] - All same letter = same region! ✅\n",
            "Patch 1: ['B' 'B' 'B' 'B'] - All same letter = same region! ✅\n",
            "Patch 2: ['C' 'C' 'C' 'C'] - All same letter = same region! ✅\n",
            "Patch 3: ['D' 'D' 'D' 'D'] - All same letter = same region! ✅\n",
            "\n",
            "==================================================\n",
            "\n",
            "FINAL PROOF - Tracking pixel positions:\n",
            "\n",
            "Each patch contains positions:\n",
            "Patch 0: ['(0,0)' '(0,1)' '(1,0)' '(1,1)']\n",
            "Patch 1: ['(0,2)' '(0,3)' '(1,2)' '(1,3)']\n",
            "Patch 2: ['(2,0)' '(2,1)' '(3,0)' '(3,1)']\n",
            "Patch 3: ['(2,2)' '(2,3)' '(3,2)' '(3,3)']\n",
            "\n",
            "✅ Each patch contains a 2x2 SQUARE of adjacent positions!\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Create a 4x4 image where each pixel is numbered\n",
        "# This makes it easy to track where pixels end up\n",
        "image = np.array([[1,  2,  3,  4],\n",
        "                  [5,  6,  7,  8],\n",
        "                  [9,  10, 11, 12],\n",
        "                  [13, 14, 15, 16]])\n",
        "\n",
        "print(\"Original 4x4 image:\")\n",
        "print(image)\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "# WRONG WAY: Just flatten and chunk\n",
        "print(\"❌ WRONG WAY: Just flatten and reshape\")\n",
        "flat = image.flatten()\n",
        "wrong_patches = flat.reshape(4, 4)  # 4 patches, 4 pixels each\n",
        "print(\"\\nFlattened:\", flat)\n",
        "print(\"\\nWrong patches:\")\n",
        "for i, patch in enumerate(wrong_patches):\n",
        "    print(f\"Patch {i}: {patch}\")\n",
        "print(\"\\n⚠️  These are NOT spatial regions! Patch 0 has pixels 1,2,3,4\")\n",
        "print(\"   which are all from the TOP ROW, not a 2x2 region!\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "# RIGHT WAY: Reshape to preserve spatial locality\n",
        "print(\"✅ RIGHT WAY: Reshape to group spatial regions\")\n",
        "patch_size = 2\n",
        "n_patches_h = 4 // patch_size  # 2\n",
        "n_patches_w = 4 // patch_size  # 2\n",
        "\n",
        "# The magic reshape - this groups spatially adjacent pixels!\n",
        "reshaped = image.reshape(n_patches_h, patch_size, n_patches_w, patch_size)\n",
        "print(f\"\\nAfter reshape{reshaped.shape}:\")\n",
        "print(\"(n_patches_h, patch_height, n_patches_w, patch_width)\")\n",
        "\n",
        "# Now transpose to get patches in the right order\n",
        "# and flatten each patch\n",
        "patches = reshaped.transpose(0, 2, 1, 3).reshape(4, 4)\n",
        "\n",
        "print(\"\\nCorrect patches:\")\n",
        "for i in range(4):\n",
        "    print(f\"Patch {i}: {patches[i]}\")\n",
        "\n",
        "print(\"\\n✅ These ARE spatial regions!\")\n",
        "print(\"   Patch 0: [1,2,5,6] = top-left 2x2 region\")\n",
        "print(\"   Patch 1: [3,4,7,8] = top-right 2x2 region\")\n",
        "print(\"   Patch 2: [9,10,13,14] = bottom-left 2x2 region\")\n",
        "print(\"   Patch 3: [11,12,15,16] = bottom-right 2x2 region\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "# VISUAL PROOF: Let's mark each patch with a different letter\n",
        "visual_image = np.array([['A', 'A', 'B', 'B'],\n",
        "                         ['A', 'A', 'B', 'B'],\n",
        "                         ['C', 'C', 'D', 'D'],\n",
        "                         ['C', 'C', 'D', 'D']], dtype='U1')\n",
        "\n",
        "print(\"Visual proof - each letter represents a patch region:\")\n",
        "print(visual_image)\n",
        "\n",
        "# Extract patches the right way\n",
        "visual_reshaped = visual_image.reshape(2, 2, 2, 2)\n",
        "visual_patches = visual_reshaped.transpose(0, 2, 1, 3).reshape(4, 4)\n",
        "\n",
        "print(\"\\nExtracted patches:\")\n",
        "for i in range(4):\n",
        "    print(f\"Patch {i}: {visual_patches[i]} - All same letter = same region! ✅\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "# FINAL PROOF: Show the actual positions\n",
        "print(\"FINAL PROOF - Tracking pixel positions:\")\n",
        "positions = np.array([[f'(0,0)', f'(0,1)', f'(0,2)', f'(0,3)'],\n",
        "                      [f'(1,0)', f'(1,1)', f'(1,2)', f'(1,3)'],\n",
        "                      [f'(2,0)', f'(2,1)', f'(2,2)', f'(2,3)'],\n",
        "                      [f'(3,0)', f'(3,1)', f'(3,2)', f'(3,3)']], dtype='U5')\n",
        "\n",
        "pos_reshaped = positions.reshape(2, 2, 2, 2)\n",
        "pos_patches = pos_reshaped.transpose(0, 2, 1, 3).reshape(4, -1)\n",
        "\n",
        "print(\"\\nEach patch contains positions:\")\n",
        "for i in range(4):\n",
        "    print(f\"Patch {i}: {pos_patches[i]}\")\n",
        "\n",
        "print(\"\\n✅ Each patch contains a 2x2 SQUARE of adjacent positions!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a872ce2-0d56-43e1-babf-8cfe9c47021a",
      "metadata": {
        "id": "2a872ce2-0d56-43e1-babf-8cfe9c47021a",
        "outputId": "adfbaf7d-c6a2-43ed-a735-4a43afacfa69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== ACTUAL PROCESSOR PARAMETERS ===\n",
            "patch_size: 14\n",
            "merge_size: 2\n",
            "temporal_patch_size: 2\n",
            "\n",
            "=== TRACING THROUGH PREPROCESSING ===\n",
            "\n",
            "pixel_values shape: torch.Size([784, 1176])\n",
            "image_grid_thw shape: torch.Size([1, 3])\n",
            "image_grid_thw values: tensor([[ 1, 28, 28]])\n",
            "\n",
            "=== MANUAL PATCH CREATION ===\n",
            "\n",
            "Image: 392x392\n",
            "Grid: 28x28 = 784 patches\n",
            "Each patch: 14x14 pixels\n",
            "\n",
            "With merge_size=2:\n",
            "Groups: 14x14 = 196 groups\n",
            "Each group contains: 2x2 = 4 patches\n",
            "Total patches/tokens: 784 = 784 = 784\n",
            "\n",
            "=== FINAL OUTPUT ===\n",
            "Expected shape: (784, 1176)\n",
            "Expected tokens: 784 separate tokens\n",
            "Each token represents: ONE 14x14 patch\n",
            "\n",
            "=== PROOF: Each 14x14 region becomes ONE token ===\n",
            "Token 0: pixels from position (0,0) to (13,13)\n",
            "Token 1: pixels from position (0,14) to (13,27)\n",
            "Token 2: pixels from position (0,28) to (13,41)\n",
            "...\n",
            "Token 783: pixels from position (378,378) to (391,391)\n",
            "\n",
            "Total: 784 independent tokens\n",
            "NO merging of neighboring patches!\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from transformers import AutoProcessor\n",
        "\n",
        "# Load the processor\n",
        "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")\n",
        "\n",
        "# Check the actual parameters\n",
        "print(\"=== ACTUAL PROCESSOR PARAMETERS ===\")\n",
        "print(f\"patch_size: {processor.image_processor.patch_size}\")\n",
        "print(f\"merge_size: {processor.image_processor.merge_size}\")\n",
        "print(f\"temporal_patch_size: {processor.image_processor.temporal_patch_size}\")\n",
        "print()\n",
        "\n",
        "# Create a test image to trace through\n",
        "# Let's use a simple 392x392 image (divisible by patch_size * merge_size)\n",
        "test_image = np.ones((392, 392, 3), dtype=np.uint8) * 255\n",
        "\n",
        "# Create markers in specific regions so we can track them\n",
        "test_image[0:28, 0:28, :] = [255, 0, 0]      # Red top-left corner\n",
        "test_image[0:28, 364:392, :] = [0, 255, 0]   # Green top-right corner\n",
        "test_image[364:392, 0:28, :] = [0, 0, 255]   # Blue bottom-left corner\n",
        "\n",
        "from PIL import Image\n",
        "pil_image = Image.fromarray(test_image)\n",
        "\n",
        "# Process the image\n",
        "messages = [{\n",
        "    \"role\": \"user\",\n",
        "    \"content\": [\n",
        "        {\"type\": \"image\", \"image\": pil_image},\n",
        "        {\"type\": \"text\", \"text\": \"test\"}\n",
        "    ]\n",
        "}]\n",
        "\n",
        "# Apply chat template\n",
        "text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "# Process with debugging\n",
        "from qwen_vl_utils import process_vision_info\n",
        "image_inputs, video_inputs = process_vision_info(messages)\n",
        "\n",
        "# Now let's manually trace through _preprocess to see what happens\n",
        "print(\"=== TRACING THROUGH PREPROCESSING ===\")\n",
        "\n",
        "# Get the actual preprocessing parameters\n",
        "img_processor = processor.image_processor\n",
        "\n",
        "# Manually call _preprocess with instrumentation\n",
        "import torch\n",
        "with torch.no_grad():\n",
        "    # Get the preprocessed output\n",
        "    inputs = processor(\n",
        "        text=[text],\n",
        "        images=image_inputs,\n",
        "        videos=video_inputs,\n",
        "        padding=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    # Check the pixel_values shape\n",
        "    print(f\"\\npixel_values shape: {inputs['pixel_values'].shape}\")\n",
        "    print(f\"image_grid_thw shape: {inputs['image_grid_thw'].shape}\")\n",
        "    print(f\"image_grid_thw values: {inputs['image_grid_thw']}\")\n",
        "\n",
        "# Let's manually trace the reshape to understand patch creation\n",
        "print(\"\\n=== MANUAL PATCH CREATION ===\")\n",
        "\n",
        "# Simulate the preprocessing steps\n",
        "height, width = 392, 392\n",
        "patch_size = img_processor.patch_size  # 14\n",
        "merge_size = img_processor.merge_size  # 2\n",
        "temporal_patch_size = img_processor.temporal_patch_size  # 2\n",
        "\n",
        "# Calculate grid dimensions\n",
        "grid_h = height // patch_size  # 392 // 14 = 28\n",
        "grid_w = width // patch_size   # 392 // 14 = 28\n",
        "\n",
        "print(f\"\\nImage: {height}x{width}\")\n",
        "print(f\"Grid: {grid_h}x{grid_w} = {grid_h * grid_w} patches\")\n",
        "print(f\"Each patch: {patch_size}x{patch_size} pixels\")\n",
        "\n",
        "# Now the hierarchical grouping\n",
        "groups_h = grid_h // merge_size  # 28 // 2 = 14\n",
        "groups_w = grid_w // merge_size  # 28 // 2 = 14\n",
        "\n",
        "print(f\"\\nWith merge_size={merge_size}:\")\n",
        "print(f\"Groups: {groups_h}x{groups_w} = {groups_h * groups_w} groups\")\n",
        "print(f\"Each group contains: {merge_size}x{merge_size} = {merge_size*merge_size} patches\")\n",
        "print(f\"Total patches/tokens: {grid_h * grid_w} = {28 * 28} = 784\")\n",
        "\n",
        "# Verify with the actual output shape\n",
        "total_patches = grid_h * grid_w\n",
        "patch_dim = 3 * temporal_patch_size * patch_size * patch_size  # 3*2*14*14 = 1176\n",
        "\n",
        "print(f\"\\n=== FINAL OUTPUT ===\")\n",
        "print(f\"Expected shape: ({total_patches}, {patch_dim})\")\n",
        "print(f\"Expected tokens: {total_patches} separate tokens\")\n",
        "print(f\"Each token represents: ONE {patch_size}x{patch_size} patch\")\n",
        "\n",
        "# Prove that each patch is independent\n",
        "print(f\"\\n=== PROOF: Each 14x14 region becomes ONE token ===\")\n",
        "print(f\"Token 0: pixels from position (0,0) to (13,13)\")\n",
        "print(f\"Token 1: pixels from position (0,14) to (13,27)\")\n",
        "print(f\"Token 2: pixels from position (0,28) to (13,41)\")\n",
        "print(f\"...\")\n",
        "print(f\"Token 783: pixels from position (378,378) to (391,391)\")\n",
        "print(f\"\\nTotal: {total_patches} independent tokens\")\n",
        "print(\"NO merging of neighboring patches!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3aa2ba0-4d7d-4173-bca4-41532eb43878",
      "metadata": {
        "id": "c3aa2ba0-4d7d-4173-bca4-41532eb43878"
      },
      "outputs": [],
      "source": [
        "# so each token 14*14pixels\n",
        "# ok but then why to group tokens like 14×14 groups of 2×2 patches\n",
        "# I am making sure that all of my tokens will be packed like 2*2,\n",
        "# so like neighboring 4 tokens belong to same image region ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "299e6e9b-f3e3-4e7c-b6bd-dec942ebcc62",
      "metadata": {
        "id": "299e6e9b-f3e3-4e7c-b6bd-dec942ebcc62"
      },
      "outputs": [],
      "source": [
        "# and last reshape\n",
        "# flatten_patches = patches.reshape(\n",
        "#     grid_t * grid_h * grid_w,  # Total number of patches\n",
        "#     channel * temporal_patch_size * patch_size * patch_size  # Flattened patch content\n",
        "# )\n",
        "\n",
        "# Breaking it down:\n",
        "# grid_t * grid_h * grid_w = 1 * 28 * 28 = 784 patches\n",
        "# 3 * 2 * 14 * 14 = 1176 dimensions per patch\n",
        "\n",
        "# Shape: (784, 1176)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de767593-7165-4c16-9df6-a85f1f27568f",
      "metadata": {
        "id": "de767593-7165-4c16-9df6-a85f1f27568f"
      },
      "outputs": [],
      "source": [
        "# ok so now what haapens with all of these image tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4211c001-9240-41c2-a771-4f1b97746655",
      "metadata": {
        "id": "4211c001-9240-41c2-a771-4f1b97746655",
        "outputId": "8e232819-29f6-4378-d5b9-2296bd734143"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== STEP 1: Original Text ===\n",
            "Text before processing: <|im_start|>system\n",
            "You are a helpful assistant.<|im_end|>\n",
            "<|im_start|>user\n",
            "<|vision_start|><|image_p...\n",
            "Count of <|image_pad|> tokens: 1\n",
            "\n",
            "=== STEP 2: After Processing ===\n",
            "pixel_values shape: torch.Size([784, 1176])\n",
            "image_grid_thw: tensor([[ 1, 28, 28]])\n",
            "\n",
            "=== STEP 3: Token Count ===\n",
            "Number of <|image_pad|> tokens in input_ids: 196\n",
            "\n",
            "=== STEP 4: Mathematical Proof ===\n",
            "Grid dimensions (t, h, w): [1, 28, 28]\n",
            "Total patches: 784\n",
            "merge_size: 2\n",
            "merge_length (patches per group): 4\n",
            "Expected <|image_pad|> tokens: 784 // 4 = 196\n",
            "\n",
            "=== STEP 5: The Proof ===\n",
            "Visual patches (pixel_values): 784 patches\n",
            "Text placeholders (<|image_pad|>): 196 tokens\n",
            "Ratio: 784 / 196 = 4.0\n",
            "\n",
            "✓ Each <|image_pad|> represents 4 visual patches\n",
            "✓ That's exactly a 2×2 group!\n",
            "\n",
            "=== STEP 6: Simulating the Replacement Logic ===\n",
            "Original text has 1 <|image_pad|>\n",
            "Will be replaced with 196 <|image_pad|> tokens\n",
            "Because: [1, 28, 28] = 784 patches\n",
            "Divided by merge_length 4 = 196 placeholder tokens\n",
            "\n",
            "=== VISUAL PROOF ===\n",
            "\n",
            "392×392 image → 28×28 patches (784 total)\n",
            "\n",
            "With merge_size=2, patches are grouped 2×2:\n",
            "Group 0: [patch0, patch1, patch2, patch3]     → <|image_pad|> token 0\n",
            "Group 1: [patch4, patch5, patch6, patch7]     → <|image_pad|> token 1\n",
            "...\n",
            "Group 195: [patch780, patch781, patch782, patch783] → <|image_pad|> token 195\n",
            "\n",
            "Total: 196 <|image_pad|> tokens for 784 patches\n",
            "Ratio: 784 / 196 = 4 patches per <|image_pad|> token\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoProcessor\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "# Create processor\n",
        "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")\n",
        "\n",
        "# Create a test image\n",
        "test_image = Image.fromarray(np.ones((392, 392, 3), dtype=np.uint8) * 255)\n",
        "\n",
        "# Create a message with image\n",
        "messages = [{\n",
        "    \"role\": \"user\",\n",
        "    \"content\": [\n",
        "        {\"type\": \"image\", \"image\": test_image},\n",
        "        {\"type\": \"text\", \"text\": \"Describe this image\"}\n",
        "    ]\n",
        "}]\n",
        "\n",
        "# Apply chat template to get the text WITH placeholder\n",
        "text_with_placeholder = processor.apply_chat_template(\n",
        "    messages, tokenize=False, add_generation_prompt=True\n",
        ")\n",
        "\n",
        "print(\"=== STEP 1: Original Text ===\")\n",
        "print(f\"Text before processing: {text_with_placeholder[:100]}...\")\n",
        "print(f\"Count of <|image_pad|> tokens: {text_with_placeholder.count('<|image_pad|>')}\")\n",
        "print()\n",
        "\n",
        "# Now process the full inputs\n",
        "from qwen_vl_utils import process_vision_info\n",
        "image_inputs, video_inputs = process_vision_info(messages)\n",
        "\n",
        "inputs = processor(\n",
        "    text=[text_with_placeholder],\n",
        "    images=image_inputs,\n",
        "    videos=video_inputs,\n",
        "    padding=True,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "print(\"=== STEP 2: After Processing ===\")\n",
        "print(f\"pixel_values shape: {inputs['pixel_values'].shape}\")\n",
        "print(f\"image_grid_thw: {inputs['image_grid_thw']}\")\n",
        "print()\n",
        "\n",
        "# Decode the input_ids to see the actual tokens\n",
        "decoded_text = processor.tokenizer.decode(inputs['input_ids'][0])\n",
        "image_pad_token_id = processor.tokenizer.encode(\"<|image_pad|>\", add_special_tokens=False)[0]\n",
        "num_image_pad_tokens = (inputs['input_ids'][0] == image_pad_token_id).sum().item()\n",
        "\n",
        "print(\"=== STEP 3: Token Count ===\")\n",
        "print(f\"Number of <|image_pad|> tokens in input_ids: {num_image_pad_tokens}\")\n",
        "print()\n",
        "\n",
        "# Now let's calculate what SHOULD happen\n",
        "print(\"=== STEP 4: Mathematical Proof ===\")\n",
        "grid_thw = inputs['image_grid_thw'][0]\n",
        "total_patches = grid_thw.prod().item()\n",
        "merge_size = processor.image_processor.merge_size\n",
        "merge_length = merge_size ** 2\n",
        "\n",
        "print(f\"Grid dimensions (t, h, w): {grid_thw.tolist()}\")\n",
        "print(f\"Total patches: {total_patches}\")\n",
        "print(f\"merge_size: {merge_size}\")\n",
        "print(f\"merge_length (patches per group): {merge_length}\")\n",
        "print(f\"Expected <|image_pad|> tokens: {total_patches} // {merge_length} = {total_patches // merge_length}\")\n",
        "print()\n",
        "\n",
        "# Verify the relationship\n",
        "print(\"=== STEP 5: The Proof ===\")\n",
        "print(f\"Visual patches (pixel_values): {inputs['pixel_values'].shape[0]} patches\")\n",
        "print(f\"Text placeholders (<|image_pad|>): {num_image_pad_tokens} tokens\")\n",
        "print(f\"Ratio: {inputs['pixel_values'].shape[0]} / {num_image_pad_tokens} = {inputs['pixel_values'].shape[0] / num_image_pad_tokens}\")\n",
        "print()\n",
        "print(f\"✓ Each <|image_pad|> represents {inputs['pixel_values'].shape[0] // num_image_pad_tokens} visual patches\")\n",
        "print(f\"✓ That's exactly a {merge_size}×{merge_size} group!\")\n",
        "\n",
        "# Let's also trace through the actual replacement code\n",
        "print(\"\\n=== STEP 6: Simulating the Replacement Logic ===\")\n",
        "# Simulate what happens in __call__\n",
        "test_text = \"<|im_start|>user\\n<|image_pad|>Describe this image<|im_end|>\"\n",
        "image_grid_thw_test = torch.tensor([[1, 28, 28]])\n",
        "\n",
        "merge_length = 4  # 2^2\n",
        "num_replacements = image_grid_thw_test[0].prod().item() // merge_length\n",
        "print(f\"Original text has 1 <|image_pad|>\")\n",
        "print(f\"Will be replaced with {num_replacements} <|image_pad|> tokens\")\n",
        "print(f\"Because: {image_grid_thw_test[0].tolist()} = {image_grid_thw_test[0].prod().item()} patches\")\n",
        "print(f\"Divided by merge_length {merge_length} = {num_replacements} placeholder tokens\")\n",
        "\n",
        "# Visual representation\n",
        "print(\"\\n=== VISUAL PROOF ===\")\n",
        "print(\"\"\"\n",
        "392×392 image → 28×28 patches (784 total)\n",
        "\n",
        "With merge_size=2, patches are grouped 2×2:\n",
        "Group 0: [patch0, patch1, patch2, patch3]     → <|image_pad|> token 0\n",
        "Group 1: [patch4, patch5, patch6, patch7]     → <|image_pad|> token 1\n",
        "...\n",
        "Group 195: [patch780, patch781, patch782, patch783] → <|image_pad|> token 195\n",
        "\n",
        "Total: 196 <|image_pad|> tokens for 784 patches\n",
        "Ratio: 784 / 196 = 4 patches per <|image_pad|> token\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa129c83-94d6-4fe2-af8c-9eb9ebfe2f37",
      "metadata": {
        "id": "fa129c83-94d6-4fe2-af8c-9eb9ebfe2f37"
      },
      "outputs": [],
      "source": [
        "# ok so based on number of tokens/4 there is an extra <image_pad>\n",
        "# as if model knows single <>"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}